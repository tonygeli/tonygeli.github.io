{"pages":[],"posts":[{"title":"130个免费 微信小程序源码分享","text":"130个免费 微信小程序源码分享130个免费 微信小程序源码分享 关注微信公众号获取：","link":"/article-17/"},{"title":"常见限流算法","text":"限流算法为什么需要限流算法？避免在流量高峰的生活导致系统被压垮，造成系统不可用的问题。 1.计数器限流​ 说明：限制访问频次。每调一次，就增加次数，到达时间就自动解除。 ​ 使用场景：短信60秒只能发送一次，接口的调用次数 2.滑动窗口​ 说明：以时间为维度的可滑动窗口，来减少临界值的问题。 ​ 使用场景：SpringCloud中的Hytrix，SpringCloud Alibaba中的Sentinel 3.漏洞算法​ 说明：不管请求量多少，服务端的处理效率是固定的。 ​ 使用场景：基于阻塞队列来实现的生产者和消费者模型 4.令牌桶​ 说明：固定时间往令牌桶中加入令牌，桶的大小是固定的。桶满了后，就不会生产新的令牌。每个请求都从令牌桶中获取一个令牌，否则需要等待。在流量低峰的时候令牌会堆积，出现瞬时高峰时，有足够的令牌可以获取。 ​ 使用场景：Guava和Redisson的限流","link":"/2022/05/12/12000%E9%9D%A2%E8%AF%95/12001%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"title":"内存分配","text":"内存分配https://mp.weixin.qq.com/s/xkVxRqVTgdff5zE-kwMnLA","link":"/2022/05/12/12000%E9%9D%A2%E8%AF%95/12002%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"title":"","text":"[TOC] 1.Hooks1.1 useState123456789101112import { useState } from &quot;react&quot;;function App2() { // Hook const [num, setNum] = useState(1); return ( &lt;&gt; &lt;h2&gt;{num}&lt;/h2&gt; &lt;button onClick={()=&gt;setNum(num+1)}&gt;累加&lt;/button&gt; &lt;/&gt; );}export default App2; 1.2 useEffect12345678910111213141516171819202122import { useState,useEffect } from &quot;react&quot;;function App2() { // Hook const [num1, setNum1] = useState(1); const [num2, setNum2] = useState(1); // 模拟mounted 一般在整个位置写ajax useEffect(()=&gt;{ console.log(&quot;num1更新了&quot;) }, [num1]) // 模拟beforeDestory,处理脏数据或者垃圾回收 useEffect(()=&gt;{ }) return ( &lt;&gt; &lt;h2&gt;{num1}&lt;/h2&gt; &lt;button onClick={()=&gt;setNum1(num1+1)}&gt;累加&lt;/button&gt; &lt;/&gt; );}export default App2; 1.3 useRef12345678910111213import { useState, useRef } from &quot;react&quot;;export default function App6() { // Hook const [value, setValue] = useState(&quot;&quot;); const element = useRef(null) return ( &lt;&gt; &lt;input type=&quot;text&quot; ref={element} /&gt; &lt;button onClick={()=&gt;setNum(num+1)}&gt;累加&lt;/button&gt; &lt;/&gt; );} 2.父子组件1234567891011function Child(props) { return &lt;h2&gt;子组件 - {props.num}&lt;/h2&gt;}function Father(props) { return &lt;Child num={props.num} /&gt;}export default function App3() { return &lt;Father num={123}/&gt;} 上下文空间传递 1234567891011121314151617181920212223import React, {createContext} from 'react'// 创建上下文空间(Provider, Consumer)const NumContext = createContext()function Child() { // 不需要props const {num, setNum} = useContext(NumContext) // consumer需要结构一次 return ( &lt;&gt; &lt;h2&gt;{num}&lt;/h2&gt; &lt;button onClick={()=&gt;setNum(456)}&gt;修改num&lt;/button&gt; &lt;/&gt; )}export default function App3() { const [num, setNum] = useState(123) return ( &lt;NumContext.Provider value={{num, setNum}}&gt; &lt;Child/&gt; &lt;/NumContext&gt; )} 防止父组件刷新子组件 123456789101112131415161718import { useState, memo } from &quot;react&quot;;const Child = memo((props) =&gt; { // memo防止刷新子组件 - 子组件只能是静态的 return &lt;div&gt;子组件&lt;/div&gt;})export default function App7() { // Hook const [num, setNum] = useState(1); return ( &lt;&gt; &lt;h3&gt;{num}&lt;/h3&gt; &lt;button onClick={()=&gt;setNum(num+1)}&gt;累加&lt;/button&gt; &lt;Child/&gt; &lt;/&gt; );} 将点击事件放在子组件里面 123456789101112131415161718192021import { useState, memo, useCallback } from &quot;react&quot;;const Child = memo((props) =&gt; { // memo防止刷新子组件 return &lt;button onClick={()=&gt;props.doSth()}&gt;累加&lt;/div&gt;})export default function App7() { // Hook const [num, setNum] = useState(1); /* setNum(newValue) 覆盖旧值 setNum((num)=&gt;num+1) 不断使用新值覆盖旧值 */ const doSth = useCallback(()=&gt;setNum(num=&gt;num+1), []) // useEffect return ( &lt;&gt; &lt;h3&gt;{num}&lt;/h3&gt; &lt;Child doSth={doSth}/&gt; &lt;/&gt; );} 12345678910111213141516171819202122package com.pinkfong.gateway.admin.configuration;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;@Configurationpublic class WebConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOriginPatterns(&quot;*&quot;) .allowedMethods(&quot;*&quot;) .allowedHeaders(&quot;*&quot;) .allowCredentials(true).maxAge(3600); // Add more mappings... }} Table增加编辑列 12345678910&lt;Column key=&quot;action&quot; title=&quot;操作&quot; render={(text, record: User) =&gt; ( &lt;&gt; &lt;a style={{ marginRight: '3px' }}&gt;编辑&lt;/a&gt; &lt;Popconfirm title=&quot;Sure to delete?&quot; onConfirm={() =&gt; handleDelete(record.uid || 0)}&gt; &lt;a&gt;删除&lt;/a&gt; &lt;/Popconfirm&gt; &lt;/&gt; )}/&gt;","link":"/2022/04/21/11000%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/React/"},{"title":"ThreadLocal源码介绍","text":"ThreadLocal源码理解1.为什么需要ThreadLocal？答：可以解决多线程共享变量的问题，每个线程中都会有ThreadLocalMap变量，Map的key就是threadLocal当前对象，value就是存入的值。来避免线程不安全的问题。 2.如何使用ThreadLocal1234567891011121314151617181920212223242526272829303132333435363738394041424344// ThreadLocal.javapublic void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t);` if (map != null) map.set(this, value); // this:就是当前ThreadLocal else createMap(t, value); // 👇下面创建ThreadLocalMap}👇👇👇👇👇👇👇👇👇👇👇👇void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue);}public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { T result = (T)e.value; return result; } } return setInitialValue(); // 👇}👇👇👇👇👇👇👇👇👇👇👇👇private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { map.set(this, value); } else { createMap(t, value); } if (this instanceof TerminatingThreadLocal) { TerminatingThreadLocal.register((TerminatingThreadLocal&lt;?&gt;) this); } return value;} 来看下ThreadLocalMap的数据结构： 1234567891011121314151617// 可以发现这里用的是弱引用static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; }}//初始容量16private static final int INITIAL_CAPACITY = 16;//散列表private Entry[] table;//entry 有效数量 private int size = 0;//负载因子private int threshold; 参考：Java的引用问题 3.代码演示内存泄漏内存泄漏：有一块空间被占据却不能回收 ThreadLocal运行完业务代码，不释放1Mb的内存，很快就把内存占满了。不过你要是把线程销毁了，这块内存也就回收了。 1234567891011121314151617181920212223public static void main(String[] args) { // -Xms25m -Xmx25m ExcutorService es = Executors.newFixedThreadPool(30); for (int i = 0; i &lt; 30; i++) { es.execute(() -&gt; { doSth(); }) }}public static void doSth() { // 干活的代码 ThreadLocal&lt;MyClass&gt; threadLocal = new ThreadLocal&lt;&gt;(); try { MyClass myClass = new MyClass(); // 👇 threadLocal.set(myClass); } finally { // 解决内存泄漏 threadLocal.remove(); }}public class MyClass { private byte[] bytes = new byte[1024*1024]; //1Mb}","link":"/2022/05/12/12000%E9%9D%A2%E8%AF%95/12003ThreadLocal%E6%BA%90%E7%A0%81/"},{"title":"Java的引用问题","text":"Java的引用问题共4种：强软弱虚1.强引用1List list = new ArrayList(); 2.软引用123// -Xms20M -Xmx20Mnew SoftReference&lt;&gt;(new byte[1024*1024*10]);//10Msystem().gc(); // 并不会把10M内存回收掉 软应用适合缓存使用。堆heap装不下，回收一次，不够把软应用干掉 关联： JAVA四种缓存 3.弱引用ThreadLocal类中的ThreadLocalMap的key就是弱引用，可参考ThreadLocal源码 4.虚引用1new PhantomReference(new M(), Queue); 虚引用回收被放入Queue队列中","link":"/2022/05/13/12000%E9%9D%A2%E8%AF%95/12004Java%E7%9A%84%E5%BC%95%E7%94%A8%E9%97%AE%E9%A2%98/"},{"title":"Spring事务失效6种场景","text":"Spring事务失效6种场景1.方法的访问类型不是public（JDK动态代理无法访问）2.类没有被Spring管理3.抛出的异常被捕获4.方法中的调用同一个类中的方法（因为没有使用代理类）5.事务的传播行为配置错误6.rollbackFor属性配置错误Spring的事务机制基于什么来实现的？事务是通过AOP切面，JDK的动态代理","link":"/2022/05/13/12000%E9%9D%A2%E8%AF%95/12005Spring%E4%BA%8B%E5%8A%A1%E5%A4%B1%E6%95%886%E7%A7%8D%E5%9C%BA%E6%99%AF/"},{"title":"如何查看线程死锁","text":"如何查看线程死锁1.可以通过jstack命令来查看，会显示发生死锁的线程 2.线程操作数据库时，数据库发生了死锁，可以查询死锁情况 123456781.查询是否锁表show OPEN TABLES where In_use &gt; 0;2.查询进程show processlist;3.查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS;4.查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;","link":"/2022/05/16/12000%E9%9D%A2%E8%AF%95/12007%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E7%BA%BF%E7%A8%8B%E6%AD%BB%E9%94%81/"},{"title":"CPU过高如何排查","text":"Linux CPU过高如何排查 引用：https://mp.weixin.qq.com/s/Y2-so8CFfXv5bM4sN4aJSw CPU指标解析 平均负载 如果平均负载大于逻辑 CPU 个数，则负载比较重 进程上下文切换 无法获取资源而导致的自愿上下文切换 被系统强制调度导致的非自愿上下文切换 CPU 使用率 用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙 系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断），系统 CPU 使用率高，说明内核比较繁忙 等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，说明系统与硬件设备的 I/O 交互时间比较长 软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，表明系统发生了大量的中断 查看系统的平均负载123$ uptime当前时间 平均负载 1分钟 5分钟 15分钟23:24 up 7:52, 2 users, load averages: 1.73 2.07 2. 平均负载与 CPU 使用率关系 CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的 I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高 CPU 上下文切换 进程上下文切换： 进程的运行空间可以分为内核空间和用户空间，当代码发生系统调用时（访问受限制的资源），CPU 会发生上下文切换，系统调用结束时，CPU 则再从内核空间换回用户空间。一次系统调用，两次 CPU 上下文切换 系统平时会按一定的策略调用进程，会导致进程上下文切换 进程在阻塞等到访问资源时，也会发生上下文切换 进程通过睡眠函数挂起，会发生上下文切换 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起 线程上下文切换： 同一进程里的线程，它们共享相同的虚拟内存和全局变量资源，线程上下文切换时，这些资源不变 线程自己的私有数据，比如栈和寄存器等，需要在上下文切换时保存切换 中断上下文切换： 为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件 查看系统的上下文切换情况：vmstat 和 pidstat。vmvmstat 可查看系统总体的指标，pidstat则详细到每一个进程服务的指标 12345678910$ vmstat 2 1 procs --------memory--------- --swap-- --io--- -system-- ----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 3498472 315836 3819540 0 0 0 1 2 0 3 1 96 0 0--------cs（context switch）是每秒上下文切换的次数in（interrupt）则是每秒中断的次数r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数.当这个值超过了CPU数目，就会出现CPU瓶颈b（Blocked）则是处于不可中断睡眠状态的进程数 12345678910111213# pidstat -wLinux 3.10.0-862.el7.x86_64 (8f57ec39327b) 07/11/2021 _x86_64_ (6 CPU)06:43:23 PM UID PID cswch/s nvcswch/s Command06:43:23 PM 0 1 0.00 0.00 java06:43:23 PM 0 102 0.00 0.00 bash06:43:23 PM 0 150 0.00 0.00 pidstat------各项指标解析---------------------------PID 进程idCswch/s 每秒主动任务上下文切换数量Nvcswch/s 每秒被动任务上下文切换数量。大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换Command 进程执行命令 怎么排查 CPU 过高问题 先使用 top 命令，查看系统相关指标。如需要按某指标排序则 使用 top -o 字段名 如：top -o %CPU。 -o 可以指定排序字段，顺序从大到小 12345678910111213141516171819202122232425262728293031323334353637383940414243# top -o %MEMtop - 18:20:27 up 26 days, 8:30, 2 users, load average: 0.04, 0.09, 0.13Tasks: 168 total, 1 running, 167 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.3 us, 0.5 sy, 0.0 ni, 99.1 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 stKiB Mem: 32762356 total, 14675196 used, 18087160 free, 884 buffersKiB Swap: 2103292 total, 0 used, 2103292 free. 6580028 cached MemPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2323 mysql 20 0 19.918g 4.538g 9404 S 0.333 14.52 352:51.44 mysqld 1260 root 20 0 7933492 1.173g 14004 S 0.333 3.753 58:20.74 java 1520 daemon 20 0 358140 3980 776 S 0.333 0.012 6:19.55 httpd 1503 root 20 0 69172 2240 1412 S 0.333 0.007 0:48.05 httpd ---------各项指标解析---------------------------------------------------第一行统计信息区 18:20:27 当前时间 up 25 days, 17:29 系统运行时间，格式为时:分 1 user 当前登录用户数 load average: 0.04, 0.09, 0.13 系统负载，三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值Tasks：进程相关信息 running 正在运行的进程数 sleeping 睡眠的进程数 stopped 停止的进程数 zombie 僵尸进程数Cpu(s)：CPU相关信息 %us：表示用户空间程序的cpu使用率（没有通过nice调度） %sy：表示系统空间的cpu使用率，主要是内核程序 %ni：表示用户空间且通过nice调度过的程序的cpu使用率 %id：空闲cpu %wa：cpu运行时在等待io的时间 %hi：cpu处理硬中断的数量 %si：cpu处理软中断的数量Mem 内存信息 total 物理内存总量 used 使用的物理内存总量 free 空闲内存总量 buffers 用作内核缓存的内存量Swap 内存信息 total 交换区总量 used 使用的交换区总量 free 空闲交换区总量 cached 缓冲的交换区总量 找到相关进程后，我们则可以使用 top -Hp pid 或 pidstat -t -p pid 命令查看进程具体线程使用 CPU 情况，从而找到具体的导致 CPU 高的线程 %us 过高，则可以在对应 java 服务根据线程ID查看具体详情，是否存在死循环，或者长时间的阻塞调用。java 服务可以使用 jstack 如果是 %sy 过高，则先使用 strace 定位具体的系统调用，再定位是哪里的应用代码导致的 如果是 %si 过高，则可能是网络问题导致软中断频率飙高 %wa 过高，则是频繁读写磁盘导致的。 linux 内存查看内存使用情况 使用 top 或者 free、vmstat 命令 1234567891011# top top - 18:20:27 up 26 days, 8:30, 2 users, load average: 0.04, 0.09, 0.13Tasks: 168 total, 1 running, 167 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.3 us, 0.5 sy, 0.0 ni, 99.1 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 stKiB Mem: 32762356 total, 14675196 used, 18087160 free, 884 buffersKiB Swap: 2103292 total, 0 used, 2103292 free. 6580028 cached MemPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2323 mysql 20 0 19.918g 4.538g 9404 S 0.333 14.52 352:51.44 mysqld 1260 root 20 0 7933492 1.173g 14004 S 0.333 3.753 58:20.74 java .... bcc-tools 软件包里的 cachestat 和 cachetop、memleak achestat 可查看整个系统缓存的读写命中情况 cachetop 可查看每个进程的缓存命中情况 memleak 可以用检查 C、C++ 程序的内存泄漏问题 free 命令内存指标12345# free -m total used free shared buffers cached Mem: 32107 30414 1692 0 1962 8489 -/+ buffers/cache: 19962 12144 Swap: 0 0 0 shared 是共享内存的大小, 一般系统不会用到，总是0 buffers/cache 是缓存和缓冲区的大小，buffers 是对原始磁盘块的缓存，cache 是从磁盘读取文件系统里文件的页缓存 available 是新进程可用内存的大小 内存 swap 过高Swap 其实就是把一块磁盘空间或者一个本地文件，当成内存来使用。swap 换出，把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。swap 换入，在进程再次访问这些内存的时候，把它们从磁盘读到内存中来 swap 和 内存回收的机制 内存的回收既包括了文件页（内存映射获取磁盘文件的页）又包括了匿名页（进程动态分配的内存） 对文件页的回收，可以直接回收缓存，或者把脏页写回磁盘后再回收 而对匿名页的回收，其实就是通过 Swap 机制，把它们写入磁盘后再释放内存 swap 过高会造成严重的性能问题，页失效会导致频繁的页面在内存和磁盘之间交换 一般线上的服务器的内存都很大，可以禁用 swap 可以设置 /proc/sys/vm/min_free_kbytes，来调整系统定期回收内存的阈值，也可以设置 /proc/sys/vm/swappiness，来调整文件页和匿名页的回收倾向 linux 磁盘I/O 问题文件系统和磁盘 磁盘是一个存储设备（确切地说是块设备），可以被划分为不同的磁盘分区。而在磁盘或者磁盘分区上，还可以再创建文件系统，并挂载到系统的某个目录中。系统就可以通过这个挂载目录来读写文件 磁盘是存储数据的块设备，也是文件系统的载体。所以，文件系统确实还是要通过磁盘，来保证数据的持久化存储 系统在读写普通文件时，I/O 请求会首先经过文件系统，然后由文件系统负责，来与磁盘进行交互。而在读写块设备文件时，会跳过文件系统，直接与磁盘交互 linux 内存里的 Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。内核就可以把分散的写集中起来（优化磁盘的写入） linux 内存里的 Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读写的数据。下次访问这些文件数据时，则直接从内存中快速获取，而不再次访问磁盘 磁盘性能指标 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。 IOPS（Input/Output Per Second），是指每秒的 I/O 请求数 吞吐量，是指每秒的 I/O 请求大小 响应时间，是指 I/O 请求从发出到收到响应的间隔时间 IO 过高怎么找问题，怎么调优 查看系统磁盘整体 I/O 12345678910111213141516171819# iostat -x -k -d 1 1Linux 4.4.73-5-default (ceshi44) 2021年07月08日 _x86_64_ (40 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.08 2.48 0.37 11.71 27.80 507.24 88.53 0.02 1.34 14.96 0.90 0.09 0.10sdb 0.00 1.20 1.28 16.67 30.91 647.83 75.61 0.17 9.51 9.40 9.52 0.32 0.57------ rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并wrqm/s: 每秒对该设备的写请求被合并次数r/s: 每秒完成的读次数w/s: 每秒完成的写次数rkB/s: 每秒读数据量(kB为单位)wkB/s: 每秒写数据量(kB为单位)avgrq-sz: 平均每次IO操作的数据量(扇区数为单位)avgqu-sz: 平均等待处理的IO请求队列长度await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位)svctm: 平均每次IO请求的处理时间(毫秒为单位)%util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 查看进程级别 I/O 1234567891011# pidstat -dLinux 3.10.0-862.el7.x86_64 (8f57ec39327b) 07/11/2021 _x86_64_ (6 CPU)06:42:35 PM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command06:42:35 PM 0 1 1.05 0.00 0.00 java06:42:35 PM 0 102 0.04 0.05 0.00 bash------kB_rd/s 每秒从磁盘读取的KBkB_wr/s 每秒写入磁盘KBkB_ccwr/s 任务取消的写入磁盘的KB。当任务截断脏的pagecache的时候会发生Command 进程执行命令 当使用 pidstat -d 定位到哪个应用服务时，接下来则需要使用 strace 和 lsof 定位是哪些代码在读写磁盘里的哪些文件，导致IO高的原因 123456789101112$ strace -p 18940 strace: Process 18940 attached ...mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f7aee9000 mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f682e8000 write(3, &quot;2018-12-05 15:23:01,709 - __main&quot;..., 314572844 ) = 314572844 munmap(0x7f0f682e8000, 314576896) = 0 write(3, &quot;\\n&quot;, 1) = 1 munmap(0x7f0f7aee9000, 314576896) = 0 close(3) = 0 stat(&quot;/tmp/logtest.txt.1&quot;, {st_mode=S_IFREG|0644, st_size=943718535, ...}) = 0 strace 命令输出可以看到进程18940 正在往文件 /tmp/logtest.txt.1 写入300m 12345678$ lsof -p 18940 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 18940 root cwd DIR 0,50 4096 1549389 / … java 18940 root 2u CHR 136,0 0t0 3 /dev/pts/0 java 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt ----FD 表示文件描述符号，TYPE 表示文件类型，NODE NAME 表示文件路径 lsof 也可以看出进程18940 以每次 300MB 的速度往 /tmp/logtest.txt 写入 linux 网络I/O 问题当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中；然后通过硬中断，告诉中断处理程序已经收到了网络包。接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧。内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧 硬中断：与系统相连的外设(比如网卡、硬盘)自动产生的。主要是用来通知操作系统系统外设状态的变化。比如当网卡收到数据包的时候，就会发出一个硬中断 软中断：为了满足实时系统的要求，中断处理应该是越快越好。linux为了实现这个特点，当中断发生的时候，硬中断处理那些短时间就可以完成的工作，而将那些处理事件比较长的工作，交给软中断来完成 网络I/O指标 带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒） 吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT） PPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响 网络的连通性 并发连接数（TCP 连接数量） 丢包率（丢包百分比） 查看网络I/O指标 查看网络配置 123456789101112131415161718# ifconfig em1em1 Link encap:Ethernet HWaddr 80:18:44:EB:18:98 inet addr:192.168.0.44 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr: fe80::8218:44ff:feeb:1898/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3098067963 errors:0 dropped:5379363 overruns:0 frame:0 TX packets:2804983784 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:1661766458875 (1584783.9 Mb) TX bytes:1356093926505 (1293271.9 Mb) Interrupt:83-----TX 和 RX 部分的 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，通常表示出现了网络 I/O 问题。errors 表示发生错误的数据包数，比如校验错误、帧同步错误等dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等collisions 表示碰撞数据包数 网络吞吐和 PPS 12345678910111213# sar -n DEV 1Linux 4.4.73-5-default (ceshi44) 2022年03月31日 _x86_64_ (40 CPU)15时39分40秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil15时39分41秒 em1 1241.00 1022.00 600.48 590.39 0.00 0.00 165.00 0.4915时39分41秒 lo 636.00 636.00 7734.06 7734.06 0.00 0.00 0.00 0.0015时39分41秒 em4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0015时39分41秒 em3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0015时39分41秒 em2 26.00 20.00 6.63 8.80 0.00 0.00 0.00 0.01----rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒 宽带 12# ethtool em1 | grep Speed Speed: 1000Mb/s 连通性和延迟 123456# ping www.baidu.comPING www.a.shifen.com (14.215.177.38) 56(84) bytes of data.64 bytes from 14.215.177.38: icmp_seq=1 ttl=56 time=53.9 ms64 bytes from 14.215.177.38: icmp_seq=2 ttl=56 time=52.3 ms64 bytes from 14.215.177.38: icmp_seq=3 ttl=56 time=53.8 ms64 bytes from 14.215.177.38: icmp_seq=4 ttl=56 time=56.0 ms 统计 TCP 连接状态工具 ss 和 netstat 12345678910111213[root@root ~]$&gt;#ss -ant | awk '{++S[$1]} END {for(a in S) print a, S[a]}'LISTEN 96CLOSE-WAIT 527ESTAB 8520State 1SYN-SENT 2TIME-WAIT 660[root@root ~]$&gt;#netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'CLOSE_WAIT 530ESTABLISHED 8511FIN_WAIT2 3TIME_WAIT 809 网络请求变慢，怎么调优 高并发下 TCP 请求变多，会有大量处于 TIME_WAIT 状态的连接，它们会占用大量内存和端口资源。此时可以优化与 TIME_WAIT 状态相关的内核选项 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用到新建的连接中 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整体的并发能力 增加最大文件描述符的数量。可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和系统的最大文件描述符数 SYN FLOOD 攻击，利用 TCP 协议特点进行攻击而引发的性能问题，可以考虑优化与 SYN 状态相关的内核选项 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries 加快 TCP 长连接的回收，优化与 Keepalive 相关的内核选项 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time 缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数 net.ipv4.tcp_keepalive_probes java 应用内存泄漏和频繁 GC区分内存溢出、内存泄漏、内存逃逸 内存泄漏：内存被申请后始终无法释放，导致内存无法被回收使用，造成内存空间浪费 内存溢出：指内存申请时，内存空间不足 1-内存上限过小 2-内存加载数据太多 3-分配太多内存没有回收，出现内存泄漏 内存逃逸：是指程序运行时的数据，本应在栈上分配，但需要在堆上分配，称为内存逃逸 java中的对象都是在堆上分配的，而垃圾回收机制会回收堆中不再使用的对象，但是筛选可回收对象，回收对象还有整理内存都需要消耗时间。如果能够通过逃逸分析确定对象不会逃出方法之外，那就可以让这个对象在栈上分配内存，对象所占用的内存就可以随栈帧出栈而销毁，就减轻了垃圾回收的压力 线程同步本身比较耗时，如果确定一个变量不会逃逸出线程，无法被其它线程访问到，那这个变量的读写就不会存在竞争，对这个变量的同步措施可以清除 java 虚拟机中的原始数据类型(int，long及reference类型等) 都不能再进一步分解，它们称为标量。如果一个数据可以继续分解，那它称为聚合量，java 中最典型的聚合量是对象。如果逃逸分析证明一个对象不会被外部访问，并且这个对象是可分解的，那程序运行时可能不创建该对象，而改为直接创建它的若干个被方法使用到的成员变量来代替。拆散后的变量便可以被单独分析与优化，可以各自分别在栈帧或寄存器上分配空间，原本的对象就无需整体分配空间 内存泄漏，该如何定位和处理 使用 jmap -histo:live [pid] 和 jmap -dump:format=b,file=filename [pid] 前者可以统计堆内存对象大小和数量，后者可以把堆内存 dump 下来 启动参数中指定-XX:+HeapDumpOnOutOfMemoryError来保存OOM时的dump文件 使用 JProfiler 或者 MAT 软件查看 heap 内存对象，可以更直观地发现泄露的对象 java线程问题排查java 线程状态 NEW：对应没有 Started 的线程，对应新生态 RUNNABLE：对于就绪态和运行态的合称 BLOCKED，WAITING，TIMED_WAITING：三个都是阻塞态 sleep 和 join 称为WAITING sleep 和 join 方法设置了超时时间的，则是 TIMED_WAITING wait 和 IO 流阻塞称为BLOCKED TERMINATED：死亡态 线程出现死锁或者被阻塞 jstack –l pid | grep -i –E 'BLOCKED | deadlock' ， 加上参数 -l，jstack 命令可以快速打印出造成死锁的代码 123456789101112131415161718192021222324252627282930313233343536373839404142# jstack -l 28764Full thread dump Java HotSpot(TM) 64-Bit Server VM (13.0.2+8 mixed mode, sharing):.....&quot;Thread-0&quot; #14 prio=5 os_prio=0 cpu=0.00ms elapsed=598.37s tid=0x000001b3c25f7000 nid=0x4abc waiting for monitor entry [0x00000061661fe000] java.lang.Thread.State: BLOCKED (on object monitor) at com.Test$DieLock.run(Test.java:52) - waiting to lock &lt;0x0000000712d7c230&gt; (a java.lang.Object) - locked &lt;0x0000000712d7c220&gt; (a java.lang.Object) at java.lang.Thread.run(java.base@13.0.2/Thread.java:830) Locked ownable synchronizers: - None&quot;Thread-1&quot; #15 prio=5 os_prio=0 cpu=0.00ms elapsed=598.37s tid=0x000001b3c25f8000 nid=0x1984 waiting for monitor entry [0x00000061662ff000] java.lang.Thread.State: BLOCKED (on object monitor) at com.Test$DieLock.run(Test.java:63) - waiting to lock &lt;0x0000000712d7c220&gt; (a java.lang.Object) - locked &lt;0x0000000712d7c230&gt; (a java.lang.Object) at java.lang.Thread.run(java.base@13.0.2/Thread.java:830).....Found one Java-level deadlock:=============================&quot;Thread-0&quot;: waiting to lock monitor 0x000001b3c1e4c480 (object 0x0000000712d7c230, a java.lang.Object), which is held by &quot;Thread-1&quot;&quot;Thread-1&quot;: waiting to lock monitor 0x000001b3c1e4c080 (object 0x0000000712d7c220, a java.lang.Object), which is held by &quot;Thread-0&quot;Java stack information for the threads listed above:===================================================&quot;Thread-0&quot;: at com.Test$DieLock.run(Test.java:52) - waiting to lock &lt;0x0000000712d7c230&gt; (a java.lang.Object) - locked &lt;0x0000000712d7c220&gt; (a java.lang.Object) at java.lang.Thread.run(java.base@13.0.2/Thread.java:830)&quot;Thread-1&quot;: at com.Test$DieLock.run(Test.java:63) - waiting to lock &lt;0x0000000712d7c220&gt; (a java.lang.Object) - locked &lt;0x0000000712d7c230&gt; (a java.lang.Object) at java.lang.Thread.run(java.base@13.0.2/Thread.java:830)Found 1 deadlock. 从 jstack 输出的日志可以看出线程阻塞在 Test.java:52 行，发生了死锁 常用 jvm 调优启动参数 -verbose:gc 输出每次GC的相关情况 -verbose:jni 输出native方法调用的相关情况，一般用于诊断jni调用错误信息 -Xms n 指定jvm堆的初始大小，默认为物理内存的1/64，最小为1M；可以指定单位，比如k、m，若不指定，则默认为字节 -Xmx n 指定jvm堆的最大值，默认为物理内存的1/4或者1G，最小为2M；单位与-Xms一致 -Xss n 设置单个线程栈的大小，一般默认为512k -XX:NewRatio=4 设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -Xmn 设置新生代内存大小。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8 -XX:SurvivorRatio=4 设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6 -XX:MaxTenuringThreshold=0 设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率","link":"/2022/05/15/12000%E9%9D%A2%E8%AF%95/12006CPU%E8%BF%87%E9%AB%98%E5%A6%82%E4%BD%95%E6%8E%92%E6%9F%A5/"},{"title":"","text":"[toc] 1、自我介绍，项目介绍，遇到的难点？产生原因？如何解决？ 2、HashMap1.8与1.7区别？ConcurrentHashMap实现原理 ？HashMap 组成差别1.7:数组+单链表1.8:数据+单链表+红黑树链表存放差别：出现哈希冲突时：1.7直接把数据存放在链表，再无其它操作1.8把数据存放在链表，链表长度超过8就转红黑树扩容差别：1.7扩容条件是数组大于阈值且存在哈希冲突时扩容1.8扩容条件是数组长度大于阈值或链表转红黑树时且数组元素小于64时扩容插值方法：1.7用的是头插法，(在链表头部插入新值)，弊端：可能造成逆序死循环1.8用的是尾插法可避免上面的问题 ConcurrentHashMap采用了非常精妙的”分段锁”策略 1234567891011121314151617final Segment&lt;K,V&gt;[] segments;static class Segment&lt;K,V&gt; extends ReentrantLock;// 一个Segment维护着一个HashEntry数组transient volatile HashEntry&lt;K,V&gt;[] table;public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) //MAX_SEGMENTS 为1&lt;&lt;16=65536，也就是最大并发数为65536 if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; //2的sshif次方等于ssize，例:ssize=16,sshift=4;ssize=32,sshif=5 int sshift = 0; //ssize 为segments数组长度，根据concurrentLevel计算得出 int ssize = 1; while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } Segment数组的大小ssize是由concurrentLevel来决定的，但是却不一定等于concurrentLevel，ssize一定是大于或等于concurrentLevel的最小的2的次幂。比如：默认情况下concurrentLevel是16，则ssize为16；若concurrentLevel为14，ssize为16；若concurrentLevel为17，则ssize为32。 3、jvm类加载器，自定义类加载器，双亲委派机制，优缺点，tomcat类加载机制?避免类的重复加载， 确保一个类的全局唯一性Java 类随着它的类加载器一起具备了一种带有优先级的层级关系， 通过这种层级关系可以避免类的重复加载， 当父亲已经加载了该类时， 就没有必要子ClassLoader 再加载一次 Tomcat各个web应用自己的类加载器(WebAppClassLoader)会优先加载，加载不到时再交给commonClassLoader走双亲委托。 123456static class ExtClassLoader extends URLClassLoader{ ... ...}static class AppClassLoader extends URLClassLoader{ ... ...} 二者同时继承了 URLClassLoader ，继承关系如下： Tomcat 自己实现了自己的类加载器 WebAppClassLoader。 先在本地cache查找该类是否已经加载过，看看 Tomcat 有没有加载过这个类。 如果Tomcat 没有加载过这个类，则从系统类加载器的cache中查找是否加载过。 如果没有加载过这个类，尝试用ExtClassLoader类加载器类加载，重点来了，这里并没有首先使用 AppClassLoader 来加载类。这个Tomcat 的 WebAPPClassLoader 违背了双亲委派机制，直接使用了 ExtClassLoader来加载类。这里注意 ExtClassLoader 双亲委派依然有效，ExtClassLoader 就会使用 Bootstrap ClassLoader 来对类进行加载，保证了 Jre 里面的核心类不会被重复加载。 比如在 Web 中加载一个 Object 类。WebAppClassLoader → ExtClassLoader → Bootstrap ClassLoader，这个加载链，就保证了 Object 不会被重复加载。 如果 BoostrapClassLoader，没有加载成功，就会调用自己的 findClass 方法由自己来对类进行加载，findClass 加载类的地址是自己本 web 应用下的 class。 加载依然失败，才使用 AppClassLoader 继续加载。 都没有加载成功的话，抛出异常。 总结一下以上步骤，WebAppClassLoader 加载类的时候，故意打破了JVM 双亲委派机制，绕开了 AppClassLoader，直接先使用 ExtClassLoader 来加载类。 保证了基础类不会被同时加载。 又保证了在同一个 Tomcat 下不同 web 之间的 class 是相互隔离的。 5、cms收集器过程，g1收集器原理，怎么实现可预测停顿的，region的大小结构？CMS 处理过程有七个步骤： 初始标记(CMS-initial-mark) ,会导致stw; 并发标记(CMS-concurrent-mark)，与用户线程同时运行； 预清理（CMS-concurrent-preclean），与用户线程同时运行； 可被终止的预清理（CMS-concurrent-abortable-preclean） 与用户线程同时运行； 重新标记(CMS-remark) ，会导致swt； 并发清除(CMS-concurrent-sweep)，与用户线程同时运行； 并发重置状态等待下次CMS的触发(CMS-concurrent-reset)，与用户线程同时运行； 其运行流程图如下所示： 6、内存溢出，内存泄漏遇到过吗？什么场景产生的，怎么解决的？引起内存溢出的原因有很多种，常见的有以下几种： 1.内存中加载的数据量过于庞大，如一次从数据库取出过多数据； 2.集合类中有对对象的引用，使用完后未清空，使得JVM不能回收； 3.代码中存在死循环或循环产生过多重复的对象实体； 4.使用的第三方软件中的BUG； 5.启动参数内存值设定的过小； 【情况一】：java.lang.OutOfMemoryError:Javaheapspace：这种是java堆内存不够，一个原因是真不够（如递归的层数太多等），另一个原因是程序中有死循环； 如果是java堆内存不够的话，可以通过调整JVM下面的配置来解决： -Xms3062m -Xmx3062m 【情况二】java.lang.OutOfMemoryError:GCoverheadlimitexceeded 【解释】：JDK6新增错误类型，当GC为释放很小空间占用大量时间时抛出；一般是因为堆太小，导致异常的原因，没有足够的内存。 【解决方案】： 1、查看系统是否有使用大内存的代码或死循环； 2、通过添加JVM配置，来限制使用内存： -XX:-UseGCOverheadLimit 【情况五】： java.lang.OutOfMemoryError:unabletocreatenewnativethread 【原因】：Stack空间不足以创建额外的线程，要么是创建的线程过多，要么是Stack空间确实小了。 【解决】：由于JVM没有提供参数设置总的stack空间大小，但可以设置单个线程栈的大小；而系统的用户空间一共是3G， 除了Text/Data/BSS/MemoryMapping几个段之外，Heap和Stack空间的总量有限，是此消彼长的。因此遇到这个错误， 可以通过两个途径解决：1.通过-Xss启动参数减少单个线程栈大小，这样便能开更多线程（当然不能太小，太小会出现StackOverflowError）； 2.通过-Xms-Xmx两参数减少Heap大小，将内存让给Stack（前提是保证Heap空间够用）。 【情况六】： java.lang.StackOverflowError 【原因】：这也内存溢出错误的一种，即线程栈的溢出，要么是方法调用层次过多（比如存在无限递归调用），要么是线程栈太小。 【解决】：优化程序设计，减少方法调用层次；调整-Xss参数增加线程栈大小 Java都采用了“可达性分析”算法来进行内存回收，原理是：会有几个引用作为root节点，对于任意对象来说，如果从root层层遍历，如果找不到对于他的引用链，那么这个对象就被标记为无用，就会在gc时被销毁。 7、volatile的原理？synchronized和重入锁实现原理以及区别？volatile保证可见性、防止指令重排，不保证原子性。在JVM底层volatile是采用“内存屏障”来实现的。 8、redis字符串实现，sds和c字符串区别？1、在求长度的时候 C字符串 O(n) SDS只需要访问len属性即可 时间复杂度O(1).2、缓冲区溢出问题 C字符串会修改与它相邻 SDS 这里会先根据空间是否够用,实际空间长度为 free + len + 13、字符串内存分配 SDS 内部使用两种机制 惰性空间释放跟空间预分配空间预分配：这里SDS&lt;1M的时候是 free = len,若SDS=6byte 则空间为 6byte + 6byte + 1byte大于1M的时候free = 1M, 若SDS长度为60M 则实际空间为 60M + 1M + 1byte惰性空间释放 不立即使用内存重新分配来回收缩短后的字节，而是通过free记录起来，以供后续使用，SDS也提供了相应的API，防止惰性空间导致内存浪费。 9、redis集群，为什么是16384个slot？选举过程，会有脑裂问题么，raft算法，优缺点？(1)如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。如上所述，在消息头中，最占空间的是myslots[CLUSTER_SLOTS/8]。当槽位为65536时，这块的大小是:65536÷8÷1024=8kb因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。(2)redis的集群主节点数量基本不可能超过1000个。如上所述，集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。(3)槽位越小，节点少的情况下，压缩比高Redis主节点的配置信息中，它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中，会对bitmap进行压缩，但是如果bitmap的填充率slots / N很高的话(N表示节点数)，bitmap的压缩率就很低。如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。 10、redis有序集合怎么实现的，跳表是什么？往跳表添加一个元素的过程获取分数的时间复杂度，为什么不用红黑树，红黑树有什么特点，左旋右旋操作？1234567# Redis使用了两种数据结构来共同实现有序集合typedef struct zset{ //跳跃表 zskiplist *zsl; // 范围操作 查找O(logN) //字典 dict *dice; // 无序保存元素 查找O(1) } zset; 当有序集合对象同时满足以下两个条件时，对象使用 ziplist 编码：1、保存的元素数量小于128；2、保存的所有元素长度都小于64字节。 不能满足上面两个条件的使用 skiplist 编码。以上两个条件也可以通过Redis配置文件zset-max-ziplist-entries 选项和 zset-max-ziplist-value 进行修改。 假设我们要插入的结点是10，首先我们按照跳表查找结点的方法，找到待插入结点的前置结点（仅小于待插入结点）： 接下来，按照一般链表的插入方式，把结点10插入到结点9的下一个位置： 这样是不是插入工作就完成了呢？并不是。随着原始链表的新结点越来越多，索引会渐渐变得不够用了，因此索引结点也需要相应作出调整。 如何调整索引呢？我们让新插入的结点随机“晋升”，也就是成为索引结点。新结点晋升成功的几率是50%。 假设第一次随机的结果是晋升成功，那么我们把结点10作为索引结点，插入到第1层索引的对应位置，并且向下指向原始链表的结点10： 新结点在成功晋升之后，仍然有机会继续向上一层索引晋升。我们再进行一次随机，假设随机的结果是晋升失败，那么插入操作就告一段落了。 小灰说的是什么意思呢？让我们看看下图， 新结点10已经晋升到第2层索引，下一次随机的结果仍然是晋升成功，这时候该怎么办呢？ Redis之所以使用跳表而不使用红黑树原因如下： 1.实现简单，相对于红黑树来说，实现更加的简单，不容易出错，代码更加容易维护和调试。 2.跳表的底层节点有都是通过双向指针相互链接，这和B+树一样，对于范围查找会更加的方便。 3.跳表的效率和红黑树一样，查找单个Key时间复杂度都是O(logn) 4.跳表更加灵活，可以通过改变索引构建策略，有效的平衡执行效率和内存消耗。 zrangebyscorezrevrangebyscore O(log(n)+k)，k为要获取成员个数，n为当前成员个数 zadd O(k*log(n))，k为添加 成员个数，n为当前成员个数 11、锁升级过程，轻量锁可以变成偏向锁么？偏向锁可以变成无锁么？对象头结构，锁状态变化过程？锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率. 如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁恢复到标准的轻量级锁。偏向锁通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。一旦有第二个线程加入锁竞争，偏向锁就升级为轻量级锁（自旋锁）。升级为轻量级锁的时候需要撤销偏向锁，撤销偏向锁的时候会导致STW(stop the word)操作； 锁竞争：如果多个线程轮流获取一个锁，但是每次获取锁的时候都很顺利，没有发生阻塞，那么就不存在锁竞争。只有当某线程尝试获取锁的时候，发现该锁已经被占用，只能等待其释放，这才发生了锁竞争。 轻量级锁（自旋锁） 自旋锁：自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。 在轻量级锁状态下继续锁竞争，没有抢到锁的线程将自旋，即不停地循环判断锁是否能够被成功获取。长时间的自旋操作是非常消耗资源的，一个线程持有锁，其他线程就只能在原地空耗CPU，执行不了任何有效的任务，这种现象叫做忙等（busy-waiting）。如果锁竞争情况严重，某个达到最大自旋次数的线程，会将轻量级锁升级为重量级锁。 12、Innodb的结构了解么？磁盘页和缓存区是怎么配合的？缓冲区和磁盘数据不一致怎么办，服务器突然宕机了数据会丢失么？MySQL底层架构，涉及到： 内存结构：buffer pool、log buffer、change buffer，buffer pool的页淘汰机制是怎样的； 磁盘结构：系统表空间、独立表空间、通用表空间、undo表空间、redo log； 以及IO相关底层原理、查询SQL执行流程、数据页结构和行结构描述、聚集索引和辅助索引的底层数据组织方式、MVCC多版本并发控制的底层实现原理，以及可重复读、读已提交是怎么通过MVCC实现的。 13、InnoDB 索引为什使用B+树而不是用B树？B+树只有叶节点存放数据，其余节点用来索引，而B-树是每个索引节点都会有Data域。所以从Mysql（Inoodb）的角度来看，B+树是用来充当索引的，一般来说索引非常大，尤其是关系性数据库这种数据量大的索引能达到亿级别，所以为了减少内存的占用，索引也会被存储在磁盘上。 那么Mysql如何衡量查询效率呢？– 磁盘IO次数。 B-树/B+树 的特点就是每层节点数目非常多，层数很少，目的就是为了就少磁盘IO次数，但是B-树的每个节点都有data域（指针），这无疑增大了节点大小，说白了增加了磁盘IO次数（磁盘IO一次读出的数据量大小是固定的，单个数据变大，每次读出的就少，IO次数增多，一次IO多耗时），而B+树除了叶子节点其它节点并不存储数据，节点小，磁盘IO次数就少。这是优点之一。另一个优点是： B+树所有的Data域在叶子节点，一般来说都会进行一个优化，就是将所有的叶子节点用指针串起来。这样遍历叶子节点就能获得全部数据，这样就能进行区间访问啦。在数据库中基于范围的查询是非常频繁的，而B树不支持这样的遍历操作。 B树相对于红黑树的区别 AVL 数和红黑树基本都是存储在内存中才会使用的数据结构。在大规模数据存储的时候，红黑树往往出现由于树的深度过大而造成磁盘IO读写过于频繁，进而导致效率低下的情况。为什么会出现这样的情况，我们知道要获取磁盘上数据，必须先通过磁盘移动臂移动到数据所在的柱面，然后找到指定盘面，接着旋转盘面找到数据所在的磁道，最后对数据进行读写。磁盘IO代价主要花费在查找所需的柱面上，树的深度过大会造成磁盘IO频繁读写。根据磁盘查找存取的次数往往由树的高度所决定，所以，只要我们通过某种较好的树结构减少树的结构尽量减少树的高度，B树可以有多个子女，从几十到上千，可以降低树的高度。 数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 14、MySQL 分表是怎么实现的？跨库join如何解决？数据量突增怎么解决？15、数据库的隔离级别，怎么实现的？当前读，快照读？MVCC？16、mysql优化的实践经验17、分布式事务出现过不一致吗？为什么？怎么解决？有什么方法避免？怎么监控？监控到怎么处理？什么时候需要人工接入？18、io模型了解么？多路复用？selete，poll，epoll，epoll的结构？怎么注册事件？19、你们用的什么消息中间件，kafka，为什么用kafka？kafka是怎么保证高吞吐量的？kafka是怎么保证高吞吐量的 1.顺序读写 kafka的消息是不断追加到文件中的，这个特性使kafka可以充分利用磁盘的顺序读写性能。顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写。生产者负责写入数据，Kafka会将消息持久化到磁盘，保证不会丢失数据，Kafka采用了俩个技术提高写入的速度。1.顺序写入：如果是随机IO，磁盘会进行频繁的寻址，导致写入速度下降。Kafka使用了顺序IO提高了磁盘的写入速度，Kafka会将数据顺序插入到文件末尾，消费者端通过控制偏移量来读取消息，这样做会导致数据无法删除，时间一长，磁盘空间会满，kafka提供了2种策略来删除数据：基于时间删除和基于partition文件的大小删除。2.Memory Mapped Files：这个和Java NIO中的内存映射基本相同，在大学的计算机原理里我们学过（划重点），mmf直接利用操作系统的Page来实现文件到物理内存的映射，完成之后对物理内存的操作会直接同步到硬盘。mmf通过内存映射的方式大大提高了IO速率，省去了用户空间到内核空间的复制。它的缺点显而易见–不可靠，当发生宕机而数据未同步到硬盘时，数据会丢失，Kafka提供了produce.type参数来控制是否主动的进行刷新，如果kafka写入到mmp后立即flush再返回给生产者则为同步模式，反之为异步模式。 2.零拷贝 在这之前先来了解一下零拷贝(直接让操作系统的 Cache 中的数据发送到网卡后传输给下游的消费者)：平时从服务器读取静态文件时，服务器先将文件从复制到内核空间，再复制到用户空间，最后再复制到内核空间并通过网卡发送出去，而零拷贝则是直接从内核到内核再到网卡，省去了用户空间的复制。Kafka把所有的消息存放到一个文件中，当消费者需要数据的时候直接将文件发送给消费者，比如10W的消息共10M，全部发送给消费者，10M的消息在内网中传输是非常快的，假如需要1s，那么kafka的tps就是10w。Zero copy对应的是Linux中sendfile函数，这个函数会接受一个offsize来确定从哪里开始读取。现实中，不可能将整个文件全部发给消费者，他通过消费者传递过来的偏移量来使用零拷贝读取指定内容的数据返回给消费者。 在Linux kernel2.2 之后出现了一种叫做”零拷贝(zero-copy)”系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”，系统上下文切换减少为2次，可以提升一倍的性能。 3.分区 kafka中的topic中的内容可以被分为多分partition存在,每个partition又分为多个段segment,所以每次操作都是针对一小部分做操作，很轻便，并且增加并行操作的能力 4.批量发送 kafka允许进行批量发送消息，producter发送消息的时候，可以将消息缓存在本地,等到了固定条件发送到kafka 等消息条数到固定条数 一段时间发送一次 5.数据压缩 Kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩。压缩的好处就是减少传输的数据量，减轻对网络传输的压力。 Producer压缩之后，在Consumer需进行解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得*批量发送和数据压缩一起使用,单条做数据压缩的话，效果不明显* Kafka的设计目标是高吞吐量，它比其它消息系统快的原因体现在以下几方面： 1、Kafka操作的是序列文件I / O（序列文件的特征是按顺序写，按顺序读），为保证顺序，Kafka强制点对点的按顺序传递消息，这意味着，一个consumer在消息流（或分区）中只有一个位置。 2、Kafka不保存消息的状态，即消息是否被“消费”。一般的消息系统需要保存消息的状态，并且还需要以随机访问的形式更新消息的状态。而Kafka 的做法是保存Consumer在Topic分区中的位置offset，在offset之前的消息是已被“消费”的，在offset之后则为未“消费”的，并且offset是可以任意移动的，这样就消除了大部分的随机IO。 3、Kafka支持点对点的批量消息传递。 4、Kafka的消息存储在OS pagecache（页缓存，page cache的大小为一页，通常为4K，在Linux读写文件时，它用于缓存文件的逻辑内容，从而加快对磁盘上映像和数据的访问）。 20、kafka重平衡，重启服务怎么保证kafka不发生重平衡，有什么方案？21、netty的原理和使用？tcp的连接过程？一台服务器能支持多少连接，为什么 ？tcp各个参数怎么设置？服务端 我们现在在来回头考虑服务器端。对于服务器来说，最大支持的并发连接是多少呢？就有人开始可爱地糊涂了：“服务器端理论也是端口限制吗？”。好，假设如果受影响的话，那我们的Nginx服务器只监听了一个80端口。那Nginx只能接受一个TCP连接喽？这明显是太荒唐了。 好，我们再看另外一个靠谱一点的答案。那就是一条TCP连接是由一个四元组组成的。不考虑地址重用（unix的SO_REUSEADDR选项）的情况下，对于我们这台Nginx Server来说，它的IP和端口是固定的。cp连接4元组中只有remote ip（也就是client ip）和remote port（客户端port）是可变的。它可能建立的最大的连接数是2的32次方（ip数）×2的16次方（port数）。这是2.8*10的14次方的一个大数字，两百万亿！！ Linux上除了监听80以外，还可以监听其它的端口，例如Mysql的3306, Redis的6339，当然所有65535个端口你都可以用来监听一遍。这样理论上线就到了2的32次方（ip数）×2的16次方（port数）×2的16次方（服务器port数）个。感兴趣你可以算一下，这个基本相当于无穷个了。 不过理想和实际总是会有差距的，因为Linux每维护一条TCP连接都要花费资源。处理连接请求，保活，数据的收发时需要消耗一些CPU，维持TCP连接主要消耗内存。我们题目的问题是考虑最大多少个连接，所以我们先不考虑数据的收发。那么TCP在静止的状态下，就不怎么消耗CPU了，主要消耗内存。而Linux上内存是有限的。我们今天先直接把结论抛出来，一条TCP连接如果不发送数据的话，消耗内存是3.3K左右。如果有数据发送，需要为每条TCP分配发送缓存区，大小受你的参数net.ipv4.tcp_wmem配置影响，默认情况下最小是4K。如果发送结束，缓存区消耗的内存会被回收详细的分析过程敬请期待接下来的另一篇文章。 假设你只保持连接不发送数据，那么你服务器可以建立的连接最大数量 = 你的内存/3.3K。 假如是4GB的内存，那么大约可接受的TCP连接数量是100万左右。 这个例子里，我们考虑的前提是在一个进程下hold所有的服务器端连接。而在实际中的项目里，为了收发数据方便，很多网络IO模型还会为TCP连接再创建一个线程或协程。拿最轻量的golang来说，一个协程栈也需要2KB的内存开销。 结论 一台机器最大究竟能支持多少个网络连接？这个简单的问题里其实埋了坑，导致无数的英雄好汉被困惑不解。就和树上九只鸟打死一只还剩几只的问题一样，没有和你说清楚树上是真鸟，还是假鸟。也没有说枪是有声还是无声的。通过今天的分析，相信你终于可以扬眉吐气把这个问题踩在脚下摩擦了。来，总结下： TCP连接的客户端机：每一个ip可建立的TCP连接理论受限于内核net.ip_local_port_range参数，也受限于65535。但可以通过配置多ip的方式来加大自己的建立连接的能力。 TCP连接的服务器机：每一个监听的端口虽然理论值很大，但这个数字没有实际意义。最大并发数取决你的内存大小，每一条静止状态的TCP连接大约需要吃3.3K的内存。 22、Sping的AOP实现原理，以及对象生成方式的种类，单例的还是原型的？23、讲讲调度接口是怎么实现的Timer 的设计核心是一个 TaskQueue 和一个 TimerThread。Timer 将接收到的任务丢到自己的 TaskQueue中。TimerThread 在创建 Timer 时会启动成为一个守护线程。这个线程会轮询所有任务，找到一个最近要执行的任务，然后休眠，当到达最近要执行任务的开始时间点，TimerThread 被唤醒并执行该任务。之后 TimerThread 更新最近一个要执行的任务，继续休眠。 24、分布式唯一ID是怎么实现的25、设计模式，以及自己使用的场景26、有没有用过分布式锁，怎么实现的，讲讲原理27、如何解决线上问题？cpu狂飙怎么办？频繁minor gc怎么办？可能造成的原因是什么？如何避免？28、怎么理解分布式和微服务，为什么要拆分服务，会产生什么问题，怎么解决这些问题 ？面试题剖析 为什么要将系统进行拆分？ 网上查查，答案极度零散和复杂，很琐碎，原因一大坨。但是我这里给大家直观的感受： 1.代码量大，容易冲突，合并非常耗费时间2.不敢随意乱改技术。 拆分了以后，每个人维护自己的服务就可以了。技术上想怎么升级就怎么升级，大幅度提升复杂系统大型团队的开发效率 拆分后不用 dubbo 可以吗？ 当然可以了，大不了最次，就是各个系统之间，直接基于 spring mvc，就纯 http 接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为 http 接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了 5 台机器，你怎么把请求均匀地甩给那 5 台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。 所以 dubbo 说白了，是一种 rpc 框架，就是说本地就是进行接口调用，但是 dubbo 会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡了、服务实例上下线自动感知了、超时重试了，等等乱七八糟的问题。那你就不用自己做了，用 dubbo 就可以了。 29、怎么理解高可用，如何保证高可用，有什么弊端，熔断机制，怎么实现 ？30、对于高并发怎么看，怎么算高并发，你们项目有么，如果有会产生什么问题，怎么解决31、有没有做过压测的项目？首页接口优化是怎么做的？32、如何优雅的写代码？什么代码算做优雅？什么代码是规范？你们代码规范是什么样的？如何进行code review？33、算法：给定一个长度为N的整形数组arr，其中有N个互不相等的自然数1-N，请实现arr的排序，但是不要把下标0∼N−1位置上的数通过直接赋值的方式替换成1∼N34、算法：判断一个树是否是平衡二叉树35、算法：给定一个二叉树，请计算节点值之和最大的路径的节点值之和是多少，这个路径的开始节点和结束节点可以是二叉树中的任意节点36、算法：LRU 缓存37、算法：实现带有getMin功能的栈，要求push，pop，getMin的时间复杂度都是O(1)38、算法：两数之和39、算法：实现二叉树先序，中序和后序遍历","link":"/2021/07/28/12000%E9%9D%A2%E8%AF%95/2011%E9%A2%981/"},{"title":"线程之间如何进行通讯","text":"线程之间如何进行通讯1.线程之间可以通过共享内存 - 1.1如果是通过共享内存来进行通信，需要考虑并发问题 - 1.2Java中Object阻塞和唤醒的`wait() notify()` 2.基于网络来进行通信 ​ 并发时，通过加锁保证线程安全","link":"/2022/05/16/12000%E9%9D%A2%E8%AF%95/12008%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E9%80%9A%E8%AE%AF/"},{"title":"","text":"[toc] 异步与非阻塞异步是目的. 异步线程中 rpc调用， 网络io多 设置线程数 2n 同步方式读取外部服务时，首先主线程会从用户模式进入到内核模式，在内核模式中windows会将你的请求数据交给对应的网络驱动程序，继后会让这个线程进入休眠状态 异步方式就是为了解放主线程，步骤三中将thread数据交给网络驱动程序之后，该thread就直接返回不管了，当后续网络驱动程序获取数据后，将数据丢给CLR线程池中的IO线程再由它触发你的回调函数。 多线程优先级子线程默认优先级和父线程一样，Java 主线程默认的优先级是 5。Java 优先级范围是 [1, 10] 注解开发 内置tomcat jar包运行 自动化配置的效果： spI机制 排除一些依赖包： exclude MyBatis # $的区别#防止sql注入 $不做转义，传字段、表名 会出现问题，系统会试图把’A’转成数字，改为 ; 结果集 对一个字段加密： 插件机制 一二级缓存：数据一致性问题 hibnate NoSql mongodb优势：①弱一致性（最终一致），更能保证用户的访问速度：举例来说，在 传统的关系型数据库中，一个COUNT类型的操作会锁定数据集，这样可以保证得到“当前”情况下的精确值。这在某些情况下，例如通过ATM查看账户信息的 时候很重要，但对于Wordnik来说，数据是不断更新和增长的，这种“精确”的保证几乎没有任何意义，反而会产生很大的延迟。他们需要的是一个“大约” 的数字以及更快的处理速度。 但某些情况下MongoDB会锁住数据库。如果此时正有数百个请求，则它们会堆积起来，造成许多问题。我们使用 了下面的优化方式来避免锁定：每次更新前，我们会先查询记录。查询操作会将对象放入内存，于是更新则会尽可能的迅速。在主/从部署方案中，从节点可以使用 “-pretouch”参数运行，这也可以得到相同的效果。使用多个mongod进程。我们根据访问模式将数据库拆分成多个进程。 ②文档结构的存储方式，能够更便捷的获取数据。对于一个层级式的数据结构来说，如果要将这样的数据使用扁平式的，表状的结构来保存数据，这无论是在查询还是获取数据时都十分困难。 ③内置GridFS，支持大容量的存储。GridFS是一个出色的分布式文件系统，可以支持海量的数据存储。内置了GridFS了MongoDB，能够满足对大数据集的快速范围查询。 ④内置Sharding。提供基于Range的AutoSharding机制：一个collection可按照记录的范围，分成若干个段，切分到不同的Shard上。 Shards可以和复制结合，配合Replicasets能够实现Sharding+fail-over，不同的Shard之间可以负载均衡。查询是对客 户端是透明的。客户端执行查询，统计，MapReduce等操作，这些会被MongoDB自动路由到后端的数据节点。这让我们关注于自己的业务，适当的时 候可以无痛的升级。MongoDB的Sharding设计能力最大可支持约20petabytes，足以支撑一般应用。 这可以保证MongoDB运行在便宜的PC服务器集群上。PC集群扩充起来非常方便并且成本很低，避免了“sharding”操作的复杂性和成本。 ⑤第三方支持丰富。(这是与其他的NoSQL相比，MongoDB也具有的优势)现在网络上的很多NoSQL开源数据库完全属于社区型的，没有官方支持，给使用者带来了很大的风险。 而开源文档数据库MongoDB背后有商业公司10gen为其提供供商业培训和支持。 而且MongoDB社区非常活跃，很多开发框架都迅速提供了对MongDB的支持。不少知名大公司和网站也在生产环境中使用MongoDB，越来越多的创新型企业转而使用MongoDB作为和Django，RoR来搭配的技术方案。 ⑥性能优越在 使用场合下，千万级别的文档对象，近10G的数据，对有索引的ID的查询不会比mysql慢，而对非索引字段的查询，则是全面胜出。mysql实际无法胜 任大数据量下任意字段的查询，而mongodb的查询性能实在让我惊讶。写入性能同样很令人满意，同样写入百万级别的数据，mongodb比我以前试用过 的couchdb要快得多，基本10分钟以下可以解决。补上一句，观察过程中mongodb都远算不上是CPU杀手。 与关系型数据库相比，MongoDB的缺点：①mongodb不支持事务操作。所以事务要求严格的系统（如果银行系统）肯定不能用它。(这点和优点①是对应的) ②mongodb占用空间过大。关于其原因，在官方的FAQ中，提到有如下几个方面： 1、 空间的预分配：为避免形成过多的硬盘碎片，mongodb每次空间不足时都会申请生成一大块的硬盘空间，而且申请的量从64M、128M、256M那样的 指数递增，直到2G为单个文件的最大体积。随着数据量的增加，你可以在其数据目录里看到这些整块生成容量不断递增的文件。 2、字段名所占用 的空间：为了保持每个记录内的结构信息用于查询，mongodb需要把每个字段的key-value都以BSON的形式存储，如果value域相对于 key域并不大，比如存放数值型的数据，则数据的overhead是最大的。一种减少空间占用的方法是把字段名尽量取短一些，这样占用空间就小了，但这就 要求在易读性与空间占用上作为权衡了。我曾建议作者把字段名作个index，每个字段名用一个字节表示，这样就不用担心字段名取多长了。但作者的担忧也不 无道理，这种索引方式需要每次查询得到结果后把索引值跟原值作一个替换，再发送到客户端，这个替换也是挺耗费时间的。现在的实现算是拿空间来换取时间吧。 3、删除记录不释放空间：这很容易理解，为避免记录删除后的数据的大规模挪动，原记录空间不删除，只标记“已删除”即可，以后还可以重复利用。 4、可以定期运行db.repairDatabase()来整理记录，但这个过程会比较缓慢 ③MongoDB没有如MySQL那样成熟的维护工具，这对于开发和IT运营都是个值得注意的地方es和mongodb区别 设计模式 工厂模式： 1.简单工厂： 2.抽象工厂： 和建造者什么区别 抽象类和接口类他们的区别如下： 在抽象类中可以写非抽象的方法，从而避免在子类中重复书写他们，这样可以提高代码的复用性，这是抽象类的优势；接口中只能有抽象的方法。 一个类只能继承一个直接父类，这个父类可以是具体的类也可是抽象类；但是一个类可以实现多个接口。 API和SPI区别API可以直接使用，SPI一种规范，需要被继承 性能压测","link":"/2021/06/18/12000%E9%9D%A2%E8%AF%95/2012%E9%A2%982/"},{"title":"","text":"[toc] 1.开始是自我介绍； 2.HashMap的实现原理，什么是hash碰撞，怎样解决hash碰撞？ 3.ConcurrentHashMap的原理，与HashTable的区别？ 4.HashSet和TreeSet的区别以及底层实现原理 5.HashMap中存key-value，value有重复但是都是Comparable类型可比较；怎样根据value排序此集合，介绍实现方法 6.ReentrantLock和synchronized关键字有什么区别？ 7.synchronized 修饰static方法，具体锁的是什么？8.工作当中cpu和内存异常排查方法；详细说明分析过程及定位解决方式9.接着是讲项目，项目里的问题比较简单； 然后就是各种基础，jvm内存模型，nio，bio，aio，高并发，sychronized和volltail，HashMap，数据结构和扩容； 还有一些场景题目，大并发/海量数量的情况下，怎么设计系统。从里面拿出两点来问，一个是系统解耦，一个是分库分表； 最后一个是编码题，HashMap里key是自定义对象的情况，排序 jvm问的比较多，线上发版如何做到分批发的，redis命令，数据结构，数据库内部锁机制，线上问题解决，sql优化等等; 1.ClassLoader的原理,举出应用场景及工作实例，介绍类加载过程及工作中的应用2.HashMap的实现原理，什么是hash碰撞，怎样解决hash碰撞？开放寻址法和拉链法都是想办法找到下一个空位置来存发生冲突的值 3.ConcurrentHashMap的原理，与HashTable的区别？4.HashSet和TreeSet的区别以及底层实现原理TreeSet的本质是一个”有序的，并且没有重复元素”的集合，它是通过TreeMap实现的。 123456789101112131415public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt; { // 不带参数的构造函数。创建一个空的TreeMap public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } // 带比较器的构造函数。 public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;E,Object&gt;(comparator)); }}// TreeSet不支持快速随机遍历，只能通过迭代器进行遍历！for(Iterator iter = set.iterator(); iter.hasNext(); ) { iter.next();} 5.HashMap中存key-value，value有重复但是都是Comparable类型可比较；怎样根据value排序此集合，介绍实现方法 6.ReentrantLock和synchronized关键字有什么区别？ 7.synchronized 修饰static方法，具体锁的是什么？ 8.工作当中cpu和内存异常排查方法；详细说明分析过程及定位解决方式 9。一个jvm的原理及优化； 10.sql的优化； 11.现在使用的框架原理，比如使用了dubbo，会问dubbo的原理，还有h5怎么调用dubbo等； 12.接着是讲项目，项目里的问题比较简单； 13.然后就是各种基础，jvm内存模型，nio，bio，aio，高并发sychronized和volltail，HashMap，数据结构和扩容； 14.还有一些场景题目，大并发/海量数量的情况下，怎么设计系统。从里面拿出两点来问，一个是系统解耦，一个是分库分表； 15.最后一个是编码题，HashMap里key是自定义对象的情况，排序 1、executor service实现的方法，可以设置的参数； 2、出了个算法提，找出链表中倒数第n个节点； 3、还问了thread和runable的区别； 4、聚簇索引是什么； 5、redis问了一个实际问题的解决办法，如果redis一个value特别大，有什么解决方案； 6、redis内存淘汰机制； 7、mysql的默认隔离级别； \\8. 堆排序 \\9. paxos协议 \\10. 跨机房部署，遇到的问题及解决方案，全年的9999率 11. MMM的DB架构，主从未完全同步，master挂了，未同步的内容会造成什么影响，怎么恢复 \\12. ng和tomcat什么区别？能否将两者角色互换。即：tomcat做反向代理，ng做服务容器。说明原因。 \\13. DNS协议 \\14. volatile实现原理 \\15. NAT：公网ip和局域网ip转换 16.类加载委托机制，锁的应用，项目架构 四面： 0、 jit，nio，排序算法，hashmap，更多的是项目细节~ 1、谈谈项目里主要负责了什么，负责的项目是怎样的架构，自己负责了什么等等； 2、JVM类加载机制； 3、JVM内存模型，栈空间都放什么，什么情况下栈内存会溢出等； 4、JVM调优； 5、JVM垃圾收集机制； 6、比较关心金融方面的知识是否了解，是否有过金融项目开发的经验； 7、jvm栅栏问题，threadlocal的使用； 8、JVM 9、多线程 10、List求交集 11、解决项目运行时，CPU占用过高的问题 12、线程同步几种机制 13、linux检索log，匹配某一请求最多的top10","link":"/2021/08/30/12000%E9%9D%A2%E8%AF%95/2013%E9%A2%983/"},{"title":"","text":"1Object o = new Object(); 1.解释下对象的创建过程 分配空间 初始化 引用赋值 1234567891011class T { int m = 8;}T t = new T();0 new #2 &lt;T&gt; // 分配空间3 dup4 invokespecial #3 &lt;T.&lt;init&gt;&gt; // 初始化7 astore_1 // 引用赋值8 return 2.DCL单例 double check lock 到底需不需要volatile 123456789101112if (INSTANCE == null) { synchronized(Mg.class) { if (INSTANCE == null) { /** * 没加volatile 指令重排后，先引用赋值 * INSTANCE != null */ INSTANCE = new Mg(); } }}return INSTANCE; 3.对象在内存中的存储布局 1.Markword 8B 2.Class pointer类型指针 4B 3.instance data示例对象 4.padding 对齐 4.对象头信息 5.对象怎么定位 6.对象怎么分配 7.内存多少字节 new Object(); 16字节 1.最顶级的异常类是什么，什么情况下会用到，他的子类有什么，error什么时候会用到***Throwable*： 有两个重要的子类：Exception（异常）和 Error（错误） ***Error（错误）*:**是程序无法处理的错误 OutOfMemoryError ***Exception（异常）*:**是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException 2.java语法糖有了解吗自动装箱与拆箱 可变参数 1234public static void print(String... strs) { for (int i = 0; i &lt; strs.length; i++) System.out.println(strs[i]); } 7.新生代和老年代的比例大小，怎么设置老年代的比例-Xmx 最大堆内存 -Xms 最小堆内存 -XX:SurvivorRatio 设置eden:from:to的比例 默认是8 实际默认并不一定 -XX:NewRatio 新生代与老年代的比率 默认是2，老年代是新生代的2倍 非堆空间也是持久代，方法区，jdk8以后的元数据区。 -XX:PermSize 初始永久代的大小 -XX:MaxPermSize 最大永久代的大小 注意永久代就是通常指的方法区，保存类信息，常量表等，在jdk8中已经废弃了永久代，而是用元数据去代取，默认的非常大可以用： 持久代的空间与我们的类的大小有很大的关系，类加载后放着持久代，如果不是频繁使用动态字节码技术、发射、OSGI等技术，持久代的大小基本上可以预估的，测试环境是多少，上线后基本没有差别。 -XX:MaxMetaspaceSize配置最大元数据区的大小 8.jvm怎么查看内存占用情况9.aqs10.synchronized的底层原理，它是可重入锁吗。怎么实现可重入锁 13.有a,b,c三列，建了一个 组合索引，和在a上建了索引。现在查询a=？.会选择哪个索引14.mysql有哪些约束15.innodb默认的事务隔离级别，要做分布式事务，最高能到哪个隔离级别16.tcp三次握手，tcp协议是在网络链路哪一层17.http头部有哪些参数18.hashmap1.8的改动。写出123456的红黑树19.集合中有哪些有序的集合20.linkedhashmap和treemap的区别 守护线程程序结束，直接中断。不可访问固有资源，因为可能随时中断 Thread.setDaemon() ThreadLocal原理与场景1.对象跨层传递，避免多次传递2.线程间数据隔离3.事务操作，存储线程事务信息4.数据库连接，session会话管理 内存泄漏原因： weakreference无论内存是否足够，都会被回收 强软弱虚","link":"/2022/03/08/12000%E9%9D%A2%E8%AF%95/2014%E9%A2%984/"},{"title":"","text":"[toc] 主从复制三个线程：Master一条线程和Slave中的两条线程 master（binlog dump thread) Slave (I/O thread SQL thread) 1.主节点binlog，主从复制的基础是主库记录数据库的所有变更记录到binlog。2.主节点log dump线程，当binlog有变动时，log dump线程读取其内容并发送给从节点。3.从节点I/O线程接收binlog内容，并将其写入relay log文件中。4.从节点的SQL线程读取relay log文件内容对数据更新进行重放，最终保证主从数据库的一致性。 注：主从节点使用binlog文件+position偏移量来定位主从同步的位置，从节点会保存其已接收到的偏移量，如果从节点重启，则自动从position的位置发起同步。 全同步复制主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回客户端，性能有影响。 半同步复制和半同步不同的是，半同步复制的逻辑，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作成功 索引类型和对性能的影响普通索引 唯一索引 主键：一种特殊的唯一索引 联合索引：覆盖多个数据列，INDEX(colA, colB); 全文索引：通过建立倒排索引， Alter Table t_name Add Fulltext(column) RDB和AOF机制RDB： redis database 将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，将数据集写入临时文件，写入成功后，替换之前的文件，用二进制压缩存储。 优点： 1.整个Redis数据库只包含一个文件dump.rdb，方便持久化2.容灾性好，方便备份3.性能最大化，fork子进程来完成写操作，主进程继续处理命令，所以IO最大化。4.对于数据集大时，比AOF的启动效率更高。 缺点： 1.数据安全性低。RDB是间隔一段时间进行持久化，2次持久化之间发生故障数据丢失2.数据大时，可能导致服务停止 AOF：Append Only File","link":"/2021/08/13/12000%E9%9D%A2%E8%AF%95/2015%E9%A2%985/"},{"title":"","text":"分布式事务解决方案XA规范：分布式事务规范，定义了分布式事务模型 四个角色：事务管理器、资源管理器、应用程序AP、通信资源管理器CRM 全局事务：一个横跨多个数据库的事务，要不全提交，要不全回滚 两阶段提交第一阶段：prepare：每个参与执行本地事务但不提交，进入ready状态，并通知协调者已经准备就绪。 第二阶段commint：当协调者确认每个参与者都ready后，通知参与者进行commit操作；如有参与者fail，则发送rollback命令，各参与者做回滚。 问题： 单点故障：一旦事务管理器出现故障，整个系统不可用 数据不一致：在二阶段，如果事务管理器只发送部分commit消息，只有部分参与者提交了事务，使得系统数据不一致 响应时间长：参与者资源都被锁，提交、回滚才释放 不确定性：部分参与者commit，事务管理器宕机，新管理器无法确定消息是否成功 三阶段提交解决2PC单点故障问题，性能和不一致问题没有根本解决 超时机制（可用性提高）：如果消息执行成功，如果一定时间没收到DoCommit消息，自己执行DoCommit操作 TCC（补偿事务）：Try、Confirm、Cancel","link":"/2022/03/08/12000%E9%9D%A2%E8%AF%95/21016%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"","text":"ZAB协议Zookeeper专门设计的一种支持崩溃恢复的原子广播协议，实现分布式数据一致性。 ZK是CP，放弃高可用 消息广播Leader节点处理所有事务请求，Leader将客户端的事务请求转换成事务Proposal，并且将Proposal分发给集群中其他所有的Follower。 完成广播后，等待Follower反馈，当有过半数的Follower反馈信息后，Leader将再次向集群中Follower广播Commit信息，Commit信息就是确认之前的Proposal提交。 Leader节点的写入是一个两步操作，第一步是广播事务操作，第二步是广播提交操作。 崩溃恢复 初始化机器，刚刚启动的时候 Leader崩溃，故障宕机 Leader失去半数的机器支持，与集群超过一般的节点断连 此时开启新一轮Leader选举，选举产生的Leader会与过半的Follower进行同步，使数据一致，当过半的机器同步完成后，就退出恢复模式，进入消息广播模式 Zxid是Zab协议的一个事务编号，64位数字，低32是单调递增计数器，针对客户端每一个事务请求，计数器+1，高32位代表Leader周期年代编号。 Leader周期（epoch）可以理解成当前集群所处的年代或者周期，每当有一个新的Leader选举出现，就会从这个Leader服务器上取出其本地日志中最大事务的Zxid，并从中读取epoch值，然后+1，以此作为新的周期ID，高32位代表了每代Leader唯一性，低32位事务的唯一性。 Zab节点的三种状态：following：服从leader的命令 leading：负责协调事务","link":"/2022/03/08/12000%E9%9D%A2%E8%AF%95/21017%E7%AE%80%E8%BF%B0ZAB%E5%8D%8F%E8%AE%AE/"},{"title":"","text":"ZK的命名服务、配置管理、集群管理命名服务：通过指定的名字来获取资源或者服务地址。Zookeeper可以创建一个全局唯一的路径，这个路径就可以作为一个名字。被命名的实体可以是集群中的机器，服务的地址，或者是远程的对象等。一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息等 配置管理：实际项目开发中，经常使用.properties或者xml需要配置很多信息，如数据库连接信息、fps地址端口等等。程序分布式部署时，如果把程序的这些配置信息保存在zk的znode节点下，当你需要修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知各个客户端，从而更改配置。 集群管理：集群管理包含集群监控和集群控制，就是监控集群机器状态，剔除机器和加入机器。实时监控znode节点的变化，一旦返现机器宕机，该机器就会与zk断开链接，对应的临时目录节点会被删除，其他机器都收到通知，新机器加入也是类似。","link":"/2022/03/08/12000%E9%9D%A2%E8%AF%95/21018%E7%AE%80%E8%BF%B0ZK%E7%9A%84%E5%91%BD%E5%90%8D%E6%9C%8D%E5%8A%A1/"},{"title":"","text":"[TOC] 5对象怎么分配","link":"/2022/04/17/12000%E9%9D%A2%E8%AF%95/2022%E5%B9%B4%E9%9D%A2%E8%AF%95/"},{"title":"","text":"[TOC] 1.数据同步原理1.PeerLastZxid：Follower或Observer最后处理的zxid2.minCommittedLog： Leader服务proposal缓存队列committedLog中最小的zxid3.maxCommittedLog: Leader服务proposal缓存队列committedLog中最大的zxid zk中的数据同步有4类： 直接差异化同步 PeerLastZxid介于min和max之间 TRUNC+DIFF 先回滚再同步 Follower中有Leader没有的事务记录 TRUNC 仅回滚同步 PeerLastZxid&gt;max SNAP 全量同步 PeerLastZxid&lt;min 2.zk的watch机制的实现原理客户端实现过程： 标记会话有Watch事件的请求 通过DataWatchRegistration类来保存watch事件和节点的对应关系 客户端对服务器发送请求，将请求封装成一个Packet对象，添加到一个等待发送队列outgoingQueue中 将Watch注册到ZKWatchManager，保存了Map&lt;String, Set&gt; dataWatchers,key为path 3.zk分布式锁实现原理 创建一个锁节点下的一个接一个的临时顺序节点 如果自己不是第一个节点，就对自己上一个节点加监听器 只要上一个节点释放锁，自己就排到前面去了 使用临时节点，如果客户端创建临时顺序节点后宕机了，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，获取自动取消自己的排队。解决了惊群效应。 4.zk应用场景1.数据发布订阅：配置中心2.负载均衡：提供服务者列表3.命名服务：注册中心，提供服务名到服务地址的映射4.分布式协调通知：watch机制和临时节点，获取各节点的任务进度5.集群管理：是否有机器退出和加入，选举master6.分布式锁7.分布式队列 如何选择消息队列可维护性：Kafka是Scala语言编写，没有经验，很难快速处理问题。 业务场景：Kafka是大容量的日志消息传输，而我们的消息队列是为了业务数据的可靠传输。","link":"/2022/03/28/12000%E9%9D%A2%E8%AF%95/zk%E9%9D%A2%E8%AF%95/"},{"title":"","text":"[TOC] ▲ 34 进程和线程之间有什么区别？ ▲ 30 进程间有哪些通信方式？ ▲ 29 简述 socket 中 select 与 epoll 的使用场景以及区别，epoll 中水平触发以及边缘触发有什么不同？ ▲ 26 Linux 进程调度中有哪些常见算法以及策略？ ▲ 18 操作系统如何申请以及管理内存的？ ▲ 12 简单介绍进程调度的算法 ▲ 11 简述 Linux 系统态与用户态，什么时候会进入系统态？ ▲ 11 简述 LRU 算法及其实现方式 ▲ 11 线程间有哪些通信方式？ ▲ 8 简述同步与异步的区别，阻塞与非阻塞的区别 ▲ 8 简述操作系统如何进行内存管理 ▲ 8 什么时候会由用户态陷入内核态？ ▲ 7 简述操作系统中的缺页中断 ▲ 3 简述操作系统中 malloc 的实现原理 ▲ 2 BIO、NIO 有什么区别？怎么判断写文件时 Buffer 已经写满？简述 Linux 的 IO模型 ▲ 1 进程空间从高位到低位都有些什么？ 进程间有哪些通信方式？进程间通信（InterProcess Communication，IPC） 管道: 只能在父子进程间 12345678910111213141516171819202122232425#define MAX_LEN 128/*0为读，1为写*/int fd[2] = {0}; //描述符pid_t pid = 0;char line[MAX_LEN] = {0};int n = 0;/*创建管道，需要传入两个文件描述符*/pipe(fd);/*fork子进程*/pid = fork();/*父进程*/if(pid &gt; 0) { /*关闭管道的写描述符*/ close(fd[1]); /*从管道读取数据*/ n = read(fd[0], line, MAX_LEN); printf(&quot;read %d bytes from pipe :%s\\n&quot;, n, line);}/*子进程*//*关闭管道的读描述符*/close(fd[0]);/*向管道写入数据*/write(fd[1],&quot;www.baidu.com&quot;,sizeof(&quot;www.baidu.com&quot;));&gt; read 18 bytes from pipe :www.baidu.com FIFO: FIFO也被称为命名管道，与管道不同的是，不相关的进程也能够进行数据交换。 消息队列信号量共享内存UNXI域套接字套接字（Socket）","link":"/2021/04/01/12000%E9%9D%A2%E8%AF%95/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"","text":"8.1.5 如何从尾到头输出单链表​ 从头到尾输出单链表比较简单，通过借鉴的想法，要想解决本问题，很自然地想把链表中链接节点的指针反转过来，改变链表的方向，然后就可以从尾到头输出了，但该方法需要额外的操作，是否还有更好的方法呢？答案是有。​ 接下来的想法是从头到尾遍历链表，每经过一个结点，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始输出结点的值，此时输出的结点的顺序已经反转过来了。该方法虽然没有只需要遍历一遍链表，但是需要维护一个额外的栈空间，实现起来会比较麻烦。​ 是否还能有更高效的方法？既然想到了栈来实现这个函数，而递归本质上就是一个栈结构，于是很自然地又想到了递归来实现。要实现反过来输出链表，每访问到一个结点，先递归输出它后面的结点，再输出该结点自身，这样链表的输出结果就反过来了。 123456public void printListReverse(Node pList) { if (pList != null) { printListReverse(pList.next); System.out.println(pList.data); }}","link":"/2021/04/03/12000%E9%9D%A2%E8%AF%95/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"title":"","text":"▲ 58 MySQL 为什么使用 B+ 树来作索引，对比 B 树它的优点和缺点是什么？ ▲ 31 数据库的事务隔离级别有哪些？各有哪些优缺点？ ▲ 25 什么是数据库事务，MySQL 为什么会使用 InnoDB 作为默认选项 ▲ 21 什么情况下会发生死锁，如何解决死锁？ ▲ 21 简述乐观锁以及悲观锁的区别以及使用场景 ▲ 21 Redis 有几种数据结构？Zset 是如何实现的？ ▲ 19 聚簇索引和非聚簇索引有什么区别？什么情况用聚集索引？ ▲ 18 简述脏读和幻读的发生场景，InnoDB 是如何解决幻读的？ ▲ 17 唯一索引与普通索引的区别是什么？使用索引会有哪些优缺点？ ▲ 14 简述 MySQL 的间隙锁 ▲ 13 简述 Redis 持久化中 rdb 以及 aof 方案的优缺点 ▲ 11 Redis 如何实现延时队列，分布式锁的实现原理 ▲ 9 简述 Redis 中如何防止缓存雪崩和缓存击穿 ▲ 8 MySQL 有什么调优的方式？ ▲ 7 简述 MySQL 的主从同步机制，如果同步失败会怎么样？ ▲ 6 MySQL 的索引什么情况下会失效？ ▲ 5 简述数据库中的 ACID 分别是什么？ ▲ 4 Kafka 发送消息是如何保证可靠性的？ ▲ 3 简述 Redis 中跳表的应用以及优缺点 ▲ 1 假设Redis 的 master 节点宕机了，你会怎么进行数据恢复？ 锁定读的语句对读取的记录加共享-S锁： 1SELECT ... LOCK IN SHARE MODE; 对读取的记录加 排他锁-X锁： 1SELECT ... FOR UPDATE;","link":"/2021/02/20/12000%E9%9D%A2%E8%AF%95/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"","text":"▲ 41 简述 TCP 三次握手以及四次挥手的流程。为什么需要三次握手以及四次挥手？ ▲ 32 RestFul 与 RPC 的区别是什么？RestFul 的优点在哪里？ ▲ 29 HTTP 与 HTTPS 有哪些区别？ ▲ 26 RestFul 是什么？RestFul 请求的 URL 有什么特点？ ▲ 24 从输入 URL 到展现页面的全过程 ▲ 19 TCP 与 UDP 在网络协议中的哪一层，他们之间有什么区别？ ▲ 18 TCP 中常见的拥塞控制算法有哪些？ ▲ 17 TCP 怎么保证可靠传输？ ▲ 17 从系统层面上，UDP如何保证尽量可靠？ ▲ 9 TCP 四次挥手的时候 CLOSE_WAIT 的话怎么处理？ ▲ 8 TCP 的 keepalive 了解吗？说一说它和 HTTP 的 keepalive 的区别？ ▲ 8 简述 TCP 滑动窗口以及重传机制 ▲ 8 简述 HTTP 1.0，1.1，2.0 的主要区别 ▲ 7 简述 TCP 的 TIME_WAIT ▲ 5 HTTP 的方法有哪些？ ▲ 4 简述 TCP 协议的延迟 ACK 和累计应答 ▲ 1 简述 TCP 的报文头部结构 ▲ 1 简述 TCP 半连接发生场景 ▲ 1 什么是 SYN flood，如何防止这类攻击？","link":"/2020/11/13/12000%E9%9D%A2%E8%AF%95/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"title":"","text":"责任链模式","link":"/2021/07/02/12000%E9%9D%A2%E8%AF%95/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"","text":"[TOC] java中你知道哪些锁？问题回答 乐观锁/悲观锁 共享锁/独享锁 公平锁/非公平锁 互斥锁/读写锁 可重入锁 自旋锁 分段锁 偏向锁/轻量级锁/重量级锁 Java线程的状态或者生命周期？问题回答 Java的线程状态被定义在公共枚举类java.lang.Thread.state中。一种有六种状态 新建（NEW）：表示线程新建出来还没有被启动的状态，比如：Thread t = new MyThread(); 就绪/运行（RUNNABLE）：该状态包含了经典线程模型的两种状态：就绪(Ready)、运行(Running)： 阻塞（BLOCKED）：通常与锁有关系，表示线程正在获取有锁控制的资源，比如进入synchronized代码块，获取ReentryLock等；发起阻塞式IO也会阻塞，比如字符流字节流操作。 等待（WAITING）：线程在等待某种资源就绪。 超时等待（TIMED_WAIT）：线程进入条件和等待类似，但是它调用的是带有超时时间的方法。 终止（TERMINATED）：线程正常退出或异常退出后，就处于终结状态。也可以叫线程的死亡。 看下源码在Java.lang.Thread里，有个内部枚举: State，一个线程在某一时刻可以是State里的一种状态 看图理解 哪些情况或者方法可以进入等待状态？ 当一个线程执行了Object.wait()的时候，它一定在等待另一个线程执行Object.notify()或者Object.notifyAll()。 一个线程thread，其在主线程中被执行了thread.join()的时候，主线程即会等待该线程执行完成。 当一个线程执行了LockSupport.park()的时候，其在等待执行LockSupport.unpark(thread)。 哪些情况或者方法可以进入超时等待状态？问题回答 该状态不同于WAITING，它可以在指定的时间后自行返回 Object.wait(long) Thread.join(long) LockSupport.parkNanos() LockSupport.parkUntil() Thread.sleep(long) synchronized 与lock区别？问题回答 lock是一个接口，而synchronized是java的一个关键字 synchronized异常会释放锁，lock异常不会释放，所以一般try catch包起来，finally中写入unlock，避免死锁。 Lock可以提高多个线程进行读操作的效率 synchronized关键字，可以放代码块，实例方法，静态方法，类上 lock一般使用ReentrantLock类作为锁，配合lock()和unlock()方法。在finally块中写unlock()以防死锁。 jdk1.6之前synchronized低效。jdk1.6之后synchronized高效。 synchronized 与ReentrantLock区别？问题回答 synchronized依赖JVM实现，ReentrantLock是JDK实现的。synchronized是内置锁，只要在代码开始的地方加synchronized，代码结束会自动释放。Lock必须手动加锁，手动释放锁。 ReenTrantLock比synchronized增加了一些高级功能。synchronized代码量少，自动化，但扩展性低，不够灵活；ReentrantLock扩展性好，灵活，但代码量相对多。 两者都是可重入锁。都是互斥锁。 synchronized是非公平锁，ReentrantLock可以指定是公平锁还是非公平锁。 synchronized 与ThreadLocal区别？问题回答 都是为了解决多线程中相同变量的访问冲突问题。 Synchronized同步机制，提供一份变量，让不同的线程排队访问。 ThreadLocal关键字，为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 看代码 synchronized 与volatile区别？问题回答 volatile是一个类型修饰符（type specifier）。 volatile，它能够使变量在值发生改变时能尽快地让其他线程知道。 关键字volatile是线程同步的轻量级实现，所以volatile性能肯定比synchronized要好，并且只能修改变量，而synchronized可以修饰方法，以及代码块。 多线程访问volatile不会发生阻塞，而synchronized会出现阻塞 volatile能保证数据的可见性，但不能保证原子性；而synchronized可以保证原子性，也可以间接保证可见性，因为它会将私有内存和公共内存中的数据做同步 关键字volatile解决的下变量在多线程之间的可见性；而synchronized解决的是多线程之间资源同步问题 Thread类中的start()和run()方法有什么区别? 通过调用线程类的start()方法来启动一个线程，使线程处于就绪状态，即可以被JVM来调度执行，在调度过程中，JVM通过调用线程类的run()方法来完成实际的业务逻辑，当run()方法结束后，此线程就会终止。 如果直接调用线程类的run()方法，会被当作一个普通的函数调用，程序中仍然只有主线程这一个线程。即start()方法能够异步地调用run()方法，但是直接调用run()方法却是同步的，无法达到多线程的目的。 因此，只有通过调用线程类的start()方法才能达到多线程的目的。 事务的隔离级别及引发的问题？问题回答 4个隔离级别：读未提交、读已提交、可重复读、串行化 分别怎么理解呢？ 读未提交（READ UNCOMMITTED），事务中的修改，即使没有提交，对其它事务也是可见的。 读已提交（READ COMMITTED），一个事务能读取已经提交的事务所做的修改，不能读取未提交的事务所做的修改。也就是事务未提交之前，对其他事务不可见。 可重复读（REPEATABLE READ），保证在同一个事务中多次读取同样数据的结果是一样的。 串行化（SERIALIZABLE），强制事务串行执行。 3.读已提交是sql server的默认隔离级别。 可重复读是mysql的默认隔离级别。 简要回答 4个隔离级别，读未提交、读已提交、可重复读、可串行化。 读未提交（READ UNCOMMITTED），事务提交与否都可见，引发脏读、不可重复读、幻读。 读已提交（READ COMMITTED），已提交的事务可见，引发不可重复读、幻读。 可重复读（REPEATABLE READ），多次读取，数据一致，引发幻读。 串行化（SERIALIZABLE），串行执行。 大多数数据库的默认隔离级别为: Read Commited,如Sql Server , Oracle。 少数数据库默认的隔离级别为Repeatable Read, 如MySQL InnoDB存储引擎。 理解脏读、不可重复读、幻读脏读：读到未提交的数据。 不可重复读：重点是修改，同样的条件, 你读取过的数据, 再次读取出来发现值不一样了。 幻读：重点在于新增或者删除，同样的条件, 第1次和第2次读出来的记录数不一样。 简单理解4个隔离级别 读未提交，比如事务A和事务B同时进行，事务A在整个执行阶段，会将某数据的值从1开始一直加到10，然后进行事务提交。此时，事务B能够读取事务A操作过程中的未提交的数据（1、2、3、4、5、6…10）。 读已提交，事务A在整个执行阶段，会将某数据的值从1开始一直加到10，然后进行事务提交。此时，事务B只能读取到最终的10。 可重复读，事务B开始读取到的是某个值是0，事务A对值进行修改提交多次，事务B读取到的依然是0。多次读取，结果一致。 串行化，是最严格的事务隔离级别，它要求所有事务被串行执行，一个事务没有结束，另外的事务没法继续。 案列演示读未提交 读已提交 可重复读 串行化 串行化1 什么是线程安全，java如何保证线程安全？问题回答 在多线程环境中，能永远保证程序的正确性。执行结果不存在二义性。说白了，运行多少次结果都是一致的。 换种说法，当多个线程访问某一个类（对象或方法）时，这个类始终都能表现出正确的行为，那么这个类（对象或方法）就是线程安全的。 使用synchronized关键字和使用锁。 介绍一下线程池？结果问题回答 线程池就是预先创建一些线程，它们的集合称为线程池。 线程池可以很好地提高性能，在系统启动时即创建大量空闲的线程，程序将一个task给到线程池，线程池就会启动一条线程来执行这个任务，执行结束后，该线程不会死亡，而是再次返回线程池中成为空闲状态，等待执行下一个任务。 线程的创建和销毁比较消耗时间，线程池可以避免这个问题。 Executors是jdk1.5之后的一个新类，提供了一些静态方法，帮助我们方便的生成一些常见的线程池 newSingleThreadExecutor：创建一个单线程化的Executor。 newFixedThreadPool：创建一个固定大小的线程池。 newCachedThreadPool：创建一个可缓存的线程池 newScheduleThreadPool：创建一个定长的线程池，可以周期性执行任务。 我们还可以使用ThreadPoolExecutor自己定义线程池，弄懂它的构造参数即可 int corePoolSize，//核心池的大小 int maximumPoolSize，//线程池最大线程数 long keepAliveTime，//保持时间/额外线程的存活时间 TimeUnit unit，//时间单位 BlockingQueue workQueue，//任务队列 ThreadFactory threadFactory，//线程工厂 RejectedExecutionHandler handler //异常的捕捉器 简要回答 线程池就是预先创建一些线程 线程池可以很好地提高性能 线程池可以避免线程的频繁创建和销毁 Executors可以创建常见的4种线程（单线程池、固定大小的、可缓存的、可周期性执行任务的）。 可以通过ThreadPoolExecutor自己定义线程池。 看看Excutors中的方法 Excutors中的方法 看看ThreadPoolExecutor的构造函数 ThreadPoolExecutor的构造函数 常见的线程池有哪些？问题回答 Executors是jdk1.5之后的一个新类，提供了一些静态方法，帮助我们方便的生成一些常见的线程池 单线程线程池，通过newSingleThreadExecutor()创建 固定大小的线程池，通过newFixedThreadPool()创建 可缓存的线程池，通过newCachedThreadPool()创建 可周期性执行任务的线程池，通过newScheduleThreadPool()创建 看下Excutors的使用 Excutors的使用 几个线程的区别？newCachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 这种类型的线程池特点是： 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。 newFixedThreadPool 创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。 FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor 创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newScheduleThreadPool 创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。 同步和异步有何异同？问题回答 同步发了指令，会等待返回，然后再发送下一个。 异步发了指令，不会等待返回，随时可以再发送下一个请求 同步可以避免出现死锁，读脏数据的发生 异步则是可以提高效率 实现同步的机制主要有临界区、互斥、信号量和事件 哪些集合是线程安全？ 问题回答 Vector：就比Arraylist多了个同步化机制（线程安全）。 Hashtable：就比Hashmap多了个线程安全。 ConcurrentHashMap:是一种高效但是线程安全的集合。 如何异步获取多线程返回的数据？问题包含说一下Callable这个接口的理解？ 说一下Future接口的理解？ 说一下FutureTask类的理解？ 说一下CompletionService接口的理解？ 问题回答 通过Callable+Future，Callable负责执行返回，Future负责接收。Callable接口对象可以交给ExecutorService的submit方法去执行。 通过Callable+FutureTask，Callable负责执行返回，FutureTask负责接收。FutureTask同时实现了Runnable和Callable接口，可以给到ExecutorService的submit方法和Thread去执行。 通过CompletionService，jdk1.8之后提供了完成服务CompletionService，可以实现这样的需求。 注意，实现Runnable接口任务执行结束后无法获取执行结果。 Callable有返回值，Runnable没有返回值 Callable有返回值，Runnable没有返回值 看看Future接口 看看所有方法 Future接口方法 重点看看get方法 get方法 总结下： V get() ：获取异步执行的结果，如果没有结果可用，此方法会阻塞直到异步计算完成。 V get(Long timeout , TimeUnit unit) ：获取异步执行结果，如果没有结果可用，此方法会阻塞，但是会有时间限制，如果阻塞时间超过设定的timeout时间，该方法将抛出异常。 boolean isDone() ：判断任务是否完成如果任务执行结束，无论是正常结束或是中途取消还是发生异常，都返回true。future.isDone() boolean isCanceller() ：如果任务完成前被取消，则返回true。 future.isCanceller() boolean cancel(boolean mayInterruptRunning) ：如果任务还没开始，执行cancel(…)方法将返回false；如果任务已经启动，执行cancel(true)方法将以中断执行此任务线程的方式来试图停止任务，如果停止成功，返回true；当任务已经启动，执行cancel(false)方法将不会对正在执行的任务线程产生影响(让线程正常执行到完成)，此时返回false；当任务已经完成，执行cancel(…)方法将返回false。mayInterruptRunning参数表示是否中断执行中的线程。 通过方法分析我们也知道实际上Future提供了3种功能： （1）能够中断执行中的任务 （2）判断任务是否执行完成 （3）获取任务执行完成后的结果。 看看ExecutorService的submit方法 submit方法 注意： Callable接口的call方法有返回值，Runnable接口的run方法没有返回值 submit（Runnable）：Future&lt;?&gt; ，返回的Future无法获取返回值。submit（Runnable, T）：Future，返回的Future无法获取返回值。 看看FutureTask 看FutureTask定义 FutureTask定义 看RunnableFuture定义，同时实现了Runnable、Future RunnableFuture定义 所以，FutureTask ExecutorService的submit方法接收Runnable和Callable，所以接收FutureTask。 Thread只支持Runnable，所以也支持FutureTask。FutureTask让Thread也可以有返回值的效果。 看代码 Callable接口配合ExecutorService的submit方法 AsynTask1.java 结果 结果 Callable接口配合FutureTask，ExecutorService的submit方法去执行 AsynTask2.java 结果 结果 Callable接口配合FutureTask，给Thread类去执行12345678public class AsynTask3 { public static void main(String[] args) { asynTask3(); System.out.println(&quot;主线程执行完成&quot;); } } AsynTask3.java 结果 CompletionService配合Callable AsynTask4 结果 结果 CompletionService和Future的区别呢？ Future获取结果，一个一个地取，一个取完了，再取另外一个，就会等待 CompletionService，任意一个线程有返回，就立马取出 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class AsynTask5 { public static void main(String[] args) { asynCompletionService(); System.out.println(&quot;----------------分割线-------------------&quot;); asynFuture(); System.out.println(&quot;主线程执行完毕&quot;); } private static void asynCompletionService() { try { //使用ExecutorService ExecutorService executorService = Executors.newCachedThreadPool(); // 构建完成服务 CompletionService&lt;Integer&gt; completionService = new ExecutorCompletionService&lt;Integer&gt;(executorService); for (int i = 1; i &lt;= 5; i++) { //提交任务 completionService.submit(new HandleFuture&lt;&gt;(i)); } //获取结果，一个一个阻塞的取出。这中间肯定会浪费一定的时间在等待上 for (int i = 1; i &lt;= 5; i++) { Integer result = completionService.take().get(); System.out.println(&quot;结果：&quot; + result); } } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } private static void asynFuture() { try { //使用ExecutorService ExecutorService executorService = Executors.newCachedThreadPool(); //Future列表 List&lt;Future&lt;Integer&gt;&gt; result = new ArrayList&lt;Future&lt;Integer&gt;&gt;(); for (int i = 1; i &lt;= 5; i++) { //提交任务 Future&lt;Integer&gt; submit = executorService.submit(new HandleFuture&lt;&gt;(i)); result.add(submit); } //获取结果，输出和线程的放入顺序无关系。每一个线程执行成功后，立刻就输出 for (Future&lt;Integer&gt; integerFuture : result) { Integer integer = integerFuture.get(); System.out.println(&quot;结果：&quot; + integer); } } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } }}class HandleFuture&lt;Integer&gt; implements Callable&lt;Integer&gt; { private Integer num; public HandleFuture(Integer num) { this.num = num; } @Override public Integer call() throws Exception { Thread.sleep(2 * 1000l); System.out.println(Thread.currentThread().getName()); return num; }} 结果 如何自定义线程池？corePoolSize：核心池的大小 默认情况下，在创建了线程池之后，线程池中的线程数为0 当有任务到来后，如果线程池中存活的线程数小于corePoolSize，则创建一个线程。 maximumPoolSize：线程池最大线程数 线程池中允许的最大线程数，这个参数表示了线程池中最多能创建的线程数量。 当任务数量比corePoolSize大时，任务添加到workQueue 当workQueue满了，将继续创建线程以处理任务。 maximumPoolSize表示当wordQueue满了，线程池中最多可以创建的线程数量。 keepAliveTime： 当线程池处于空闲状态时，超过keepAliveTime时间之后，空闲的线程会被终止。 只有当线程池中的线程数大于corePoolSize时，这个参数才会起作用，但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； 当线程数大于corePoolSize时，如果一个线程的空闲时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。 unit：参数keepAliveTime的时间单位，有7种取值 TimeUnit.DAYS; //天 TimeUnit.HOURS; //小时 TimeUnit.MINUTES; //分钟 TimeUnit.SECONDS; //秒 TimeUnit.MILLISECONDS; //毫秒 TimeUnit.MICROSECONDS; //微妙 TimeUnit.NANOSECONDS; //纳秒 workQueue ： 任务队列，阻塞队列，存储提交的等待任务。常见子类有： ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; threadFactory : 线程工厂，指定创建线程的工厂 handler ： 任务队列添加异常的捕捉器，当任务超出线程池范围和队列容量时，采取何种拒绝策略。参考 RejectedExecutionHandler，常见实现类。 ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 看一个自定义线程的例子 简单演示 执行结果 执行结果 工作中哪些地方使用了多线程？问题回答 一般业务，web层–&gt; service层 –&gt;dao –&gt; sql基本用不到多线程 数据量很大（1000w级别、TB级别）的I/O操作，可以考虑多线程 举一些例子 自己做并发测试的时候，假如想写想模拟3000个并发请求。 多线程下单抢单，假如支持5000人的并发下单。 多线程写入mysql，假如有1000w条数据要入库。 多线程写入redis，假如有1000w的数据要存入redis。 多线程导入ES索引，假如有1000w的数据要添加到ES索引。 poi多线程导出，假如xls里面有10w的数据需要导出。 poi多线程导入，假如有10w条数据需要导入到xls。 多线程发送邮件，假如有10w用户需要发送邮件。 多线程发送短信，假如有10w用户需要发送邮件。 10.多线程备份日志，假如10tb日志文件要备份。 11.多线程验证数据，比如验证url是否存在，假如有100w个url 数据并发操作可能的问题?问题回答 丢失的修改 不可重复读,读第二次,数据就不对了 读脏数据 幻影读 脏读:指事务读到了其它事务做了修改而未提交的数据 不可重复读:不能重复读两次,读两次就不同了 幻读:同一个事务T1在两个不同的时间段t执行同一条查询语句得到的记录数量不同 消息等待通知wait/notify具体的应用问题回答 一个线程修改了一个对象的值，另外一个线程需要感知到这个变化 Java中我们使用的对象锁以及wait/notify方法进行线程通信 等待方遵循的原则：获取对象的锁不满足条件 就调用wait()方法条件满足继续执行 通知方原则：获取对象的锁改变条件， 然后notify 线程池中 submit() 和 execute() 方法有什么区别？问题回答 execute() 参数 Runnable ； submit() 参数 (Runnable) 或 (Runnable 和 结果 T) 或 (Callable)； execute(Runnable x) 没有返回值。可以执行任务，但无法判断任务是否成功完成。 submit(Callable x)有返回值，返回一个Future类的对象。 Future对象 通过get方法，获取线程返回结果 通过get方法，接收任务执行时候抛出的异常 通过isDone方法，可以判断线程是否执行完成。 看execute和submit方法的定义 看submit方法的定义 看看Future的get方法 可以接收结果，可以接收任务执行时候抛出的异常 看看Future的方法 线程的创建方式有哪些？问题回答 继承Thread类实现 实现Runnable接口方式 实现Callable接口方式 其中前两种比较常用。但是，需要有返回值需要实现Callable接口。 继承Thread类实现12345678910/** * 继承Thread类，并重写run方法 */ public class MyThread extends Thread { @Override public void run() { super.run(); System.out.println(&quot;MyThread...&quot;); } } 实现Runnable接口方式1234567891011121314 /** * 实现Runnable接口，并重写run方法 */ public class MyRunnable implements Runnable{ @Override public void run() { System.out.println(&quot;MyRunnable...&quot;); } }// 调用MyRunnable runnable=new MyRunnable();Thread thread=new Thread(runnable);thread.start(); 实现Callable接口方式123456789101112131415161718192021/** * 实现Callable接口，并重写call方法 */ public class MyCallable implements Callable&lt;String&gt;{ @Override public String call() throws Exception { return &quot;MyCallable...&quot;; } } //创建和调用 MyCallable callable = new MyCallable(); ExecutorService eService = Executors.newSingleThreadExecutor(); Future&lt;String&gt; future = eService.submit(callable); //获取返回结果 try { String result=future.get(); System.out.println(result); } catch (Exception e) { e.printStackTrace(); } 注意 callable需要配合线程池使用 callable比runnable功能复杂一些Callable的call方法有返回值并且可以抛异常，而Runnable的run方法就没有返回值也没有抛异常，也就是可以知道执行线程的时候除了什么错误。 Callable运行后可以拿到一个Future对象，这个对象表示异步计算结果，可以从通过Future的get方法获取到call方法返回的结果。但要注意调用Future的get方法时，当前线程会阻塞，直到call方法返回结果。 说一下CAS锁机制？问题回答 CAS（Compare and Swap 比较并交换），是一种无锁算法，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS算法涉及到三个操作数 需要读写的内存位置(V) 进行比较的预期原值(A) 拟写入的新值(B) 3.如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B，否则处理器不做任何操作。 说一下ConcurrentHashMap？问题回答 ConcurrentHashMap是Java中的一个线程安全且高效的HashMap实现 JDK1.7 ConcurrentHashMap 由Segment 数组+HashEntry 组成，也就是数组+链表。 JDK1.7 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。支持 N 个 Segment 这么多次数的并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 JDK1.8抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性 JDK1.8将HashEntry改为了Node，和 1.8 HashMap 结构类似，当链表节点数超过指定阈值的话，会转换成红黑树的。 看看图片 jdk 1.7 ConcurrentHashMap jdk 1.7 ConcurrentHashMap jdk1.8 ConcurrentHashMap jdk1.8 ConcurrentHashMap 说一下Threadlocal关键字？问题回答 线程本地变量，可以为变量在每个线程中都创建一个副本，使每个线程都可以访问自己内部的副本变量 说一下乐观锁和悲观锁的区别？问题回答 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。synchronized、Lock属于悲观锁。Lock有三种实现类：ReentrantLock、ReadLock（读锁）和WriteLock（写锁）。 乐观锁：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁。 CAS属于乐观锁。 悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。 悲观锁对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 乐观锁不会上锁，在更新时会判断数据有没有被修改，一般会使用“数据版本机制”或“CAS操作”来实现。 数据版本机制实现数据版本一般有两种，第一种是使用版本号，第二种是使用时间戳。以版本号方式为例。 版本号方式：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。 当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。核心SQL代码： 1update table set xxx=#{xxx}, version=version+1 where id=#{id} and version=#{version}; CAS操作CAS（Compare and Swap 比较并交换），当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS操作中包含三个操作数——需要读写的内存位置(V)、进行比较的预期原值(A)和拟写入的新值(B)。如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B，否则处理器不做任何操作。 说一下事务特性?问题回答 事务特性指的就是ACID。 分别是原子性（Atomicity)、一致性（Consistency)、隔离性（Isolation）、持久性（Durability）。 分别解释下： 原子性：原子性是指事务包含的操作要么全部成功，要么全部失败。因此事务的操作成功就必须要完全应用到数据库。 一致性：一致性强调的是数据是一致性的。假设用户A和用户B两者的钱加起来一共是5000，那么不管A还是B如何转账，转几次账，事务结束后两个用户的钱加起来应该还是5000，这就是事务的一致性。 隔离性：当多个用户并发访问数据库时，多个并发事务是相互隔离的。事务之间不能相互干扰。 持久性：一个事务一旦被提交了，那么对数据库中的数据改变是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作 简要理解 也就是acid。 分别是原子性、一致性、隔离性、持久性。 原子性，要么同时成功要么同时失败。 一致性，数据应该是一致的。 隔离性，多个并发事务是相互隔离的。 持久性，事务提交，对数据的改变是永久的。 关于几个特性，补充理解 原子性，算是事务最基本的特性了。 一致性，感觉像事务的目标，其他的三个特性都是为了保证数据一致性存在的。 隔离性，为了保证并发情况下的一致性而引入，并发状态下单靠原子性不能完全解决一致性的问题，在多个事务并发进行的情况下，即使保证了每个事务的原子性，仍然可能导致数据不一致。比如，事务1需要将100元转入帐号A：先读取帐号A的值，然后在这个值上加上100。但是，在这两个操作之间，另一个事务2将100元转入帐号A，为它增加了100元。那么最后的结果应该是A增加了200元。但事实上，事务1最终完成后，帐号A只增加了100元，因为事务1覆盖了事务2的修改结果。 持久性，好理解，事务一旦提交，对数据库的影响是永久的，保证所有操作都是有效。 看图 事务特性 说一下互斥锁/读写锁？问题回答 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁，就是具体的实现。 一次只能一个线程拥有互斥锁，其他线程只有等待 互斥锁在Java中的具体实现就是ReentrantLock。 读写锁在Java中的具体实现就是ReadWriteLock。 说一下偏向锁/轻量级锁/重量级锁？问题回答 这三种锁是指锁状态，并且是针对Synchronized。在Java 5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让他申请的线程进入阻塞，性能降低。 说一下公平锁/非公平锁？问题回答 公平锁是指多个线程按照申请锁顺序来获取锁。 非公平锁是指多个线程获取锁的顺序并不是按照申请锁顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。 对于Java ReetrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。 对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。 说一下分段锁？问题回答 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。 我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7和JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock）。 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在哪一个分段中，然后对分段加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行插入。 但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。 分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 说一下可重入锁？问题回答 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 对于Java ReetrantLock而言，从名字就可以看出是一个重入锁，其名字是Re entrant Lock 重新进入锁。 对于Synchronized而言，也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。 看代码理解可重入锁 可重入锁 上面的代码就是一个可重入锁的一个特点。如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。 说一下对象锁和类锁？问题回答 java的对象锁和类锁在锁的概念上基本上和内置锁是一致的，但是，实际区别大 对象锁是用于对象实例方法，或者一个对象实例上的 类锁是用于类的静态方法或者一个类的class对象上的。 我们知道，类的对象实例可以有很多个，但是每个类只有一个class对象，所以不同对象实例的对象锁是互不干扰的，但是每个类只有一个类锁。但是有一点必须注意的是，其实类锁只是一个概念上的东西，并不是真实存在的，它只是用来帮助我们理解锁定实例方法和静态方法的区别的 说一下死锁？问题回答 Java发生死锁的根本原因是：在申请锁时发生了交叉闭环申请。即线程在获得了锁A并且没有释放的情况下去申请锁B，这时，另一个线程已经获得了锁B，在释放锁B之前又要先获得锁A，因此闭环发生，陷入死锁循环。 说一下独享锁/共享锁？问题回答 独享锁是指该锁一次只能被一个线程所持有。 共享锁是指该锁可被多个线程所持有。 对于Java ReentrantLock（重入锁）而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。 读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。 独享锁与共享锁也是通过AQS（AbstractQuenedSynchronizer抽象的队列式同步器）来实现的，通过实现不同的方法，来实现独享或者共享。 对于Synchronized而言，当然是独享锁。 说一下自旋锁？问题回答 在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁 优点是减少线程上下文切换的消耗 缺点是循环会消耗CPU。 进程和线程的区别？问题回答 程序被载入到内存中并准备执行，它就是一个进程 单个进程中执行中每个任务就是一个线程 一个线程只能属于一个进程，但是一个进程可以拥有多个线程 静态方法是否线程安全？问题回答 看静态方法是否引起线程安全问题要看在静态方法中是否使用了静态成员。 如果该静态方法不去操作一个静态成员，只在方法内部使用实例字段(instance field)，不会引起安全性问题 如果该静态方法操作了一个静态字段，则需要在静态方法中采用互斥访问的方式进行安全处理。 redis如何新节点如何同步 segment 分多少段 top Jstack pid G1 ====","link":"/2022/02/10/12000%E9%9D%A2%E8%AF%95/%E9%9D%A2%E8%AF%95%E2%80%94%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91/"},{"title":"","text":"服务端如何防止重复支付 如图是一个简化的下单流程，首先是提交订单，然后是支付。支付的话，一般是走支付网关（支付中心），然后支付中心与第三方支付渠道（微信、支付宝、银联）交互，支付成功以后，异步通知支付中心，支付中心更新自身支付订单状态，再通知业务应用，各业务再更新各自订单状态。 这个过程中经常可能遇到的问题是掉单，无论是超时未收到回调通知也好，还是程序自身报错也好，总之由于各种各样的原因，没有如期收到通知并正确的处理后续逻辑等等，都会造成用户支付成功了，但是服务端这边订单状态没更新，这个时候有可能产生投诉，或者用户重复支付。 由于③⑤造成的掉单称之为外部掉单，由④⑥造成的掉单我们称之为内部掉单 为了防止掉单，这里可以这样处理： 1、支付订单增加一个中间状态“支付中”，当同一个订单去支付的时候，先检查有没有状态为“支付中”的支付流水，当然支付（prepay）的时候要加个锁。支付完成以后更新支付流水状态的时候再讲其改成“支付成功”状态。 2、支付中心这边要自己定义一个超时时间（比如：30秒），在此时间范围内如果没有收到支付成功回调，则应调用接口主动查询支付结果，比如10s、20s、30s查一次，如果在最大查询次数内没有查到结果，应做异常处理 3、支付中心收到支付结果以后，将结果同步给业务系统，可以发MQ，也可以直接调用，直接调用的话要加重试（比如：SpringBoot Retry） 4、无论是支付中心，还是业务应用，在接收支付结果通知时都要考虑接口幂等性，消息只处理一次，其余的忽略 5、业务应用也应做超时主动查询支付结果 对于上面说的超时主动查询可以在发起支付的时候将这些支付订单放到一张表中，用定时任务去扫 为了防止订单重复提交，可以这样处理： 1、创建订单的时候，用订单信息计算一个哈希值，判断redis中是否有key，有则不允许重复提交，没有则生成一个新key，放到redis中设置个过期时间，然后创建订单。其实就是在一段时间内不可重复相同的操作 附上微信支付最佳实践：","link":"/2021/07/14/12000%E9%9D%A2%E8%AF%95/%E9%A1%B9%E7%9B%AE%E8%AE%BE%E8%AE%A1/"},{"title":"Linux虚拟文件系统","text":"Linux虚拟文件系统如何查看磁盘的文件占用情况?1.命令df 12345678$ df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/vda1 ext4 30G 30G 0 100% /devtmpfs devtmpfs 489M 0 489M 0% /devtmpfs tmpfs 497M 0 497M 0% /dev/shmtmpfs tmpfs 497M 50M 447M 11% /runtmpfs tmpfs 497M 0 497M 0% /sys/fs/cgroup 2.执行du 命令查看磁盘占用情况，把各个目录文件的大小相加，发现并没有占满磁盘，有10多G空间莫名失踪。 1$ du -h --max-depth=1 /home16M /home/logs11G /home/serverdog11G /home 3.为何会出现这样的情况呢？ 因为虽然文件已被删除，但是一些进程仍然打开这些文件，因此其占用的磁盘空间并没有被释放。执行lsof 命令显示打开已删除的文件。将有问题的进程重启（或，清空），磁盘空间就会得到释放。 1234567lsof | grep deletemysqld 2470 mysql 4u REG 253,1 0 523577 /var/tmp/ibfTeQFn (deleted) mysqld 2470 mysql 5u REG 253,1 0 523579 /var/tmp/ibaHcIdW (deleted) mysqld 2470 mysql 6u REG 253,1 0 523581 /var/tmp/ibLjiALu (deleted) mysqld 2470 mysql 7u REG 253,1 0 523585 /var/tmp/ibCFnzTB (deleted) mysqld 2470 mysql 11u REG 253,1 0 523587 /var/tmp/ibCjuqva (deleted) 那么，Linux 的文件系统，到底为什么这么设计呢？ 什么是虚拟文件系统（VFS：virtual filesystem）？什么是通用文件模型？超级块对象（superblock object）索引节点对象（inode object）文件对象（file object）目录项对象（dentry object）文件的概念文件的表达内存表达磁盘表达目录树的构建软链接 vs 硬链接文件 &amp; 磁盘管理索引节点状态文件 &amp; 进程管理操作：打开&amp;删除 虚拟文件系统（virtual filesystem）下图显示了 Linux 操作系统中负责文件管理的基本组件。上半区域为用户模式，下半区域为内核模式。应用程序使用标准库libc来访问文件，库将请求映射到系统调用，以便进入内核模式。 所有与文件相关的操作的入口都是虚拟文件系统（VFS），而非特定的额文件系统（如Ext3、ReiserFS和NFS）。VFS 提供了系统库和特定文件系统之间的接口。因此，VFS 不仅充当抽象层，而且实际上它提供了一个文件系统的基本实现，可以由不同的实现来使用和扩展。因此，要了解文件系统是如何工作的，就要先了解VFS 。 通用文件模型VFS 的主要思想在于引入了一个通用文件模型（common file model）。通用文件模型由以下对象类型组成： 超级块对象（superblock object） 内存：文件系统安装时创建，存放文件系统的有关信息磁盘：对应于存放在磁盘上的文件系统控制块（filesystem control block） 索引节点对象（inode object） 内存：访问时创建，存放关于具体文件的一般信息（inode 结构）磁盘：对应于存放在磁盘上的文件控制块（file control block）每个索引节点对象都有一个索引节点号，唯一地标识文件系统的文件 文件对象（file object） 内存：打开文件时创建，存放 打开文件 与进程之间进行交互的有关信息（file 结构）打开文件信息，仅当进程访问文件期间存在于内核内存中。 目录项对象（dentry object） 内存：目录项一旦被读入内存，VFS就会将其转换成dentry 结构的目录项对象磁盘：特定文件系统以特定的方式存储在磁盘上存放目录项（即，文件名称）与对应文件进行链接的有关信息 目录树综合来说，Linux 的 根文件系统(system’s root filessystem) 是内核启动mount的第一个文件系统。内核代码映像文件保存在根文件系统中，而系统引导启动程序会在根文件系统挂载之后，从中把一些基本的初始化脚本和服务等加载到内存中去运行（文件系统和内核是完全独立的两个部分）。其他文件系统，则后续通过脚本或命令作为子文件系统安装在已安装文件系统的目录上，最终形成整个目录树。 12345678start_kernel vfs_caches_init mnt_init init_rootfs // 注册rootfs文件系统 init_mount_tree // 挂载rootfs文件系统 … rest_init kernel_thread(kernel_init, NULL, CLONE_FS); 就单个文件系统而言，在文件系统安装时，创建超级块对象；沿树查找文件时，总是首先从初识目录的中查找匹配的目录项，以便获取相应的索引节点，然后读取索引节点的目录文件，转化为dentry对象，再检查匹配的目录项，反复执行以上过程，直至找到对应的文件的索引节点，并创建索引节点对象。 软链接 vs 硬链接软链接是一个普通的文件，其中存放的是另外一个文件的路径名。硬链接则指向同一个索引节点，硬链接数记录在索引节点对象的 i_nlink 字段。当i_nlink字段为零时，说明没有硬链接指向该文件。 文件 &amp; 进程管理下图是一个简单示例，说明进程是怎样与文件进行交互。三个不同进程打开同一个文件，每个进程都有自己的文件对象，其中两个进程使用同一个硬链接（每个硬链接对应一个目录对象），两个目录项对象都指向同一个 索引节点对象。 索引节点的数据又由两部分组成：内存数据和磁盘数据。Linux 使用 Write back 作为索引节点的数据一致性策略。对于索引节点的数据，当文件被打开时，才会加载索引节点到内存；当不再被进程使用，则从内存踢出；如果中间有更新，则需要把数据写回磁盘。 123* &quot;in_use&quot; - valid inode, i_count &gt; 0, i_nlink &gt; 0* &quot;dirty&quot; - as &quot;in_use&quot; but also dirty* &quot;unused&quot; - valid inode, i_count = 0 索引节点是否仍在使用，是通过 open() 和 close() 操作建立和销毁文件对象，文件对象通过索引节点提供的 iget 和 iput 更新索引节点的i_count字段，以完成使用计数。open 操作使得 i_count 加一， close 操作使得 i_count 减一。在 close 操作时判断索引节点是否释放，如果 i_count = 0，则意味着不再有进程引用，将会从内存释放。 文件 &amp; 磁盘管理文件与磁盘管理联系最紧密的操作，莫过于touch和rm操作，而尤以后者最为关键。通过strace(或 dtruss)，查看 rm 的实际的系统调用 1234567# dtruss rm tmp...geteuid(0x0, 0x0, 0x0) = 0 0ioctl(0x0, 0x4004667A, 0x7FFEE06F09C4) = 0 0lstat64(&quot;tmp\\0&quot;, 0x7FFEE06F0968, 0x0) = 0 0access(&quot;tmp\\0&quot;, 0x2, 0x0) = 0 0unlink(&quot;tmp\\0&quot;, 0x0, 0x0) = 0 0 可以发现 rm 实际是通过 unlink 完成的。unlink代表删除目录项，以及减少其索引节点的计数。由通用文件模型可知，父目录本身同样是一个文件，也就意味着目录项是其文件数据的一部分。删除目录项等价于从父目录的文件中删除数据，也就意味着首先要打开父目录的文件。那么，删除操作即可理解为： 删除命令（一个进程）使用 open 操作获得父目录文件对象 通过 iget 增加 目录文件的索引节点对象计数 读取目录文件数据 将目录文件数据转化为目录项对象 由于目录项包含文件的索引节点，类似的，需要通过 iget 增加文件的索引节点对象计数 删除目录的目录项 减少文件索引节点对象的硬链接计数i_nlink 通过 iput 结束对文件索引节点对象的操作，使用计数 i_count 减一 判断i_count是否为零，如果为零，则释放内存 然后，判断i_nlink是否为零，如果为零，则释放磁盘空间 通过 iput 结束对目录索引节点对象的操作。 回头来看遇到的问题，其实可以从两个角度来理解： 索引与数据文件系统与文件、磁盘管理与文件、进程管理与文件，最核心的都是文件的索引，而不是文件的数据。把数据和索引分开是理解文件系统的关键。 缓存策略由于操作系统使用 Write back 的策略，意味着只有先释放内存，才有可能释放磁盘。 Why lsof ?从上面的模型可以很清楚的理解，因为目录已经没有索引到文件了，但是打开文件还有索引到文件，所以不能立刻释放磁盘空间。 为什么 lsof 可以找到已删除未释放的文件呢？ lsof，顾名思义：list open files，该命令的原理就是查找打开文件的列表，因此可以找到已删除未释放的文件。","link":"/2022/05/08/13000Linux/13001Linux%E5%91%BD%E4%BB%A4lsof/"},{"title":"Linux命令df","text":"Linux命令df检查文件系统的磁盘使用情况df(disk free)： 12345678910$ df -h 使用 挂载Filesystem Size Used Avail Capacity iused ifree %iused Mounted on/dev/disk1s1 466Gi 222Gi 243Gi 48% 1489241 9223372036853286566 0% /devfs 190Ki 190Ki 0Bi 100% 656 0 100% /dev/dev/disk1s4 466Gi 20Ki 243Gi 1% 0 9223372036854775807 0% /private/var/vmmap -hosts 0Bi 0Bi 0Bi 100% 0 0 100% /netmap auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /home/dev/* 设备路径，磁盘管理","link":"/2022/05/15/13000Linux/13003Linux%E5%91%BD%E4%BB%A4df/"},{"title":"Linux命令find","text":"Linux命令find 如何删除最后修改时间超过一年的日志文件 1find . -type f -mtime +365 -exec rm -rf {} \\; 1.按照名称或正则表达式查找1234# 特定名称find . -name test.txt# pdf文件 *通配符find ./books -name &quot;*.pdf&quot; 2.查找不同类型12-type d 找目录-type l 符号链接 3.时间戳找文件Linux有3个时间戳： - 访问时间戳(atime)： 最后一次读取文件的时间 - 修改时间戳(mtime)：最后被修改时间 - 更改时间戳(ctime)：上次更改文件元数据的时间（如：所有权、位置、文件类型和权限设置） 123456# 一年没读的文件find . -type f -atime +365# 5天前文件find . -type f -mtime 5# 5~10天前的文件 -10=小于10find . -type f -ctime +5 -ctime -10 4.按大小查找-size - b: 512字节块(默认) c: 字节 w: 双字节 - k：KB - M：MB - G: GB 1find . -type f -size +10M -size -1G 5.权限查找1find -type f -user 用户名 6.找到文件后执行命令123find . -type f -atime +365 -exec rm -rf {} \\;# {}是查找结果的占位符。如果不用它，命令会执行所有文件。（删库跑路了）# \\; \\转义字符","link":"/2022/05/10/13000Linux/13002Linux%E5%91%BD%E4%BB%A4find/"},{"title":"","text":"[TOC] 1.为什么spring循环依赖需要3级缓存 2.本地方法栈如何理解 ▲ 38 HashMap 与 ConcurrentHashMap 的实现原理是怎样的？ConcurrentHashMap 是如何保证线程安全的？ ▲ 29 synchronized 关键字底层是如何实现的？它与 Lock 相比优缺点分别是什么？ ▲ 24 简述 JVM 的内存模型 JVM 内存是如何对应到操作系统内存的？ ▲ 20 集合类中的 List 和 Map 的线程安全版本是什么，如何保证线程安全的？ ▲ 14 Java 线程和操作系统的线程是怎么对应的？Java线程是怎样进行调度的? ▲ 14 ThreadLocal 实现原理是什么？ ▲ 11 简述 BIO, NIO, AIO 的区别 ▲ 8 简述 Spring AOP 的原理 ▲ 2 简述 Java 的 happen before 原则 ▲ 1 SpringBoot 是如何进行自动配置的？ 字节码编译过程？ 2.0-spring的上下文切换，如何优化上下文来减少系统资源的消耗？springboot自动配置的原理？spring的自动装配？Springmvc的工作原理？spring如何解决循环依赖？spring事务传播行为？ 4.0-redis如何实现延时队列？redis崩溃时如何处理数据？5.0-hashmap数据结构？ Redisson实现分布式锁的原理 Redis为什么能通过Lua脚本保证并发的线程安全？ Redis在内存不足时，淘汰机制是怎样的？ Redis可以做消息队列吗？ 数据库用到的数据类型（char在什么场景会用到） binlog有什么作用？MySQL崩溃回复机制依赖什么日志？ 消息消费顺序的要求 Nacos的服务注册与发现原理 接口之间的通讯协议 微服务交互采取的序列化方式 多线程使用实例 线程池是如何创建，有哪些参数，分别起什么作用 是否遇到过线程安全的问题 JAVA基础 https://blog.csdn.net/guorui_java/article/details/108153368 https://www.zhihu.com/question/60949531 HashMap底层实现原理，什么时候变成红黑树 jvm如何排查线上问题 内存溢出 线程池超出问题 线程池原理 经常用那些线程池 为什么 并发场景 重入锁 同步锁 redis用过那些数据结构 用redis队列来做延迟队列有什么优劣势 Redis 中的过期删除策略和内存淘汰机制kafka如何保证消息不丢失 不重复消费 kafka消息积压怎么处理 怎么产生的消息积压？ JAVA基础+JVM1.Java类初始化顺序​ 基类静态代码块，基类静态成员字段——&gt;派生类静态代码块，派生类静态成员字段——&gt;基类普通代码块，基类普通成员字段——&gt;基类构造函数——&gt;派生类普通代码块，派生类普通成员字段——&gt;派生类构造函数 2.对⽅法区和永久区的理解以及它们之间的关系​ 方法区是jvm规范里要求的，永久区是Hotspot虚拟机对方法区的具体实现，前者是规范，后者是实现方式。 12345678-XX:NewSize=1024m：设置年轻代初始值为1024M。-XX:PermSize=256m：设置持久代初始值为256M。-XX:MaxPermSize=256m：设置持久代最大值为256M。-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的比值。表示2个Survivor区（JVM堆内存年轻代中默认有2个大小相等的Survivor区）与1个Eden区的比值为2:4，即1个Survivor区占整个年轻代大小的1/6。JDK8中用metaspace代替permsize，因此在许多我们设置permsize大小的地方同样需要修改配置为metaspace将-XX:PermSize=200m;-XX:MaxPermSize=256m;=======&gt; 修改为：-XX:MetaspaceSize=200m;-XX:MaxMetaspaceSize=256m; 3.局部变量使⽤前需要显式地赋值，否则编译通过不了，为什么这么设计​ 这样设计是一种约束，尽最大程度减少使用者犯错的可能。 4.Semaphore拿到执⾏权的线程之间是否互斥​ Semaphore拿到执⾏权的线程之间是否互斥，Semaphore、CountDownLatch、CyclicBarrier、 Exchanger 为java并发编程的4个辅助类，⾯试中常问的 CountDownLatch CyclicBarrier之间的区别，⾯试 者肯定是经常碰到的， 所以问起来意义不⼤，Semaphore问的相对少⼀些，有些知识点如果没有使⽤过还 是会忽略，Semaphore可有多把锁，可允许多个线程同时拥有执⾏权，这些有执⾏权的线程如并发访问同⼀对象，会产⽣线程安全问题。 5.双层检测锁 - 单例模式​ 双检锁+volatile 12345678910111213public class Singleton { private Singleton() {} private volatile static Singleton instance; public static Singleton getInstance() { if (null == instance) { synchronized (Singleton.class) { if (null == instance) { instance = new Singleton(); } } } return instance; }} 6.对象在内存中占多少字节(Markword) ：16个字节，大小为8字节倍数：\u00101Markword 8字节 2类型指针Class pointer 3实例数据instance data 4对齐paddingint 4个字节 bool 1个字节 String 是个引用4个字节Markword中包含：1.hashcode计算一次就存下来 2.锁的信息 B树和B+树是解决什么样的问题的，怎样演化过来，之间区别​ B树和B+树，这题既问mysql索引的实现原理，也问数据结构基础，首先从二叉树说起，因为会产生退化现象，提出了平衡二叉树，再提出怎样让每一层放的节点多一些来减少遍历高度，引申出m叉树，m叉搜索树同样会有退化现象，引出m叉平衡树，也就是B树，这时候每个节点既放了key也放了value，怎样使每个节点放尽可能多的key值，以减少遍历高度呢（访问磁盘次数），可以将每个节点只放key值，将value值放在叶子结点，在叶子结点的value值增加指向相邻节点指针，这就是优化后的B+树。然后谈谈数据库索引失效的情况，为什么给离散度低的字段（如性别）建立索引是不可取的，查询数据反而更慢，如果将离散度高的字段和性别建立联合索引会怎样，有什么需要注意的？ 写⼀个⽣产者消费者模式​ 生产者消费者模式，synchronized锁住一个LinkedList，一个生产者，只要队列不满，生产后往里放，一个消费者只要队列不空，向外取，两者通过wait()和notify()进行协调，写好了会问怎样提高效率，最后会聊一聊消息队列设计精要思想及其使用。 synchronized代码块通过javap生成的字节码中包含 monitorenter 和 monitorexit指令。 1234567891011121314151617181920212223242526272829public static void main(String[] args) { final Object lock = new Object(); new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;1.thread A is waiting to get lock&quot;); synchronized (lock) { try{ // &quot;2 thread A get lock&quot; TimeUnit.SECONDS.sleep(1); // &quot;3 thread A do wait method&quot; lock.wait(); System.out.println(&quot;7 wait end&quot;); }catch(InterruptedException e) { e.printStackTrace(); } } } }).start(); new Thread(new Runnable() { @Override public void run () { System.out.println(&quot;4 thread B is waiting to get lock&quot;); synchronized (lock) { // &quot;5 thread B get lock&quot; TimeUnit.SECONDS.sleep(5); lock.notify(); // &quot;6 thread B do notify method&quot; } } }).start(); } 在多核环境下，线程A和B有可能同时执行monitorenter指令，并获取lock对象关联的monitor，只有一个线程可以和monitor建立关联，假设线程A执行加锁成功；notifyAll方法的注释：notifyAll方法会唤醒monitor的wait set中所有线程。什么是monitor？在HotSpot虚拟机中，monitor采用ObjectMonitor实现。 123456789ObjectMonitor { _header = null; _count = 0; _waiters = 0; _owner = null; // _WaitSet = null; // 处于wait状态的线程，会被加入到wait set； _EntryList = null; // 处于等待锁block状态的线程，会被加入到entry set； OwnerIsThread = 0;} wait方法实现1、将当前线程封装成ObjectWaiter对象node；2、通过ObjectMonitor::AddWaiter方法将node添加到_WaitSet列表中3、通过ObjectMonitor::exit方法释放当前的ObjectMonitor对象，这样其它竞争线程就可以获取该ObjectMonitor对象。4、最终底层的park方法会挂起线程； notify方法实现1、如果当前_WaitSet为空，即没有正在等待的线程，则直接返回；2、通过ObjectMonitor::DequeueWaiter方法，获取_WaitSet列表中的第一个ObjectWaiter节点，实现也很简单。这里需要注意的是，在jdk的notify方法注释是随机唤醒一个线程，其实是WaitSet列表第一个ObjectWaiter3、根据不同的策略，将取出来的ObjectWaiter节点，加入到_EntryList或则通过 notifyAll方法实现lock.notifyAll()方法最终通过ObjectMonitor的void notifyAll(TRAPS)实现：通过for循环取出_WaitSet的ObjectWaiter节点，并根据不同策略，加入到_EntryList或则进行自旋操作。从JVM的方法实现中，可以发现：notify和notifyAll并不会释放所占有的ObjectMonitor对象，其实真正释放ObjectMonitor对象的时间点是在执行monitorexit指令，一旦释放ObjectMonitor对象了，entry set中ObjectWaiter节点所保存的线程就可以开始竞争ObjectMonitor对象进行加锁操作了ßßß 写⼀个死锁123456789101112131415161718192021public class DeadLock { public static void main() { new Thread(new Runnable() { @Override public void run() { synchronized(lock1) { synchronized(lock2) {} } } }).start(); new Thread(new Runnable(){ @Override public void run() { synchronized(lock2) { synchronized(lock1) {} } } }).start(); }} cpu 100%怎样定位1234567// 先用top定位最耗cpu的java进程 例如: 12430$ top或者 htop（高级） 键入 P （大写P），按照cpu进行排序$ top -p 12430 -H 定位到最耗cpu的线程 的ID 例如：12483$ top -Hp 1865 ，显示一个进程的线程运行信息列表 键入P (大写p)，线程按照CPU使用率排序把第二步定位的线程ID，转成16进制，printf &quot;%x&quot; 12483 得到 ：30c3从jstack 输出的线程快照中找到线程的对堆栈信息 jstack 12430 |grep 30c3 -A 60 |less方法：jstack 10765 | grep ‘0x2a34’ -C5 --color` String a = “ab”; String b = “a” + “b”; a == b 是否相等，为什么​ 常量池 可以⽤for循环直接删除ArrayList的特定元素吗？可能会出现什么问题？怎样解决​ 解决方案：用 Iterator。ArrayList中的remove方法会执行System.arraycopy方法，导致删除元素时涉及到数组元素的移动 新的任务提交到线程池，线程池是怎样处理​ 第一步 ：线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池里的线程都在执行任务，则执行第二步。​ 第二步 ：线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里进行等待。如果工作队列满了，则执行第三步。 第三步 ：线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。 AQS和CAS原理​ 抽象队列同步器AQS（AbstractQueuedSychronizer），如果说java.util.concurrent的基础是CAS的话，那么AQS就是整个Java并发包的核心了，ReentrantLock、CountDownLatch、Semaphore等都用到了它。AQS实际上以双向队列的形式连接所有的Entry，比方说ReentrantLock，所有等待的线程都被放在一个Entry中并连成双向队列，前面一个线程使用ReentrantLock好了，则双向队列实际上的第一个Entry开始运行。AQS定义了对双向队列所有的操作，而只开放了tryLock和tryRelease方法给开发者使用，开发者可以根据自己的实现重写tryLock和tryRelease方法，以实现自己的并发功能。 比较并替换CAS(Compare and Swap)，假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false，整个比较并替换的操作是一个原子操作。CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的相应值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，下面永远都不可能成功。​ CAS虽然比较高效的解决了原子操作问题，但仍存在三大问题：循环时间长开销很大。只能保证一个共享变量的原子操作。ABA问题。 synchronized底层实现原理​ synchronized (this)原理：涉及两条指令：monitorenter，monitorexit；再说同步方法，从同步方法反编译的结果来看，方法的同步并没有通过指令monitorenter和monitorexit来实现，相对于普通方法，其常量池中多了ACC_SYNCHRONIZED标示符。 JVM就是根据该标示符来实现方法的同步的：当方法被调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。 这个问题会接着追问：java对象头信息，偏向锁，轻量锁，重量级锁及其他们相互间转化。 volatile作⽤，指令重排相关​ 理解volatile关键字的作用的前提是要理解Java内存模型，volatile关键字的作用主要有两点：​ 1，多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据​ 2，代码底层执行不像我们看到的高级语言—-Java程序这么简单，它的执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率​ 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger。 AOP和IOC原理​ AOP 和 IOC是Spring精华部分，AOP可以看做是对OOP的补充，对代码进行横向的扩展，通过代理模式实现，代理模式有静态代理，动态代理，Spring利用的是动态代理，在程序运行过程中将增强代码织入原代码中。IOC是控制反转，将对象的控制权交给Spring框架，用户需要使用对象无需创建，直接使用即可。AOP和IOC最可贵的是它们的思想。 Spring怎样解决循环依赖的问题​ 什么是循环依赖，怎样检测出循环依赖，Spring循环依赖有几种方式，使用基于setter属性的循环依赖为什么不会出现问题，接下来会问：Bean的生命周期。 dispatchServlet怎样分发任务的 1）. 用户发请求–&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制。 2）.DispatcherServlet–&gt;HandlerMapping，HandlerMapping将会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器,多个HandlerInterceptor拦截器)。 3）.DispatcherServlet–&gt;HandlerAdapter,HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器。 4）.HandlerAdapter–&gt;处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理，并返回一个ModelAndView对象(包含模型数据，逻辑视图名) 5）.ModelAndView的逻辑视图名–&gt;ViewResolver，ViewResoler将把逻辑视图名解析为具体的View。 6）.View–&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构 7）.返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户。 mysql给离散度低的字段建⽴索引会出现什么问题，具体说下原因 先上结论：重复性较强的字段，不适合添加索引。mysql给离散度低的字段，比如性别设置索引，再以性别作为条件进行查询反而会更慢。 一个表可能会涉及两个数据结构(文件)，一个是表本身，存放表中的数据，另一个是索引。索引是什么？它就是把一个或几个字段（组合索引）按规律排列起来，再附上该字段所在行数据的物理地址（位于表中）。比如我们有个字段是年龄，如果要选取某个年龄段的所有行，那么一般情况下可能需要进行一次全表扫描。但如果以这个年龄段建个索引，那么索引中会按年龄值根据特定数据结构建一个排列，这样在索引中就能迅速定位，不需要进行全表扫描。为什么性别不适合建索引呢？因为访问索引需要付出额外的IO开销，从索引中拿到的只是地址，要想真正访问到数据还是要对表进行一次IO。假如你要从表的100万行数据中取几个数据，那么利用索引迅速定位，访问索引的这IO开销就非常值了。但如果是从100万行数据中取50万行数据，就比如性别字段，那你相对需要访问50万次索引，再访问50万次表，加起来的开销并不会比直接对表进行一次完整扫描小。 当然如果把性别字段设为表的聚集索引，那么就肯定能加快大约一半该字段的查询速度了。聚集索引指的是表本身数据按哪个字段的值来进行排序。因此，聚集索引只能有一个，而且使用聚集索引不会付出额外IO开销。当然你得能舍得把聚集索引这么宝贵资源用到性别字段上。 可以根据业务场景需要，将性别和其它字段建立联合索引，比如时间戳，但是建立索引记得把时间戳字段放在性别前面。 怎么做到的热加载？热部署和热加载都是基于类加载器实现的，热加载是服务器监听class等文件的改变然后对改变的文件进行局部加载，所以不会删除session，也不会释放内存。热部署就是全局部署，会清空session以及释放内存。自定义加载器实现热加载 用户自定义加载器需要继承ClassLoader，实现原理就是通过一个线程去监听文件的修改时间，然后重写findClass方法，把文件以流的形式读进来，然后调defineClass方法。在JDK1.2之后，双亲委派模式已经被引入到类加载体系中，因此不建议重写loadClass方法，只需要重写findClass就可以了 12345678910private bype[] loadClassData(String name) { FileInputStream inputStream = new FileInputStream(new File()); ByteOutputStream outStream = new ByteOutputStream(); int b = 0; while ((b = inputStream.read()) != -1) { outStream.write(b); } inputStream.close(); return outStream.toByteArray();} 分布式唯一ID是怎么实现的HashMap 与 ConcurrentHashMap 的实现原理是怎样的？ConcurrentHashMap 是如何保证线程安全的？jdk1.7中是采用Segment + HashEntry + ReentrantLock的方式进行实现的，而1.8中放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现 一、使用volatile保证当Node中的值变化时对于其他线程是可见的 二、使用table数组的头结点作为synchronized的锁来保证写操作的安全 三、当头结点为null时，使用CAS操作来保证数据能正确的写入。 ConcurrentHashMap中维护着一个Segment数组，每个Segment可以看做是一个HashMap。Segment本身继承了ReentrantLock，它本身就是一个锁。 12345678private static final int DEFAULT_CONCURRENCY_LEVEL = 16;while (ssize &lt; DEFAULT_CONCURRENCY_LEVEL) { ssize &lt;&lt;= 1;}static class Segment&lt;K,V&gt; extends ReentrantLock { } Java 中垃圾回收机制中如何判断对象需要回收？常见的 GC 回收算法有哪些？1.引用计数算法：两个对象互相引用就无法回收2.可达性分析算法：“GC Roots”的对象作为起始点，没有任何引用链相连则证明对象是不可用的 虚拟机栈 引用的对象 方法区中类静态属性 引用的对象 方法区常量 引用的对象 本地方法栈中JNI（即一般说的Native方法）引用的对象 •强引用，就是指在程序代码之中普遍存在的，类似A a = new A()这样的引用，只要强引用存在，垃圾回收器就不会回收掉被引用的对象；•软引用，用来描述一些还有用但并非必须的对象，对于软引用的对象，在系统将要发生内存溢出异常之前，将会把这些对象列入回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会出现内存溢出异常；•弱引用，也是用来描述非必需的对象，但是它的强度比软引用更弱，被弱引用关联的对象只能生存到下一次回收发生之前，当垃圾回收器工作时，无论当前内存是否足够，都会回收掉；•虚引用，它是最弱的一种引用关系，一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例、为一个对象设置引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 G1垃圾回收器：G1垃圾回收器相关数据结构 1.在HotSpot的实现中，整个堆被划分成2048左右个Region。每个Region的大小在1-32MB之间，具体多大取决于堆的大小。2.对于Region来说，它会有一个分代的类型，并且是唯一一个。即，每一个Region，它要么是young的，要么是old的。还有一类十分特殊的Humongous。所谓的Humongous，就是一个对象的大小超过了某一个阈值——HotSpot中是Region的1/2，那么它会被标记为Humongous。 每一个分配的Region，都可以分成两个部分，已分配的和未被分配的。它们之间的界限被称为top。总体上来说，把一个对象分配到Region内，只需要简单增加top的值。这个做法实际上就是bump-the-pointer。即每一次回收都是回收N个Region。这个N是多少，主要受到G1回收的效率和用户设置的软实时目标有关。每一次的回收，G1会选择可能回收最多垃圾的Region进行回收。与此同时，G1回收器会维护一个空间Region的链表。每次回收之后的Region都会被加入到这个链表中。每一次都只有一个Region处于被分配的状态中，被称为current region。在多线程的情况下，这会带来并发的问题。G1回收器采用和CMS一样的TLABs的手段。即为每一个线程分配一个Buffer，线程分配内存就在这个Buffer内分配。但是当线程耗尽了自己的Buffer之后，需要申请新的Buffer。这个时候依然会带来并发的问题。G1回收器采用的是CAS（Compate And Swap）操作。3.卡片 Card在每个分区内部又被分成了若干个大小为512 Byte卡片(Card)，标识堆内存最小可用粒度所有分区的卡片将会记录在全局卡片表(Global Card Table)中，分配的对象会占用物理上连续的若干个卡片，当查找对分区内对象的引用时便可通过记录卡片来查找该引用对象(见RSet)。每次对内存的回收，都是对指定分区的卡片进行处理。4.RS(Remember Set)RS(Remember Set)是一种抽象概念，用于记录从非收集部分指向收集部分的指针的集合。在传统的分代垃圾回收算法里面，RS(Remember Set)被用来记录分代之间的指针。在G1回收器里面，RS被用来记录从其他Region指向一个Region的指针情况。因此，一个Region就会有一个RS。这种记录可以带来一个极大的好处：在回收一个Region的时候不需要执行全堆扫描，只需要检查它的RS就可以找到外部引用，而这些引用就是initial mark的根之一。 那么，如果一个线程修改了Region内部的引用，就必须要去通知RS，更改其中的记录。为了达到这种目的，G1回收器引入了一种新的结构，CT(Card Table)——卡表。每一个Region，又被分成了固定大小的若干张卡(Card)。每一张卡，都用一个Byte来记录是否修改过。卡表即这些byte的集合。实际上，如果把RS理解成一个概念模型，那么CT就可以说是RS的一种实现方式。 在RS的修改上也会遇到并发的问题。因为一个Region可能有多个线程在并发修改，因此它们也会并发修改RS。为了避免这样一种冲突，G1垃圾回收器进一步把RS划分成了多个哈希表。每一个线程都在各自的哈希表里面修改。最终，从逻辑上来说，RS就是这些哈希表的集合。哈希表是实现RS的一种通常的方式之一。它有一个极大的好处就是能够去除重复。这意味着，RS的大小将和修改的指针数量相当。而在不去重的情况下，RS的数量和写操作的数量相当。 4.2.2 G1垃圾回收器执行步骤： 1、初始标记；初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS的值，让下一个阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这一阶段需要停顿线程，但是耗时很短， 2、并发标记；并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段时耗时较长，但可与用户程序并发执行。 3、最终标记；最终标记阶段则是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remenbered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这一阶段需要停顿线程，但是可并行执行。 4、筛选回收最后在筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。 1、HashMap了解吗？说说底层的数据结构，以及put方法中详细的流程jdk1.8后数组+链表+红黑树的结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; //缓存底层数组用,都是指向一个地址的引用 Node&lt;K,V&gt; p; //插入数组的桶i处的键值对节点 int n; //底层数组的长度 int i; //插入数组的桶的下标 //刚开始table是null或空的时候，初始化个默认的table；为tab和n赋值，tab指向底层数组，n为底层数组的长度 if ((tab = table) == null || (n = tab.length) == 0){ n = (tab = resize()).length; } //(n - 1) &amp; hash：根据hash值算出插入点在底层数组的桶的位置，即下标值；为p赋值，也为i赋值（不管碰撞与否，都已经赋值了） //如果在数组上，没有发生碰撞，即当前要插入的位置上之前没有插入过值，则直接在此位置插入要插入的键值对 if ((p = tab[i = (n - 1) &amp; hash]) == null){ tab[i] = newNode(hash, key, value, null);//插入的节点的next属性是null } else { //发生碰撞，即当前位置已经插入了值 Node&lt;K,V&gt; e; //中间变量吧，跟冒泡排序里面的那个中间变量似的，起到个值交换的作用 K k; //同上 //hash值相同，key也相同，那么就是更新这个键值对的值。同 jdk 1.7 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))){ //注意在这个if内【e != null】 e = p;//这地方，e = p 他们两个都是指向数组下标为i的地方，在这if else if else结束之后，再把节点的值value给更新了 } else if (p instanceof TreeNode){ //这个树方法可能会返回null。 //jdk 1.8引入了红黑树来处理碰撞，上面判断p的类型已经是树结构了， e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);//如果是，则走添加树的方法。 } else { //注意在这个else内，当为添加新节点时，【e == 】；更新某个节点时，就不是null for (int binCount = 0; ; ++binCount) {//还未形成树结构，还是jdk 1.7的链表结构 //差别就是1.7:是头插法，后来的留在数组上，先来的链在尾上；1.8:是先来的就留在数组上，后来的链在尾上 //判断p.next是否为空，同时为e赋值，若为空，则p.next指向新添加的节点，这是在链表长度小于7的时候 if ((e = p.next) == null) { //这个地方有个不好理解的地方：在判断条件里面，把e指向p.next，也就是说现在e=null而不是下下一行错误的理解。 //这也就解释了更新的时候，返回oldValue，新建的时候，是不在那地方返回的。 p.next = newNode(hash, key, value, null);//e = p.next,p.next指向新生成的节点，也就是说e指向新节点（错误） //对于临界值的分析： //假设此次是第六次，binCount == 6,不会进行树变，当前链表长度是7；下次循环。 //binCount == 7，条件成立，进行树变，以后再put到这个桶的位置的时候，这个else就不走了，走中间的那个数结构的分叉语句啦 //这个时候，长度为8的链表就变成了红黑树啦 if (binCount &gt;= TREEIFY_THRESHOLD - 1){// -1 for 1st //TREEIFY_THRESHOLD == 8 treeifyBin(tab, hash); } break;//插入新值或进行树变后，跳出for循环。此时e未重定向，还是指向null，虽然后面p.next指向了新节点。 //但是，跟e没关系。 } //如果在循环链表的时候，找到key相同的节点，那么就跳出循环，就走不到链表的尾上了。 // e已经在上一步已经赋值了，且不为null,也会跳出for循环，会在下面更新value的值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))){ break; } //这个就是p.next也就是e不为空，然后，还没有key相同的情况出现，那就继续循环链表， // p指向p.next也就是e，继续循环，继续，e=p.next p = e; //直到p.next为空，添加新的节点；或者出现key相等，更新旧值的情况才跳出循环。 } } //经过上面if else if else之后，e在新建节点的时候，为null；更新的时候，则被赋值。在树里面处理putTreeVal（）同样如此， if (e != null) { // existing mapping for key//老外说的对，就是只有更新的时候，才走这，才会直接return oldValue V oldValue = e.value; //onlyIfAbsent 这个在调用hashMap的put()的时候，一直是false，那么下面更新value是肯定执行的 if (!onlyIfAbsent || oldValue == null){ e.value = value; } afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold){ resize(); } afterNodeInsertion(evict); return null; } 2、Spring容器注入bean的方式有哪些1.@Component注解，加@ComponentScan扫描 2.配置类中加@Bean Spring 为啥把bean默认设计成单例？答案：为了提高性能！！！从几个方面：1.少创建实例2.垃圾回收3.缓存快速获取 3、说说线程池的参数，工作原理以及拒绝策略7个参数：核心线程数、最大线程数、活跃时间、时间单位、阻塞队列、线程工厂、拒绝策略3个方法：newSingleThreadExecutor newFixedThreadPool newCachedThreadPool4个策略 AbortPolicy抛弃异常 CallerRunsPolicy DiscardPolicy DiscardOldestPolicy 1234567class ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 4、Spring事务级别有哪些？5、说说服务注册中心的原理，比如Eureka（他们公司用的这个）Mysql的事务隔离级别知道吗？底层的实现原理是什么？什么是聚簇索引？HashMap的长度为什么要是2的N次方？h%len h&amp;(len-1) Object所有方法1.getClass() 2.hashCode 3.clone 4.toString 5.equals 6.finalize 7.wait 8.notify 9.nofityAll当对象变成GC Roots不可达时，GC会判断对象是否重写finalize方法，未覆盖则回收。否则若对象未执行过finalize方法，将其放入F-Queue队列。finalize执行完后，GC会再次判断对象是否可达，不可达则回收。 Sleep(),wait(),join(),yield()1.sleep是Thread类的静态本地方法，wait是Object类的本地方法。2.sleep方法不会释放锁Lock，但wait会释放，并且会加入等待队列中。3.wait必须依赖synchronized关键字4.sleep不需要唤醒，wait没指定时间需要被唤醒5.sleep当前线程休眠，wait用于多线程之间的通信。6.sleep会让出CPU执行时间且强制上下文切换，wait不一定，wait后还是有机会重新竞争到锁继续执行。 yield()执行后线程直接进入就绪状态，马上释放CPU执行权。join()执行后线程进入阻塞状态，main线程等待T1执行完毕 12345public static void main() { Thread t1 = new Thread(); t1.start(); t1.join(); //main线程等待T1执行完毕} 内存分配方式：指针碰撞和空闲列表1.指针碰撞：如果Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”（Bump the Pointer）。 2.空闲列表：如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”（Free List）。 选择哪种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。因此，在使用Serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep标记清除算法的收集器时，通常采用空闲列表。 内存分配的并发控制：CAS和TLAB对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。 主要通过两种方式解决:1.CAS加上失败重试分配内存地址。2.TLAB 为每个线程分配一块缓冲区域进行对象分配，new对象内存的分配均需要进行加锁，这也是new开销比较大的原因，所以Sun Hotspot JVM为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间，这块空间又称为TLAB，TLAB仅作用于新生代的Eden，因此在编写Java程序时，通常多个小的对象比大的对象分配起来更加高效。使用-XX：+/-UseTLAB。 内存分配文章 分布式问题分布式ID生成方式1.UUID 优点： 唯一性 缺点： 1.无序的字符串，不具备趋势自增特性 2.没有具体的业务含义 3.长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。2.数据库自增ID 优点：实现简单，ID单调自增，数值类型查询速度快 缺点：DB单点存在宕机风险，无法扛住高并发场景3.数据库集群模式 会生成重复的ID怎么办？ 解决方案：设置起始值和自增步长 123456# MySQL_1 配置：set @@auto_increment_offset = 1; -- 起始值set @@auto_increment_increment = 2; -- 步长# MySQL_2 配置：set @@auto_increment_offset = 2; -- 起始值set @@auto_increment_increment = 2; -- 步长 ​ 优点：解决DB单点问题​ 缺点：不利于后续扩容，而且实际上单个数据库自身压力还是大，依旧无法满足高并发场景。 4.号段模式 从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成1~1000的自增ID并加载到内存。 12345678910CREATE TABLE id_generator ( id int(10) NOT NULL, max_id bigint(20) NOT NULL COMMENT '当前最大id', step int(20) NOT NULL COMMENT '号段的布长', biz_type int(20) NOT NULL COMMENT '业务类型', version int(20) NOT NULL COMMENT '版本号 保证并发时数据的正确性', PRIMARY KEY (`id`)) # 取完更新表中数据 set max_id = #{max_id+step}, version = version + 1 where version = # {version} and biz_type = XXX# 多业务端可能同时操作，所以采用版本号version乐观锁方式更新，这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。 5.Redis Redis也同样可以实现，原理就是利用redis的 incr命令实现ID的原子性自增。 1234127.0.0.1:6379&gt; set seq_id 1 // 初始化自增ID为1OK127.0.0.1:6379&gt; incr seq_id // 增加1，并返回递增后的数值(integer) 2 用redis实现需要注意一点，要考虑到redis持久化的问题。redis有两种持久化方式RDB和AOF RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。 AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。 6.雪花算法（SnowFlake） Snowflake生成的是Long类型的ID，一个Long类型占8个字节，每个字节占8比特，也就是说一个Long类型占64个比特。Snowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。 第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。 时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69年 工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。 序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID 可能出现机器时针回拨，导致重复ID 7.Leaf segment 美团 分布式锁数据库：利用主键冲突控制一次只有一个线程获取锁 问题：非阻塞、不可重入、单点、失效时间 Zookeeper分布式锁： zk通过临时节点，解决了死锁的问题，一旦客户端获取锁后突然挂掉，临时节点就是会删除掉，其他客户端自动获取锁，临时顺序节点解决惊群效应。 Redis分布式锁：setNX，单线程处理网络请求，不需要考虑并发安全性 所有服务节点设置相同的key，返回为0，则锁获取失败 setnx:问题1.早期版本没有超时时间，expire设置，存在死锁问题 （中途机器下线）问题2.后期版本提供锁和设置时间原子操作，但是存在任务重试，锁自动释放，导致并发问题，加锁和解锁不是同一个线程 可重入性和锁续期没有实现，通过redisson解决（类似AQS的实现，看门狗监听机制） redlock Git问答patch可以通过邮件发送以及从邮件应用 git blame可以查看文件每一行的提交者 git grep可以搜索工作区文件，也可以搜索再暂存区注册过的blob对象。 git grep不能搜索指定tree对象中的blob对象内容 - X 如果希望执行git push的时候默认讲远程存在的分支所对应的本地分支都推送到远程，应该 git config –global push default matching tree对象存储多个tree对象或者blob对象的信息 tag对象存储commit对象的信息 同一个blob对象的信息可以存储在不同的tree对象当中 查看本地哪些分支在远程库中已经不存在， git remote show origin 提交了一个有密码的文件，git filter -branch –tree-filter ‘rm -f passwords.txt’ HEAD 如果希望git commit弹出编辑框编辑提交信息完成提交，不要忽略提交信息中以#开头的注释行，但还是忽略头尾的空行，尾部的空格，应该git config –global commit cleanup whitespace希望git能以中文命名路径和文件名，并且在跨平台上能正常显示，应该git config –global core.quotepath false当前处于develop分支，执行git merge fixes enhancements是指合并fixes和enhancements分支到当前分支Git remote add fork https://xxx 添加了一个远程仓库fork之后，执行git branch –set-upstream feature/foo fork/master，会报错，因为没有执行git fetch 获取fork远程仓库的信息。要查看本地哪些分支在远程仓库已经不存在了，使用git remote show origin Ngix什么是信号驱动IO模型：信号驱动IO是应用进程告诉内核：当你的数据报准备好的生活，给我发送一个信号，并且调用我的信号处理函数每一个NginxWorke进程持有一个Lua解释器或者LuaJIT实例，被这个worker处理的全部请求共享这个实例。ngx_lua将Lua嵌入Nginx，能够让Nginx运行lua脚本，并且高并发、非阻塞的处理各种请求。ngx_lua在Lua中进行的IO操作都会托付给Nginx的事件模型，从而实现非阻塞调用。Nginx反向代理配置proxy_buffers的使用，设置proxy_buffers缓存区。nginx配置文件中，events是块配置项；块配置项可以嵌套；块配置项由一个块配置项名和一对大括号组成； 面试回归正题：面试官问：项目中你对数据库做了什么优化？这个我会，自由发挥（比较简单） 面试官问：对数据库分库分表了解吗？用什么算法进行分库分表 ？分库分表我会，只会简单地操作，用什么算法不懂。 https://zhuanlan.zhihu.com/p/84224499 面试官问：说一下mysql索引的原理 ？这个简单我会，我从索引的数据结构分析。 面试官问：知道MySQL插入和查询分别用的是什么锁吗？这个简单，大家都会 知道悲观锁吗？了解多少？ 这个最简单 面试官问：说一下synchronized的优点和缺点，与lock进行比较？ 这个没有难度 面试官问：说一下ReetrantLock的内部实现 ？我看过ReetrantLock的源码，这个我会。 面试官问：说一下你对HashMap的理解？ 比较简单 ，我从hashmap的数据结构分析。 面试官问：现在有一亿条数据，要求你利用HashMap对数据进行去重并排序，你会怎么做？ 这个不会 ，有难度。 ​面试官问：Redisson分布式锁？这个我会，我在上家公司用过，在高并发用分布式锁防止订单Id重复。 面试官问：Netty高性能表现在哪些方面？这个我会，回答的范围比较广。 面试官问：maven如果你的项目里有两个包的版本冲突了 你是怎么解决的？我说：用 exclusion 标签排除掉不需要的 jar 包。 面试官问：JAVA实现多态的原理？不会。（平时只是简单用一下）面试官问：RPC的原理？不懂，上次也是这个面试题。 面试官问：https原理？不会 面试官问：HashMap在并发环境下会出现什么样的问题？这个不会，我上家公司所有的数据用pojo封装返回，没有用hashmap，这个方面不是了解很深。 面试官问：项目问题，主要是问我项目你做的业务。（这个自由发挥，原因是上家公司项目是你负责，所有面试题，最简单就是这个面试题） 还有很多问题不会回答。 三面总体来说，还是比较简单，面试官没有为难我，只是我不会，面试过程暗中提醒我，但是还是不会，这次面试回答个75%左右。 我跟面试官说：这次面试你对我有什么评价，他说，我基础还是可以，业务代码基本上没有问题，他说我虽然工作了5年，能力达不到5年开发经验的水平，你的能力顶多在3年开发经验的水平。 1. synchronized 关键字底层是如何实现的？它与 Lock 相比优缺点分别是什么？1.synchronized不需要自己解锁, lock需要有unlock操作 2.JDK1.6前synchronize是重量锁，无法获取锁就挂起线程，1.6后给锁增加了四种状态，无锁状态 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 synchronized总结： 锁状态 假定情况 竞争处理策略 偏向锁 假定获取锁的一直都是同一个线程 升级为轻量锁 轻量级锁 假定锁被占用时不会有其他线程获取 自旋等待，超时升级 重量级锁 最坏情况，经常发生竞争 直接将要获取锁的线程挂起 最后的比较总结 . synchronized lock 1.使用 不需要自己解锁 手动释放 2.加锁方式 将普通对象与类对象做为锁 专门的 Lock 类对象作为锁 3.中断 不能响应 可响应中断 4.竞争解决 一系列的锁膨胀策略 加入排队队列 5.公平性 非公平 公平/非公平 两种模式可选 6.锁类型 悲观锁 乐观锁（基于volatile和cas实现） 底层实现 1、synchronznized映射成字节码指令就是增加两个指令：monitorenter、monitorexit， 当一条线程执行时遇到monitorenter指令时，它会尝试去获得锁，如果获得锁，那么所计数器+1（为什么要加1，因为它是可重入锁，可根据这个琐计数器判断锁状态），如果没有获得锁，那么阻塞，当它遇到一个monitoerexit时，琐计数器会-1，当计数器为0时，就释放锁（tips：节码中出现的两个monitoerexit指令的原因是：一个正常执行-1，令一个异常时执行，这两个用goto的方式只执行一个） 2、Lock底层则基于volatile和cas实现 synchronized是一种互斥锁，一次只允许一个线程进入被锁住的代码块。 1.1 锁的对象： 修饰的实例方法，对应的锁是对象实例 修饰的是静态方法，对应的锁是当前类的Class实例 修饰的是代码块，对应的锁是传入syn的对象实例 1.2 synchronized的原理通过反编译可以发现，当修饰方法时，编译器会生成ACC_SYNCHRONIZED关键字用来标识。当修饰代码块时，会依赖monitorenter和monitorexit指令。 在内存中，对象一般由3个部分组成，对象头、对象实际数据和对齐填充。对象头Mark Word会纪录对象关于锁的信息。每个对象都会有一个与之对应的monitor对象，monitor对象中存储着当前持有锁的线程以及等待锁的线程队列。 1.3 JDK1.6后的优化JDK1.6之前是重量级锁，线程进入同步代码块、方法时，monitor对象就会把当前进入线程的id进行存储，设置Mark Word对象地址，并把阻塞的线程存储到monitor的等待线程队列中。加锁是依赖底层操作系统的mutex相关指令实现，所以会有用户态和内核态之间的切换，性能损耗十分明显。 JDK1.6之后引入偏向锁和轻量级锁在JVM层面实现加锁的逻辑，不依赖底层操作系统，就没有切换的消耗。所以，Mark Word对锁的状态纪录一共有4种：无锁、偏向锁、轻量级锁和重量级锁。 偏向锁：JVM认为只有某个线程才会执行同步代码，没有竞争的环境。Mark Word会直接纪录线程ID，只要线程来执行代码，会对比线程ID是否相等，相等则当前线程能直接获取得到锁，执行同步代码。如果不相等，则用CAS来尝试修改当前的线程ID，如果CAS修改成功，那还是能获取得到锁，执行同步代码。如果CAS失败了，说明有竞争环境，此时会撤销偏向锁升级成轻量级锁。 轻量级锁：当前线程会在栈帧下创建Lock Record，Lock Record会把Mark Word的信息拷贝进去，且有个Owner指针指向加锁的对象。线程执行到同步代码块时，则用CAS试图将Mark Word的指向到线程栈帧的Lock Record,假设CAS修改成功，则获取得到轻量级锁。修改失败，重试一定次数，升级成为重量级锁。 1.4总结synchronized锁原来只有重量级锁，依赖操作系统的mutex指令，需要用户态和内核态切换，性能消耗明显。重量级锁用到monitor对象，而偏向锁则在Mark Word纪录线程ID进行对比。轻量级锁则是拷贝Mark Word到Lock Record，用CAS+自旋的方式获取。锁只有升级没有降级。1、只有一个线程进入临界区，偏向锁。2、多个线程交替进入临界区，轻量级锁。3、多线程同时进入临界区，重量级锁。 2. Java 中垃圾回收机制中如何判断对象需要回收？常见的 GC 回收算法有哪些？ParNew GC CMS (Concurrent Mark Sweep) 基于标记-清除 Mark-Sweep 设计目标尽量减少停顿时间 内存碎片化问题 CMS会占用更多CPU资源，和用户线程争抢 Parallel GC JDK8中的默认GC 新生代和老年代GC是并行进行的 -XX:+UseParallelGC G1 GC ZGC JDK11中新引入低延迟垃圾回收器 GC调优思路 如果服务器一次卡顿时间比较长，一般是full gc时间过长，而应用追求的是卡顿（STW）时间短，可以接受多次卡顿，那么可以考虑调整加大young gen的比例，和提高进入老年代的年龄（默认15）来尽量避免FGC 选择合适的收集器很重要。要根据应用的特点。是追求吞吐量，还是追求最小停顿。 经常对照gc日志观察现实的情况，如多久时间一卡顿，来调整自己的收集器和相关的内存比例 在有限的物理资源条件下，要避免让用户接受过多的STW，可以考虑在半夜自动进行gc（System.gc()），虽然不一定生效，但可以观察下情况。多数情况下是会出发full gc的 大多数应用是可以接受频繁的mgc，但却不能接受full gc的长时间卡顿，所以在观察gc日志时一定要注意自己full gc的频率和触发条件（是由于内存担保，还是年龄到了，还是TO内存太小，导致每次都fgc） 3. SpringBoot 是如何进行自动配置的？自动配置指的是springboot会自动将一些配置类的bean装载到IOC容器。加上@Springbootapplication就能实现，这个注解包含@EnableAutoConfiguration。1.引入starter，启动依赖组件的时候，组件必须包含一个@Configuration配置类，再通过@Bean注解去声明需要装配到IOC容器的Bean对象。2.这个配置类是放在第三方Jar包里面，通过springboot约定大于配置的理念去把这个配置类的全路径放在classpath:/META-INF/spring.factories文件里面。通过SpringFacotoryLoader来实现3.通过ImportSelector接口来实现对这些配置类的动态加载从而去完成自动装配这样一个动作。 4. 简述 JVM 的内存模型 JVM 内存是如何对应到操作系统内存的？java的虚拟机种有两种线程，一种叫叫守护线程，一种叫非守护线程，main函数就是个非守护线程，虚拟机的gc就是一个守护线程。 java虚拟机的生命周期，当一个java应用main函数启动时虚拟机也同时被启动，而只有当在虚拟机实例中的所有非守护进程都结束时，java虚拟机实例才结束生命。 线程私有的：程序计数器，虚拟机栈，本地方法栈线程共享的：堆，方法区，直接内存 1.程序计数器：是唯一不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.虚拟机栈：虚拟机栈是由一个个栈帧组成，线程在执行一个方法时，便会向栈中放入一个栈帧，每个栈帧中都拥有局部变量表、操作数栈、动态链接、方法出口信息。局部变量表存放基本数据类型（boolean、byte、char、short、int、float、long、double）和对象引用（reference)。虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError：若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。 OutOfMemoryError：若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 3.本地方法栈：本地方法栈则为虚拟机使用到的 Native 方法服务 4.堆是Java 虚拟机所管理的内存中最大的一块，几乎所有的对象实例以及数组都在这里分配内存。也被称作GC堆，收集器基本都采用分代垃圾收集算法。“分代回收”是基于这样一个事实：对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。 5.方法区:它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 6.运行时常量池：运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用）既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 5.String 类能不能被继承？为什么？不可以，因为String类有final修饰符，而final修饰的类是不能被继承的 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence String s = new String(“xyz”); //创建了几个对象答案： 1个或2个， 如果”xyz”已经存在于常量池中，则只在堆中创建”xyz”对象的一个拷贝，否则还要在常量池中在创建一份 String为immutable, 不可更改的，每次String对象做累加时都会创建StringBuilder对象， 效率低下。String s1 = &quot;a&quot;;String s2 = &quot;b&quot;; String s3 = s1 + s2; // 等效于 String s3 = (new StringBuilder(s1)).append(s2).toString(); 设计成final的好处： 不可变线程安全 字符串常量池提升性能，修改数据是在内存重新开内存 String在Map中做key保证HashCode不变 6.请解释字符串比较之中“==”和equals()的区别？ ==：比较的是两个字符串内存地址（堆内存）的数值是否相等，属于数值比较； equals()：比较的是两个字符串的内容，属于内容比较。 7.分代回收的时候，用了什么算法？ 复制算法 标记-清除算法（Mark-Sweep） 扫描所有对象标记出需要回收的对象，在标记完成后扫描回收所有被标记的对象，所以需要扫描两遍。问题：回收效率略低；标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连 续内存而不得不提前触发另一次垃圾回收动作 标记-整理算法（Mark-Compact） 首先标记出所有需要回收的对象，在标记完成后，后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 问题：效率偏低；没有内存碎片；标记整理与标记清除算法的区别主要在于对象的移动。对象移动不单单会加重系统负担，同时需要全程暂停用户线程才能进行，同时所有引用 对象的地方都需要更新（直接指针需要调整） 回收器 回收对象和算法 回收器类型 Serial 新生代，复制算法 单线程(串行) Serial Old 老年代，标记整理算法 单线程(串行) Parallel Scavenge 新生代，复制算法 并行的多线程收集器 Parallel Old 老年代，标记整理算法 并行的多线程回收器 ParNew 新生代，复制算法 并行的多线程收集器 CMS 老年代，标记清除算法 并发的多线程回收器 G1 跨新生代和老年代；标记整理 + 化整为零 并发的多线程回收器 8.讲下类加载的过程？当程序要使用某个类时，如果该类还未被加载到内存中，则系统会通过加载，连接，初始化三步来实现这个类进行初始化。 加载 加载，是指Java虚拟机查找字节流（查找.class文件），并且根据字节流创建java.lang.Class对象的过程。 系统会创建唯一的Class对象，这个Class对象描述了这个类创建出来的对象的所有信息 链接 （1）验证阶段。主要的目的是确保被加载的类（.class文件的字节流）满足Java虚拟机规范，不会造成安全错误。 （2）准备阶段。负责为类的静态成员分配内存，并设置默认初始值。 （3）解析阶段。将类的二进制数据中的符号引用替换为直接引用。 初始化 只对static修饰的变量或语句块进行初始化。 如果初始化一个类的时候，其父类尚未初始化，则优先初始化其父类。 如果同时包含多个静态变量和静态代码块，则按照自上而下的顺序依次执行。 9.springboot自动配置的原理？配置属性 1234567@ConfigurationProperties(prefix = &quot;test&quot;)public class Demo { private String msg=&quot;default&quot;;}// application.yml中配置信息test: msg: bamboo 10.spring如何解决循环依赖？什么是循环依赖? 简单的说就是A依赖B，B依赖C，C依赖A这样就构成了循环依赖。 Spring解决循环依赖的方法就是如题所述的三级缓存、预曝光。Spring的三级缓存主要是singletonObjects、earlySingletonObjects、singletonFactories这三个Map： 1234567891011121314151617protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); //首先通过beanName从一级缓存获取bean if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) { //如果一级缓存中没有，并且beanName映射的bean正在创建中 synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); //从二级缓存中获取 if (singletonObject == null &amp;&amp; allowEarlyReference) { //二级缓存也没有 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); //从三级缓存获取 if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); //获取到bean this.earlySingletonObjects.put(beanName, singletonObject); //将获取的bean提升至二级缓存 this.singletonFactories.remove(beanName); //从三级缓存删除 } } } } return singletonObject;} 为何需要三级缓存解决循环依赖，而不是二级缓存 https://www.cnblogs.com/semi-sub/p/13548479.html谈谈你对Java平台的理解？“Java是解释执行”，这句话正确吗？我们开发的Java的源代码，首先通过Javac编译成为字节码（bytecode），然后，在运行时，通过 Java虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。 synchronized和ReentrantLock有什么区别？有人说synchronized最慢，这话靠谱吗？- 极客15 理解什么是线程安全。 synchronized、ReentrantLock等机制的基本使用与案例。 掌握synchronized、ReentrantLock底层实现；理解锁膨胀、降级；理解偏斜锁、自旋锁、轻量级锁、重量级锁等概念。 掌握并发包中java.util.concurrent.lock各种不同实现和案例分析。 11. CountDownLatch11.1CountDownLatch的用法CountDownLatch典型用法：1、某一线程在开始运行前等待n个线程执行完毕。将CountDownLatch的计数器初始化为new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减1 countdownLatch.countDown()，当计数器的值变为0时，在CountDownLatch上await()的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 CountDownLatch典型用法：2、实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的CountDownLatch(1)，将其计算器初始化为1，多个线程在开始执行任务前首先countdownlatch.await()，当主线程调用countDown()时，计数器变为0，多个线程同时被唤醒。 11.2CountDownLatch的不足CountDownLatch是一次性的，计算器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当CountDownLatch使用完毕后，它不能再次被使用。 12.如何优雅中断线程12.1调用stop、resume、suspend方法，不会释放锁持有的锁，容易出现死锁12.2使用interrupt中断线程interrupt(): 用于线程中断，该方法并不能直接中断线程，只会将线程的中断标志位改为true。它只会给线程发送一个中断状态，线程是否中断取决于线程内部对该中断信号做什么响应，若不处理该中断信号，线程就不会中断。interrupted(): 判断线程是否处于中断状态，该方法调用后会将线程的中断标志位重置为false。isInterrupted(): 判断线程是否处于中断状态。 1234567891011121314151617181920212223242526public class ThreadTest implements Runnable{ @Override public void run() { // 在线程体中对线程的中断标志位进行判断，若线程中断，则不再执行 while (!Thread.currentThread().isInterrupted()){ System.out.println(&quot;Thread is running&quot;); try { System.out.println(Thread.currentThread().getName() + &quot; &quot; + Thread.currentThread().isInterrupted()); Thread.sleep(100); } /** * 需要注意的是，当方法体中的代码抛出InterruptedException异常时，线程的中断标志位会复位成 * false，若不处理，外部中断线程时，内部也无法停止，所以在catch代码中手动处理，将线程中断 */ catch (InterruptedException e) { System.out.println(Thread.currentThread().getName() + &quot;:&quot; + Thread.currentThread().isInterrupted()); // 发生InterruptedException异常时，在catch中处理，中断线程 Thread.currentThread().interrupt(); e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + &quot;:&quot; + Thread.currentThread().isInterrupted()); } }} CPU每个周期做什么事情？ 反射的缺点是什么？如何优化？ 优点： 运行期类型的判断，动态加载类，提高代码灵活度。缺点： 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的java代码要慢很多。 13.Semaphore原理浅析和相关面试题分析问题1.semaphore初始化有10个令牌，11个线程同时各调用1次acquire方法，会发生什么？ 答案：拿不到令牌的线程阻塞，不会继续往下运行。 问题2.semaphore初始化有10个令牌，一个线程重复调用11次acquire方法，会发生什么？ 答案：线程阻塞，不会继续往下运行。可能你会考虑类似于锁的重入的问题，很好，但是，令牌没有重入的概念。你只要调用一次acquire方法，就需要有一个令牌才能继续运行。 问题3.semaphore初始化有1个令牌，1个线程调用一次acquire方法，然后调用两次release方法，之后另外一个线程调用acquire(2)方法，此线程能够获取到足够的令牌并继续运行吗？ 答案：能，原因是release方法会添加令牌，并不会以初始化的大小为准。 问题4.semaphore初始化有2个令牌，一个线程调用1次release方法，然后一次性获取3个令牌，会获取到吗？ 答案：能，原因是release会添加令牌，并不会以初始化的大小为准。Semaphore中release方法的调用并没有限制要在acquire后调用。 123456789101112 public static void main(String[] args) { int permitsNum = 2; final Semaphore semaphore = new Semaphore(permitsNum); System.out.println(&quot;permits:&quot;+semaphore.availablePermits()+ &quot;,tryAcquire(3,1,TimeUnit.SECONDS):&quot;+semaphore.tryAcquire(3,1, TimeUnit.SECONDS)); semaphore.release(); System.out.println(&quot;permits:&quot;+semaphore.availablePermits()+ &quot;,tryAcquire(3,1, TimeUnit.SECONDS):&quot;+semaphore.tryAcquire(3,1, TimeUnit.SECONDS));}permits:2,tryAcquire(3,1, TimeUnit.SECONDS):falsepermits:3,tryAcquire(3,1, TimeUnit.SECONDS):true 14.Java 线程和操作系统的线程是怎么对应的？Java线程是怎样进行调度的?每个继承java.lang.Thread的类，调用start方法之后，都调用start0()的native方法;start0()的native方法在openjdk里调用的是JVM_StartThread;JVM_StartThread最终调用的是操作系统的pthread_create()函数,有四个参数,我们写的run方法就是该函数的第三个参数. 15.简述新生代与老年代的区别？新生代又分为Eden和Survivor两个区。数据会首先分配到Eden区当中（当然也有特殊情况，如果是大对象(需要大量连续内存空间的java对象)那么会直接放入到老年代。对象在Survivor每熬过一次Minor GC，年龄就加1，当年龄达到一定的程度（默认为15）时，就会被晋升到老年代中。 Java对象包含：对象头、对象体和对齐字节。 16.@Component，@Service等注解是如何被解析的？浏览ContextNamespaceHandler 12345678910111213141516171819202122232425262728293031springframework/context/annotation/ClassPathBeanDefinitionScanner.class:105Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) // 读资源转换为BeanDefinition this.findCandidateComponents(basePackage); // 从classPath扫描组件，并转换为备选BeanDefinition，也就是要做的解析@Component的核心方法。public Set&lt;BeanDefinition&gt; findCandidateComponents(String basePackage) { this.addCandidateComponentsFromIndex(this.componentsIndex, basePackage); this.scanCandidateComponents(basePackage);}public class ClassPathScanningCandidateComponentProvider implements EnvironmentCapable, ResourceLoaderAwareprivate Set&lt;BeanDefinition&gt; scanCandidateComponents(String basePackage)// this.resourcePattern; 将package转化为ClassLoader类资源搜索路径packageSearchPath，例如：com.wl.spring.boot转化为classpath*:com/wl/spring/boot/**/*.classString packageSearchPath = &quot;classpath*:&quot; + this.resolveBasePackage(basePackage)+&quot;/**/*.class&quot;;// 加载搜素路径下的资源。 Resource[] resources = getResourcePatternResolver().getResources(packageSearchPath); for (Resource resource : resources) { //省略部分代码 if (resource.isReadable()) { for (Resource resource : resources) { if (resource.isReadable()) { MetadataReader metadataReader = getMetadataReaderFactory().getMetadataReader(resource); if (isCandidateComponent(sbd)) { // 判断是否是备选组件 ScannedGenericBeanDefinition sbd = new ScannedGenericBeanDefinition(metadataReader); sbd.setSource(resource); candidates.add(sbd); // 添加到返回结果的list } } } } @Component是任何Spring管理的组件的通用原型。@Repository、@Service和@Controller是派生自@Component。 1234public @interface Service { @AliasFor(annotation = Component.class) String value() default &quot;&quot;;} @Component是@Service的元注解 总结 1.将package转化为ClassLoader类资源搜索路径packageSearchPath 2.加载搜素路径下的资源。 3.isCandidateComponent 判断是否是备选组件。 内部调用的TypeFilter的match方法： AnnotationTypeFilter#matchself中metadata.hasMetaAnnotation处理元注解 metadata.hasMetaAnnotation=AnnotationMetadataReadingVisitor#hasMetaAnnotation 就是判断当前注解的元注解在不在metaAnnotationMap中。 AnnotationAttributesReadingVisitor#visitEnd()内部方法recursivelyCollectMetaAnnotations 递归的读取注解，与注解的元注解（读@Service，再读元注解@Component），并设置到metaAnnotationMap 4.添加到返回结果的list 17.Autowired Service Component Bean@Autowired是Spring的注解，Autowired默认先按byType，如果发现找到多个bean，则又按照byName方式比对，如果还有多个，则报出异常；@Resource 是JDK1.6支持的注解，默认按照名称(Byname)进行装配, 如果没有指定name属性，当注解写在字段上时，默认取字段名，按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。不过推荐使用@Resource一点，因为当接口有多个实现时@Resource直接就能通过name属性来指定实现类，而@Autowired还要结合@Qualifier注解来使用，且@Resource是jdk的注释，可与Spring解耦。 在SpringBoot中当我们要声明一个Bean的时候，我们可以在该类上加上 @Service，@Compont 等，或者是在配置类中加上 @Bean 这个注解，除此之外还有一种方法，就是 @Import java的基本数据类型和字节数mysqI索引结构,特点，为什么使用这个聚集索引和非聚集索引String，StringBuffer, StringBuilder区别 HashMap，为什么使用红黑树 子类和父类的实例变量和方法有什么区别 java泛型 悲观锁和乐观锁 mysql底层原理，为什么效率高，主键能不能太大，为什么 linux查询tcp连接处理CLOSE_ WAIT的状态的数目 RabbitMQ, kafka, RocketMQ, ActiveMQ, 以及其他消息中间件 redis为什么效率高，线程，数据结构,网络模型，aio, nio, bio, 为什么这么设计？如何处理高并发 分布系统的设计，分布式系统CAP，分布式系统的模型 linux环境下的线上业务管理有没有，如何管理 redis的集合有没有限制，限制是多少 redis的1w条的插入和更新有什么区别 mysql join的底层原理是什么，有哪几种(不是左右连接这种) linux命令查询一个文件内出现重复最多的数字的 linux命令查询一个文件的行数 18.自定义注解之运行时注解(RetentionPolicy.RUNTIME)对注解概念不了解的可以先看这个：Java注解基础概念总结 前面有提到注解按生命周期来划分可分为3类： 1、RetentionPolicy.SOURCE：注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃；2、RetentionPolicy.CLASS：注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期；3、RetentionPolicy.RUNTIME：注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在； 这3个生命周期分别对应于：Java源文件(.java文件) —&gt; .class文件 —&gt; 内存中的字节码。 19.Java注解基础概念总结注解的概念 注解（Annotation），也叫元数据（Metadata），是Java5的新特性，JDK5引入了Metadata很容易的就能够调用Annotations。注解与类、接口、枚举在同一个层次，并可以应用于包、类型、构造方法、方法、成员变量、参数、本地变量的声明中，用来对这些元素进行说明注释。 注解的语法与定义形式 （1）以@interface关键字定义（2）注解包含成员，成员以无参数的方法的形式被声明。其方法名和返回值定义了该成员的名字和类型。（3）成员赋值是通过@Annotation(name=value)的形式。（4）注解需要标明注解的生命周期，注解的修饰目标等信息，这些信息是通过元注解实现。 以 java.lang.annotation 中定义的 Target 注解来说明： 12345@Retention(value = RetentionPolicy.RUNTIME)@Target(value = { ElementType.ANNOTATION_TYPE } )public @interface Target { ElementType[] value();} 源码分析如下：第一：元注解@Retention，成员value的值为RetentionPolicy.RUNTIME。第二：元注解@Target，成员value是个数组，用{}形式赋值，值为ElementType.ANNOTATION_TYPE第三：成员名称为value，类型为ElementType[]另外，需要注意一下，如果成员名称是value，在赋值过程中可以简写。如果成员类型为数组，但是只赋值一个元素，则也可以简写。如上面的简写形式为： 12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target { ElementType[] value();} 注解的分类有两种分法： 第一种分法 1、基本内置注解，是指Java自带的几个Annotation，如@Override、Deprecated、@SuppressWarnings等； 2、元注解（meta-annotation），是指负责注解其他注解的注解，JDK 1.5及以后版本定义了4个标准的元注解类型，如下： 1234@Target@Retention@Documented@Inherited 3、自定义注解，根据需要可以自定义注解，自定义注解需要用到上面的meta-annotation 第二种分法 注解需要标明注解的生命周期，这些信息是通过元注解 @Retention 实现，注解的值是 enum 类型的 RetentionPolicy，包括以下几种情况： 12345678910111213141516public enum RetentionPolicy { /** * 注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃. * 这意味着：Annotation仅存在于编译器处理期间，编译器处理完之后，该Annotation就没用了 */ SOURCE, /** * 注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期. */ CLASS, /** * 注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在， * 保存到class对象中，可以通过反射来获取 */ RUNTIME} 元注解 如上所介绍的Java定义了4个标准的元注解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// @Documented：标记注解，用于描述其它类型的注解应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Documented {}// @Inherited：标记注解，允许子类继承父类的注解@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Inherited {}// @Retention：指Annotation被保留的时间长短，标明注解的生命周期，3种RetentionPolicy取值含义上面已说明@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention { RetentionPolicy value();}// @Target：标明注解的修饰目标，共有@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target { ElementType[] value();}// ElementType取值public enum ElementType { /** 类、接口（包括注解类型）或枚举 */ TYPE, /** field属性，也包括enum常量使用的注解 */ FIELD, /** 方法 */ METHOD, /** 参数 */ PARAMETER, /** 构造函数 */ CONSTRUCTOR, /** 局部变量 */ LOCAL_VARIABLE, /** 注解上使用的元注解 */ ANNOTATION_TYPE, /** 包 */ PACKAGE} ThreadLocalThreadlocal主要用来线程变量的隔离 1234567891011121314151617181920212223242526272829303132333435class ThreadLocal { public void set(T value) {￼ Thread t = Thread.currentThread();￼ ThreadLocalMap map = getMap(t);￼ if(map ! = null)￼ map.set(this, value);￼ else￼ createMap(t, value);￼ } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; } return setInitialValue(); // 返回默认值 }}class ThreadLocalMap { WeakReference&lt;ThreadLocal&gt;; // 便于垃圾回收}struct thread { // 线程结构 ThreadLocalMap threadLocals;} 1）ThreadLocal使得每个线程对同一个变量有自己的独立副本，是实现线程安全、减少竞争的一种方案。2）ThreadLocal经常用于存储上下文信息，避免在不同代码间来回传递，简化代码。3）每个线程都有一个Map，调用ThreadLocal对象的get/set实际就是以ThreadLocal对象为键读写当前线程的该Map。 20.缓存缓存穿透缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大。 解决方案： 1.接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 2.从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 缓存击穿缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力 解决方案： 1.设置热点数据永远不过期。 2.加互斥锁，互斥锁参考代码如下： 123456789101112131415public static String getData(String key) { String result = getDataFromRedis(key); // REDIS if (result == null) { if (reenLock.tryLock()) { // 加锁 result = getDataFromMysql(key); // DB if (result != null) setDataToCache(key, result); reenLock.unlock(); // 释放锁 } else { // 未获取锁的线程 Thread.sleep(100); result = getData(key); // 防止都去数据库重复取数据 } } return result;} 缓存雪崩缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是， 缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案： 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 如果缓存数据库是分布式部署，将热点数据均匀分布在不同搞得缓存数据库中。 设置热点数据永远不过期。 REDIS1.Redis 为什么把简单的字符串设计成 SDS？SDS结构 12345struct sdshdr{ int free; // buf[]数组未使用字节的数量 int len; // buf[]数组所保存的字符串的长度 char buf[]; // 保存字符串的数组} 1.效率高 C字符串，在获取一个字符串长度时，需对整个字符串进行遍历，此时的复杂度是O(N)。 2.Redis 过期策略: 定期删除+惰性删除 走内存淘汰机制 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 Lfu 2个双向链表 3.如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？ redis 主从架构 redis 基于哨兵实现高可用 redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。 面试官问：假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 这个简单 面试官问：redis底层实现原理？这个面试题太难了。完全不会 面试官问：Redis集群原理？这个比较简单 ，我从redis主从，哨兵架构 ，插槽上分析。 面试官问：redis主从复制的工作原理能说一下么？这个我会。（看书学过） redis实现分布式锁有什么劣势，和其他分布式锁有什么优劣势 分布式锁实现方式1.基于数据库实现分布式锁；2.基于缓存（Redis等）实现分布式锁；3.基于Zookeeper实现分布式锁； Redis 作为消息队列为什么不能保证 100% 的可靠性？ MySQL4种隔离级别： 1.读未提交 Read Uncommitted - 脏读 2.读提交 Read Commited - 不可重复读 3.可重复读 Repeatable Read - 4.串行化 Serailizable 数据库中间件Mycat 1. 能说下myisam 和 innodb的区别吗？myisam引擎是5.1版本之前的默认引擎，支持全文检索、压缩、空间函数等，但是不支持事务和行级锁，所以一般用于有大量查询少量插入的场景来使用，而且myisam不支持外键，并且索引和数据是分开存储的。 innodb是基于聚簇索引建立的，和myisam相反它支持事务、外键，并且通过MVCC来支持高并发，索引和数据存储在一起。 2. 说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么？索引按照数据结构来说主要包含B+树和Hash索引。 假设我们有张表，结构如下： 123456create table user( id int(11) not null, age int(11) not null, primary key(id), key(age)); B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。 这是主键聚簇索引存储的结构，那么非聚簇索引的结构是什么样子呢？非聚簇索引(二级索引)保存的是主键id值，这一点和myisam保存的是数据地址是不同的。 最终，我们一张图看看InnoDB和Myisam聚簇和非聚簇索引的区别 3. 那你知道什么是覆盖索引和回表吗？覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。 而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。 以上面的user表来举例，我们再增加一个name字段，然后做一些查询试试。 12explain select * from user where age=1; //查询的name无法从索引数据获取explain select id,age from user where age=1; //可以直接从索引获取 4. 锁的类型有哪些呢mysql锁分为共享锁和排他锁，也叫做读锁和写锁。 读锁是共享的，可以通过lock in share mode实现，这时候只能读不能写。 写锁是排他的，它会阻塞其他的写锁和读锁。从颗粒度来区分，可以分为表锁和行锁两种。 表锁会锁定整张表并且阻塞其他用户对该表的所有读写操作，比如alter修改表结构的时候会锁表。 行锁又可以分为乐观锁和悲观锁，悲观锁可以通过for update实现，乐观锁则通过版本号实现。 5. 你能说下事务的基本特性和隔离级别吗？事务基本特性ACID分别是： 原子性指的是一个事务中的操作要么全部成功，要么全部失败。 一致性指的是数据库总是从一个一致性的状态转换到另外一个一致性的状态。比如A转账给B100块钱，假设中间sql执行过程中系统崩溃A也不会损失100块，因为事务没有提交，修改也就不会保存到数据库。 隔离性指的是一个事务的修改在最终提交前，对其他事务是不可见的。 持久性指的是一旦事务提交，所做的修改就会永久保存到数据库中。 而隔离性有4个隔离级别，分别是： read uncommit 读未提交，可能会读到其他事务未提交的数据，也叫做脏读。 用户本来应该读取到id=1的用户age应该是10，结果读取到了其他事务还没有提交的事务，结果读取结果age=20，这就是脏读。 read commit 读已提交，两次读取结果不一致，叫做不可重复读。 不可重复读解决了脏读的问题，他只会读取已经提交的事务。 用户开启事务读取id=1用户，查询到age=10，再次读取发现结果=20，在同一个事务里同一个查询读取到不同的结果叫做不可重复读。 repeatable read 可重复复读，这是mysql的默认级别，就是每次读取结果都一样，但是有可能产生幻读。 serializable 串行，一般是不会使用的，他会给每一行读取的数据加锁，会导致大量超时和锁竞争的问题。 6. 那ACID靠什么保证的呢？A原子性由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性一般由代码层面来保证 I隔离性由MVCC来保证 D持久性由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 7. 那你说说什么是幻读，什么是MVCC？要说幻读，首先要了解MVCC，MVCC叫做多版本并发控制，实际上就是保存了数据在某个时间节点的快照。 我们每行数实际上隐藏了两列，创建时间版本号，过期(删除)时间版本号，每开始一个新的事务，版本号都会自动递增。 还是拿上面的user表举例子，假设我们插入两条数据，他们实际上应该长这样。 id name create_version delete_version 1 张三 1 2 李四 2 这时候假设小明去执行查询，此时current_version=3 1select * from user where id&lt;=3; 同时，小红在这时候开启事务去修改id=1的记录，current_version=4 1update user set name='张三三' where id=1; 执行成功后的结果是这样的 id name create_version delete_version 1 张三 1 2 李四 2 1 张三三 4 如果这时候还有小黑在删除id=2的数据，current_version=5，执行后结果是这样的。 id name create_version delete_version 1 张三 1 2 李四 2 5 1 张三三 4 由于MVCC的原理是查找创建版本小于或等于当前事务版本，删除版本为空或者大于当前事务版本，小明的真实的查询应该是这样 1select * from user where id&lt;=3 and create_version&lt;=3 and (delete_version&gt;3 or delete_version is null); 所以小明最后查询到的id=1的名字还是’张三’，并且id=2的记录也能查询到。这样做是为了保证事务读取的数据是在事务开始前就已经存在的，要么是事务自己插入或者修改的。 明白MVCC原理，我们来说什么是幻读就简单多了。举一个常见的场景，用户注册时，我们先查询用户名是否存在，不存在就插入，假定用户名是唯一索引。 小明开启事务current_version=6查询名字为’王五’的记录，发现不存在。 小红开启事务current_version=7插入一条数据，结果是这样： id Name create_version delete_version 1 张三 1 2 李四 2 3 王五 7 小明执行插入名字’王五’的记录，发现唯一索引冲突，无法插入，这就是幻读。 8. 那你知道什么是间隙锁吗？间隙锁是可重复读级别下才会有的锁，结合MVCC和间隙锁可以解决幻读的问题。我们还是以user举例，假设现在user表有几条记录 id Age 1 10 2 20 3 30 当我们执行： 123456789begin;select * from user where age=20 for update;begin;insert into user(age) values(10); #成功insert into user(age) values(11); #失败insert into user(age) values(20); #失败insert into user(age) values(21); #失败insert into user(age) values(30); #失败 只有10可以插入成功，那么因为表的间隙mysql自动帮我们生成了区间(左开右闭) 1(negative infinity，10],(10,20],(20,30],(30,positive infinity) 由于20存在记录，所以(10,20]，(20,30]区间都被锁定了无法插入、删除。 如果查询21呢？就会根据21定位到(20,30)的区间(都是开区间)。 需要注意的是唯一索引是不会有间隙索引的。 9. 你们数据量级多大？分库分表怎么做的？首先分库分表分为垂直和水平两个方式，一般来说我们拆分的顺序是先垂直后水平。 垂直分库 基于现在微服务拆分来说，都是已经做到了垂直分库了 垂直分表 如果表字段比较多，将不常用的、数据较大的等等做拆分 水平分表 首先根据业务场景来决定使用什么字段作为分表字段(sharding_key)，比如我们现在日订单1000万，我们大部分的场景来源于C端，我们可以用user_id作为sharding_key，数据查询支持到最近3个月的订单，超过3个月的做归档处理，那么3个月的数据量就是9亿，可以分1024张表，那么每张表的数据大概就在100万左右。 比如用户id为100，那我们都经过hash(100)，然后对1024取模，就可以落到对应的表上了。 10. 那分表后的ID怎么保证唯一性的呢？因为我们主键默认都是自增的，那么分表之后的主键在不同表就肯定会有冲突了。有几个办法考虑： 设定步长，比如1-1024张表我们分别设定1-1024的基础步长，这样主键落到不同的表就不会冲突了。 分布式ID，自己实现一套分布式ID生成算法或者使用开源的比如雪花算法这种 分表后不使用主键作为查询依据，而是每张表单独新增一个字段作为唯一主键使用，比如订单表订单号是唯一的，不管最终落在哪张表都基于订单号作为查询依据，更新也一样。 11. 分表后非sharding_key的查询怎么处理呢？ 可以做一个mapping表，比如这时候商家要查询订单列表怎么办呢？不带user_id查询的话你总不能扫全表吧？所以我们可以做一个映射关系表，保存商家和用户的关系，查询的时候先通过商家查询到用户列表，再通过user_id去查询。 打宽表，一般而言，商户端对数据实时性要求并不是很高，比如查询订单列表，可以把订单表同步到离线（实时）数仓，再基于数仓去做成一张宽表，再基于其他如es提供查询服务。 数据量不是很大的话，比如后台的一些查询之类的，也可以通过多线程扫表，然后再聚合结果的方式来做。或者异步的形式也是可以的。 123456789101112131415161718192021List&lt;Callable&lt;List&lt;User&gt;&gt;&gt; taskList = Lists.newArrayList();for (int shardingIndex = 0; shardingIndex &lt; 1024; shardingIndex++) { taskList.add(() -&gt; (userMapper.getProcessingAccountList(shardingIndex)));}List&lt;ThirdAccountInfo&gt; list = null;try { list = taskExecutor.executeTask(taskList);} catch (Exception e) { //do something}public class TaskExecutor { public &lt;T&gt; List&lt;T&gt; executeTask(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws Exception { List&lt;T&gt; result = Lists.newArrayList(); List&lt;Future&lt;T&gt;&gt; futures = ExecutorUtil.invokeAll(tasks); for (Future&lt;T&gt; future : futures) { result.add(future.get()); } return result; }} 12. 说说mysql主从同步怎么做的吧？首先先了解mysql主从同步的原理 master提交完事务后，写入binlog slave连接到master，获取binlog master创建dump线程，推送binglog到slave slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中 slave再开启一个sql线程读取relay log事件并在slave执行，完成同步 slave记录自己的binglog 由于mysql默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已经处理，这样会产生一个问题就是假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。由此产生两个概念。 全同步复制 主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。 半同步复制 和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。 13. 那主从的延迟怎么解决呢？这个问题貌似真的是个无解的问题，只能是说自己来判断了，需要走主库的强制走主库查询。 mybatis优缺点？1.mybatis一级缓存与二级缓存的区别？2.mybatis的工作原理？3.Mapper方法可以重载吗？设计模式单例设计模式（懒汉，饿汉）123456789101112131415161718192021222324public class Hungry { // 缺点：类加载时就初始化，浪费内存 private static Hungry instance = new Hungry(); public static Hungry getInstance() { return instance; }}public class Lazy { private static void Lazy instance; public static Lazy getInstance() { if (instance == null) { synchronized(Lazy.class) { //假设没有第二层检查，那么第一个线程创建完对象释放锁后，后面进入对象也会创建对象，会产生多个对象 if (instance == null) { //volatile关键字作用为禁止指令重排， instance = new Lazy(); //singleton=new Singleton语句为非原子性，实际上会执行以下内容： //(1)在堆上开辟空间；(2)属性初始化;(3)引用指向对象 } } } return instance; }} 前后端分离在前后端彻底分离这一时期，前端的范围被扩展，controller层也被认为属于前端的一部分。在这一时期： 前端：负责View和Controller层。 后端：只负责Model层，业务/数据处理等。 可是服务端人员对前端HTML结构不熟悉，前端也不懂后台代码呀，controller层如何实现呢？这就是node.js的妙用了，node.js适合运用在高并发、I/O密集、少量业务逻辑的场景。最重要的一点是，前端不用再学一门其他的语言了，对前端来说，上手度大大提高。 可以就把Nodejs当成跟前端交互的api。总得来说，Nodejs的作用在mvc中相当于C（控制器）。Nodejs路由的实现逻辑是把前端静态页面代码当成字符串发送到客户端（例如浏览器），简单理解可以理解为路由是提供给客户端的一组api接口，只不过返回的数据是页面代码的字符串而已。 用NodeJs来作为桥梁架接服务器端API输出的JSON。后端出于性能和别的原因，提供的接口所返回的数据格式也许不太适合前端直接使用，前端所需的排序功能、筛选功能，以及到了视图层的页面展现，也许都需要对接口所提供的数据进行二次处理。这些处理虽可以放在前端来进行，但也许数据量一大便会浪费浏览器性能。因而现今，增加Node中间层便是一种良好的解决方案。 增加node.js作为中间层，具体有哪些好处呢？ 1、适配性提升: 我们其实在开发过程中，经常会给PC端、mobile、app端各自研发一套前端。其实对于这三端来说，大部分端业务逻辑是一样的。唯一区别就是交互展现逻辑不同。 如果controller层在后端手里，后端为了这些不同端页面展示逻辑，自己维护这些controller，模版无法重用，徒增和前端沟通端成本。如果增加了node.js层，此时架构图如下： 在该结构下，每种前端的界面展示逻辑由node层自己维护。如果产品经理中途想要改动界面什么的，可以由前端自己专职维护，后端无需操心。前后端各司其职，后端专注自己的业务逻辑开发，前端专注产品效果开发。 2、响应速度提升； 我们有时候，会遇到后端返回给前端的数据太简单了，前端需要对这些数据进行逻辑运算。那么在数据量比较小的时候，对其做运算分组等操作，并无影响。但是当数据量大的时候，会有明显的卡顿效果。这时候，node中间层其实可以将很多这样的代码放入node层处理、也可以替后端分担一些简单的逻辑、又可以用模板引擎自己掌握前台的输出。这样做灵活度、响应度都大大提升。 3、性能得到提升； 大家应该都知道单一职责原则。从该角度来看，我们，请求一个页面，可能要响应很多个后端接口，请求变多了，自然速度就变慢了，这种现象在mobile端更加严重。采用node作为中间层，将页面所需要的多个后端数据，直接在内网阶段就拼装好，再统一返回给前端，会得到更好的性能。 4、异步与模板统一； 淘宝首页就是被几十个HTML片段（每个片段一个文件）拼装成，之前PHP同步include这几十个片段，一定是串行的，Node可以异步，读文件可以并行，一旦这些片段中也包含业务逻辑，异步的优势就很明显了，真正做到哪个文件先渲染完就先输出显示。 前端机的文件系统越复杂，页面的组成片段越多，这种异步的提速效果就越明显。前后端模板统一在无线领域很有用，PC页面和WIFI场景下的页面适合前端渲染（后端数据Ajax到前端），2G、3G弱网络环境适合后端渲染（数据随页面吐给前端），所以同样的模板，在不同的条件下走不同的渲染渠道，模板只需一次开发。 增加NodeJS中间层后的前后端职责划分：","link":"/2022/03/13/12000%E9%9D%A2%E8%AF%95/JAVA/"},{"title":"","text":"2.9.1 房产防爬加密策略设计与实践","link":"/2021/05/19/14000JAVA/1.2.1/"},{"title":"","text":"并发编程-线程池1.为什么会出现线程池？创建和销毁线程都需要时间，如果频繁地进行这两个步骤就会降低系统的效率。 2.ThreadPoolExecutor类Java提供了java.util.concurrent.ThreadPoolExecutor类来帮助管理线程池。看下具体的源代码。 123456789public class ThreadPoolExecutor extends AbstractExecutorService { public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); // 上面的3个构造函数都是调用这第4个构造函数 public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler);} 总共有7个参数: corePoolSize – 最少的工作线程数量，即使它们处于空闲状态，除非设置了allowCoreThreadTimeOut。 the number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set maximumPoolSize – 最多工作的线程数 the maximum number of threads to allow in the pool keepAliveTime – 当工作线程多于最少的数量，能存活的时间。when the number of threads is greater than the core, this is the maximum time that excess idle threads will wait for new tasks before terminating. unit – 时间的单位 the time unit for the keepAliveTime argument workQueue – 保存任务的队列，由execute方法提交。the queue to use for holding tasks before they are executed. This queue will hold only the Runnable tasks submitted by the execute method. threadFactory – 创建新线程时要使用的工厂 the factory to use when the executor creates a new thread handler – 超过队列时如何处理任务 the handler to use when execution is blocked because the thread bounds and queue capacities are reached 3.看看workQueue参数阻塞队列，用来存放可执行的任务，会对线程池的运行产生重大影响，一共有以下几种选择 123456// 首先这是一个接口public interface BlockingQueue&lt;E&gt; extends Queue&lt;E&gt;; // 看看实现public class ArrayBlockingQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;;public class LinkedBlockingQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;;public class SynchronousQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;; 4.看看handler参数1234ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 5.如何合理配置线程池的大小一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 N(CPU)+1 如果是IO密集型任务，参考值可以设置为2*N(CPU) 当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。","link":"/2021/01/22/14000JAVA/1.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"JAVA四种缓存","text":"JAVA四种缓存1.堆缓存（软应用、弱引用）- 优势：速度最快（没有序列化和反序列化） - 劣势：占堆内存，影响STW时间 Guava Cache实现 123456Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder() .concurrencyLevel(4) // 重写了ConcurrentHashMap，concurrencyLevel用来设置Segment数量 .expireAfterWrite(10, TimeUnit.SECONDS) // TTL时间 .maximumSize(10000) // 容量，LRU回收 .build();// 惰性删除，PUT时会主动进行一次缓存清理 Ehcache实现 12345678CacheManager cacheManager = CacheManagerBuilder.newCacheManagerBuilder().build(true); CacheConfigurationBuilder&lt;String, String&gt; cacheConfig = CacheConfigurationBuilder.newCacheConfigurationBuilder(String.class, String.class,ResourcePoolsBuilder.newResourcePoolsBuilder().heap(100,EntryUnit.ENTRIES)).withDispatcherConcurrency(4) // 并发.withExpiry(Expirations.timeToLiveExpiration(Duration.of(10,TimeUnit.SECONDS)));// TTL Cache&lt;String,String&gt; cache = cacheManager.createCache(&quot;cache&quot;,concheConfig); 2.堆外缓存 优势：更大的缓存空间，减少GC时间 劣势：序列化/反序列化，慢很多;不支持基于容量的缓存过期策略 123456CacheConfigurationBuilder&lt;String, String&gt; cacheConfig = CacheConfigurationBuilder.newCacheConfigurationBuilder(String.class,String.class,ResourcePoolsBuilder.newResourcePoolsBuilder().offheap(100,MemoryUnit.MB)).withDispatcherConcurrency(4).withExpiry(Expirations.timeToLiveExpiration(Duration.of(10,TimeUnit.SECONDS))).withSizeOfMaxObjectGraph(3）.withSizeOfMaxObjectSize(1,MemoryUnit.KB); 3.磁盘缓存- 优势：JVM重启后数据还在 - 劣势：满 123456789101112131415161718CacheManager cacheManager = CacheManagerBuilder.newCacheManagerBuilder().using(PoolExecutionServiceConfigurationBuilder .newPooledExecutionServiceCoonfigurationBuilder() .defaultPool(&quot;default&quot;,1,10) .build()).with(new CacheManagerPersistenceConfiguration(new File(&quot;home\\back&quot;))).build(true); CacheConfigurationBuilder&lt;String,String&gt; cacheConfig = CacheConfigurationBuilder.newCacheConfigurationBuilder( String.class, String.calss, ResourcePoolsBuilder.newResourcePoolsBuilder() .disk(100,MemoryUnit.MB,true)) .withDiskStoreThreadPool(&quot;default&quot;,5) .withExpiry(Expirations.timeToLiveExpiration(Duration.of(30,TimeUnit.SECONDS))).withSizeOfMaxObjectGraph(3）.withSizeOfMaxObjectSize(1,MemoryUnit.KB); 4.分布式缓存- 优势：1.没有单机容量问题；2.不会多机器数据一致性问题","link":"/2022/05/14/14000JAVA/14002JAVA%E5%9B%9B%E7%A7%8D%E7%BC%93%E5%AD%98/"},{"title":"LongAdder","text":"LongAdder需求：多线程累加，但是不能加锁 1234567891011121314151617181920212223242526272829303132333435363738394041# unsafe.javapublic static void main(String[] args) { testAtomicLong(10, 100000); testAtomicLong(10, 100000);}static void testLongAdder(final int threadCount, final int times) { LongAdder longAdder = New LongAdder(); List&lt;Thread&gt; list = new ArrayList(); for (int i = 0; i &lt; threadCount; i++) { list.add(new Thread(() -&gt; { for (int j = 0; j &lt; times; j++) { longAdder.add(1); } })); } for (Thread thread : list) { thread.start(); } for (Thread thread : list) { thread.join(); } }static void testAtomicLong(final int threadCount, final int times) { AtomicLong atomicLong = new AtomicLong(); List&lt;Thread&gt; list = new ArrayList&lt;&gt;(); for (int i=0; i&lt; threadCount; i++) { list.add(new Thread(() -&gt; { for (int j=0; j&lt;times; j++) { atomicLong.incrementAddGet(); } })); } for (Thread thread : list) { thread.start(); } for (Thread thread : list) { thread.join(); } } LongAdder源码类 123456789class LongAdder extends Stripped64 { public void add(long x) { Cell[] as; long b, v; int m; Cell a; if ((as = cells) != null || !casBase(b = base, b + x)) { boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0) } }}","link":"/2022/05/15/14000JAVA/14003LongAdder/"},{"title":"","text":"1.JVM中的对象分配过程 1.JVM中的对象分配过程1.1检查加载先执行相应的类加载过程。如果没有，则进行类加载 1.2分配内存根据方法区的信息确定为该类分配的内存空间大小 指针碰撞 (java堆内存空间规整的情况下使用):接下来虚拟机将为新生对象分配内存。为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。如果Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”。 空闲列表(java堆空间不规整的情况下使用)：如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”。 1.2.1并发安全除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。 并发安全的解决方案有：CAS机制与分配缓存。 CAS机制： 解决这个问题有两种方案，一种是对分配内存空间的动作进行同步处理——实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性；分配缓冲：另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块私有内存，也就是本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），如果设置了虚拟机参数 -XX:+UseTLAB，在线程初始化时，同时也会申请一块指定大小的内存，只给当前线程使用，这样每个线程都单独拥有一个Buffer，如果需要分配内存，就在自己的Buffer上分配，这样就不存在竞争的情况，可以大大提升分配效率，当Buffer容量不够的时候，再重新从Eden区域申请一块继续使用。 TLAB的目的是在为新对象分配内存空间时，让每个Java应用线程能在使用自己专属的分配指针来分配空间（Eden区，默认Eden的1%），减少同步开销。 TLAB只是让每个线程有私有的分配指针，但底下存对象的内存空间还是给所有线程访问的，只是其它线程无法在这个区域分配而已。当一个TLAB用满（分配指针top撞上分配极限end了），就新申请一个TLAB。 1.3内存空间初始化（注意不是构造方法）内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值(如int值为0，boolean值为false等等)。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 1.4设置接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头之中。 1.5对象初始化在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚刚开始，所有的字段都还为零值。所以，一般来说，执行new指令之后会接着把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。","link":"/2021/05/24/14000JAVA/2.1%20JVM/"},{"title":"","text":"在并发场景中，如何内存分配过程的线程安全性？如果两个线程先后把对象引用指向了同一个内存区域，怎么办。线程TLAB局部缓存区域Thread Local Allocation Buffer1.堆是JVM中所有线程共享的，因此在其上进行对象内存的分配均需要进行加锁，这也导致了new对象的开销是比较大的2.Sun Hotspot JVM为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间TLAB（Thread Local Allocation Buffer），其大小由JVM根据运行的情况计算而得，在TLAB上分配对象时不需要加锁，因此JVM在给线程的对象分配内存时会尽量的在TLAB上分配，在这种情况下JVM中分配对象内存的性能和C基本是一样高效的，但如果对象过大的话则仍然是直接使用堆空间分配3.TLAB仅作用于新生代的Eden Space，因此在编写Java程序时，通常多个小的对象比大的对象分配起来更加高效。4.所有新创建的Object 都将会存储在新生代Yong Generation中。如果Young Generation的数据在一次或多次GC后存活下来，那么将被转移到OldGeneration。新的Object总是创建在Eden Space。 PS：虽然总体来说堆是线程共享的，但是在堆的年轻代中的Eden区可以分配给专属于线程的局部缓存区TLAB，也可以用来存放对象。相当于线程私有的对象。","link":"/2021/05/17/14000JAVA/2.1.1%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"title":"面试官：如何从10亿数据中快速判断是否存在某一个元素？","text":"2.8.1 面试官：如何从10亿数据中快速判断是否存在某一个元素？1 缓存雪崩缓存雪崩指的是 Redis 当中的大量缓存在同一时间全部失效，而假如恰巧这一段时间同时又有大量请求被发起，那么就会造成请求直接访问到数据库，可能会把数据库冲垮。 缓存雪崩一般形容的是缓存中没有而数据库中有的数据，而因为时间到期导致请求直达数据库。 1.1 解决方案解决缓存雪崩的方法有很多，常用的有以下几种： 加锁，保证单线程访问缓存。这样就不会有很多请求同时访问到数据库。 key 值的失效时间不要设置成一样。典型的就是初始化预热数据的时候，将数据存入缓存时可以采用随机时间来确保不会在同一时间有大量缓存失效。 内存允许的情况下，可以将缓存设置为永不失效。 2 缓存击穿缓存击穿和缓存雪崩很类似，区别就是缓存击穿一般指的是单个缓存失效，而同一时间又有很大的并发请求需要访问这个 key，从而造成了数据库的压力。 2.1 解决方案解决缓存击穿的方法和解决缓存雪崩的方法很类似： 加锁，保证单线程访问缓存。这样第一个请求到达数据库后就会重新写入缓存，后续的请求就可以直接读取缓存。 内存允许的情况下，可以将缓存设置为永不失效。 3 缓存穿透缓存穿透和上面两种现象的本质区别就是这时候访问的数据不但在 Redis 中不存在，而且在数据库中也不存在，这样如果并发过大就会造成数据源源不断的到达数据库，给数据库造成极大压力。 3.1 解决方案对于缓存穿透问题，加锁并不能起到很好的效果，因为本身 key 就是不存在，所以即使控制了线程的访问数，但是请求还是会源源不断的到达数据库。 解决缓存穿透问题一般可以采用以下方案配合使用： 接口层进行校验，发现非法的 key 直接返回。比如数据库中采用的是自增 id，那么如果来了一个非整型的 id 或者负数 id 可以直接返回，或者说如果采用的是 32 位 uuid，那么发现 id 长度不等于 32 位也可以直接返回。 将不存在的数据也进行缓存，可以直接缓存一个空或者其他约定好的无效 value。采用这种方案最好将 key 设置一个短期失效时间，否则大量不存在的 key 被存储到 Redis 中，也会占用大量内存。 4 布隆过滤器（Bloom Filter）针对上面缓存穿透的解决方案，我们思考一下：假如一个 key 可以绕过第 1 种方法的校验，而此时有大量的不存在 key 被访问（如 1 亿个或者 10 亿个），那么这时候全部存储到内存中，是不太现实的。 那么有没有一种更好的解决方案呢？这就是我们接下来要介绍的布隆过滤器，布隆过滤器就可以用尽可能小的空间存储尽可能多的数据。 5.1 什么是布隆过滤器？布隆过滤器（Bloom Filter）是由布隆在 1970 年提出的。它实际上是一个很长的二进制向量（位图）和一系列随机映射函数（哈希函数）。 布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好得多，缺点是有一定的误识别率而且删除困难。 5.2 位图（Bitmap）Redis 当中有一种数据结构就是位图，布隆过滤器其中重要的实现就是位图的实现，也就是位数组，并且在这个数组中每一个位置只有 0 和 1 两种状态，每个位置只占用 1 个字节，其中 0 表示没有元素存在，1 表示有元素存在。如下图所示就是一个简单的布隆过滤器示例（一个 key 值经过哈希运算和位运算就可以得出应该落在哪个位置）： 5.3 哈希碰撞上面我们发现，lonely和wolf落在了同一个位置，这种不同的key值经过哈希运算后得到相同值的现象就称之为哈希碰撞。发生哈希碰撞之后再经过位运算，那么最后肯定会落在同一个位置。 如果发生过多的哈希碰撞，就会影响到判断的准确性，所以为了减少哈希碰撞，我们一般会综合考虑以下 2 个因素： 增大位图数组的大小（位图数组越大，占用的内存越大）。 增加哈希函数的次数（同一个 key 值经过 1 个函数相等了，那么经过 2 个或者更多个哈希函数的计算，都得到相等结果的概率就自然会降低了）。 上面两个方法我们需要综合考虑：比如增大位数组，那么就需要消耗更多的空间，而经过越多的哈希计算也会消耗 cpu 影响到最终的计算时间，所以位数组到底多大，哈希函数次数又到底需要计算多少次合适需要具体情况具体分析。 5.4 布隆过滤器的 2 大特点下图这个就是一个经过了 2 次哈希函数得到的布隆过滤器，根据下图我们很容易看到，假如我们的 Redis 根本不存在，但是 Redis 经过 2 次哈希函数之后得到的两个位置已经是 1 了（一个是 wolf 通过 f2 得到，一个是 Nosql 通过 f1 得到，这就是发生了哈希碰撞，也是布隆过滤器可能存在误判的原因）。 而从元素的角度也可以得出 2 大特点： 如果元素实际存在，那么布隆过滤器一定会判断存在。 如果元素不存在，那么布隆过滤器可能会判断存在。 PS：需要注意的是，如果经过 N 次哈希函数，则需要得到的 N 个位置都是 1 才能判定存在，只要有一个是 0，就可以判定为元素不存在布隆过滤器中。 （1）fpp 因为布隆过滤器中总是会存在误判率，因为哈希碰撞是不可能百分百避免的。布隆过滤器对这种误判率称之为假阳性概率，即：False Positive Probability，简称为 fpp。 在实践中使用布隆过滤器时可以自己定义一个 fpp，然后就可以根据布隆过滤器的理论计算出需要多少个哈希函数和多大的位数组空间。需要注意的是这个 fpp 不能定义为 100%，因为无法百分保证不发生哈希碰撞。 5.5 布隆过滤器的实现（Guava）在 Guava 的包中提供了布隆过滤器的实现，下面就通过 Guava 来体会一下布隆过滤器的应用： 1.pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;29.0-jre&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.filter;import com.google.common.base.Charsets;import com.google.common.hash.BloomFilter;import com.google.common.hash.Funnels;import java.text.NumberFormat;import java.util.ArrayList;import java.util.List;import java.util.UUID;public class GuavaBloomFilter { private static final int expectedInsertions = 1000000; public static void main(String[] args) { BloomFilter&lt;String&gt; bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8), expectedInsertions); List&lt;String&gt; list = new ArrayList&lt;&gt;(expectedInsertions); for (int i = 0; i &lt; expectedInsertions; i++) { String uuid = UUID.randomUUID().toString(); bloomFilter.put(uuid); list.add(uuid); } int mightContainNum1 = 0; NumberFormat percentFormat = NumberFormat.getPercentInstance(); percentFormat.setMaximumFractionDigits(2); //最大小数位数 for (int i = 0; i &lt; 500; i++) { String key = list.get(i); if (bloomFilter.mightContain(key)) { mightContainNum1++; } } System.out.println(&quot;【key真实存在的情况】布隆过滤器认为存在的key值数：&quot; + mightContainNum1); System.out.println(&quot;-----------------------分割线---------------------------------&quot;); int mightContainNum2 = 0; for (int i = 0; i &lt; expectedInsertions; i++) { String key = UUID.randomUUID().toString(); if (bloomFilter.mightContain(key)) { mightContainNum2++; } } System.out.println(&quot;【key不存在的情况】布隆过滤器认为存在的key值数：&quot; + mightContainNum2); System.out.println(&quot;【key不存在的情况】布隆过滤器的误判率为：&quot; + percentFormat.format((float) mightContainNum2 / expectedInsertions)); }}【key真实存在的情况】布隆过滤器认为存在的key值数：500-----------------------分割线---------------------------------【key不存在的情况】布隆过滤器认为存在的key值数：29738【key不存在的情况】布隆过滤器的误判率为：2.97% 第一部分输出的 mightContainNum1一定是和 for 循环内的值相等，也就是百分百匹配。即满足了原则1：如果元素实际存在，那么布隆过滤器一定会判断存在。第二部分的输出的误判率即 fpp 总是在 3% 左右，而且随着 for 循环的次数越大，越接近 3%。即满足了原则2：如果元素不存在，那么布隆过滤器可能会判断存在。 这个 3% 的误判率是如何来的呢？我们进入创建布隆过滤器的 create 方法，发现默认的fpp就是 0.03： 123public static &lt;T&gt; BloomFilter&lt;T&gt; create(Funnel&lt;? super T&gt; funnel, long expectedInsertions) { return create(funnel, expectedInsertions, 0.03D);} 对于这个默认的 3% 的 fpp 需要多大的位数组空间和多少次哈希函数得到的呢？在 BloomFilter 类下面有两个 default 方法可以获取到位数组空间大小和哈希函数的个数： optimalNumOfHashFunctions：获取哈希函数的次数 optimalNumOfBits：获取位数组大小 debug 进去看一下： 得到的结果是 7298440 bit=0.87M，然后经过了 5 次哈希运算。可以发现这个空间占用是非常小的，100W 的 key 才占用了 0.87M。 5.6 布隆过滤器的如何删除布隆过滤器判断一个元素存在就是判断对应位置是否为 1 来确定的，但是如果要删除掉一个元素是不能直接把 1 改成 0 的，因为这个位置可能存在其他元素，所以如果要支持删除，那我们应该怎么做呢？最简单的做法就是加一个计数器，就是说位数组的每个位如果不存在就是 0，存在几个元素就存具体的数字，而不仅仅只是存 1，那么这就有一个问题，本来存 1 就是一位就可以满足了，但是如果要存具体的数字比如说 2，那就需要 2 位了，所以带有计数器的布隆过滤器会占用更大的空间。 带有计数器的布隆过滤器 下面就是一个带有计数器的布隆过滤器示例： 12345&lt;dependency&gt; &lt;groupId&gt;com.baqend&lt;/groupId&gt; &lt;artifactId&gt;bloom-filter&lt;/artifactId&gt; &lt;version&gt;1.0.7&lt;/version&gt;&lt;/dependency&gt; 12345678910111213public class CountingBloomFilter { public static void main(String[] args) { orestes.bloomfilter.CountingBloomFilter&lt;String&gt; cbf = new FilterBuilder(10000, 0.01).countingBits(8).buildCountingBloomFilter(); cbf.add(&quot;zhangsan&quot;); cbf.add(&quot;lisi&quot;); cbf.add(&quot;wangwu&quot;); System.out.println(&quot;是否存在王五：&quot; + cbf.contains(&quot;wangwu&quot;)); //true cbf.remove(&quot;wangwu&quot;); System.out.println(&quot;是否存在王五：&quot; + cbf.contains(&quot;wangwu&quot;)); //false }} 构建布隆过滤器前面 2 个参数一个就是期望的元素数，一个就是 fpp 值，后面的 countingBits 参数就是计数器占用的大小，这里传了一个 8 位，即最多允许 255 次重复，如果不传的话这里默认是 16 位大小，即允许 65535次重复。 总结本文主要讲述了使用 Redis 存在的三种问题：缓存雪崩，缓存击穿和缓存穿透。并分别对每种问题的解决方案进行了描述，最后着重介绍了缓存穿透的解决方案：布隆过滤器。原生的布隆过滤器不支持删除，但是可以引入一个计数器实现带有计数器的布隆过滤器来实现删除功能，同时在最后也提到了，带有计数器的布隆过滤器会占用更多的空间问题。","link":"/2021/05/18/14000JAVA/14001%20%E9%9D%A2%E8%AF%95%E5%AE%98%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BB%8E10%E4%BA%BF%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%AB%E9%80%9F%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%9F%90%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%EF%BC%9F/"},{"title":"","text":"2.1.2 线程安全什么是线程安全解决线程安全 方法一：使用synchronized关键字 方法二：使用Lock接口下的实现类 方法三：使用线程本地存储ThreadLocal 方法四：使用乐观锁机制 什么是线程安全什么是线程安全呢？什么样的情况会造成线程安全问题呢？怎么解决线程安全呢？这些问题都是在下文中所要讲述的。 线程安全：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象就是线程安全的。 那什么时候会造成线程安全问题呢？当多个线程同时去访问一个对象时，就可能会出现线程安全问题。那么怎么解决呢？请往下看！ 解决线程安全在这里提供4种方法来解决线程安全问题，也是最常用的4种方法。前提是项目在一个服务器中，如果是分布式项目可能就会用到分布锁了，这个就放到后面文章来详谈了。 讲4种方法前，还是先来了解一下悲观锁和乐观锁吧！ 悲观锁，顾名思义它是悲观的。讲得通俗点就是，认为自己在使用数据的时候，一定有别的线程来修改数据，因此在获取数据的时候先加锁，确保数据不会被线程修改。形象理解就是总觉得有刁民想害朕。 而乐观锁就比较乐观了，认为在使用数据时，不会有别的线程来修改数据，就不会加锁，只是在更新数据的时候去判断之前有没有别的线程来更新了数据。具体用法在下面讲解。 现在来看有那4种方法吧！ 方法一：使用synchronized关键字一个表现为原生语法层面的互斥锁，它是一种悲观锁，使用它的时候我们一般需要一个监听对象 并且监听对象必须是唯一的，通常就是当前类的字节码对象。它是JVM级别的，不会造成死锁的情况。使用synchronized可以拿来修饰类，静态方法，普通方法和代码块。比如：Hashtable类就是使用synchronized来修饰方法的。put方法部分源码： 1234public synchronized V put(K key, V value) { if (value == null) { // Make sure the value is not null throw new NullPointerException(); } 而ConcurrentHashMap类中就是使用synchronized来锁代码块的。 synchronized关键字底层实现主要是通过monitorenter 与monitorexit计数 ，如果计数器不为0，说明资源被占用，其他线程就不能访问了，但是可重入的除外。说到这，就来讲讲什么是可重入的。这里其实就是指的可重入锁：指的是同一线程外层函数获得锁之后，内层递归函数仍然有获取该锁的代码，但不受影响，执行对象中所有同步方法不用再次获得锁。避免了频繁的持有释放操作，这样既提升了效率，又避免了死锁。 其实在使用synchronized时，存在一个锁升级原理。它是指在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。锁升级的目的是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。可能你又会问什么是偏向锁？什么是轻量级锁？什么是重量级锁？这里就简单描述一下吧，能够帮你更好的理解synchronized。 偏向锁（无锁）：大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁的目的是在某个线程获得锁之后（线程的id会记录在对象的Mark Word中），消除这个线程锁重入（CAS）的开销，看起来让这个线程得到了偏护。 轻量级锁（CAS）：就是由偏向锁升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁；轻量级锁的意图是在没有多线程竞争的情况下，通过CAS操作尝试将MarkWord更新为指向LockRecord的指针，减少了使用重量级锁的系统互斥量产生的性能消耗。 重量级锁：虚拟机使用CAS操作尝试将MarkWord更新为指向LockRecord的指针，如果更新成功表示线程就拥有该对象的锁；如果失败，会检查MarkWord是否指向当前线程的栈帧，如果是，表示当前线程已经拥有这个锁；如果不是，说明这个锁被其他线程抢占，此时膨胀为重量级锁。 方法二：使用Lock接口下的实现类Lock是juc（java.util.concurrent）包下面的一个接口。常用的实现类就是ReentrantLock 类，它其实也是一种悲观锁。一种表现为 API 层面的互斥锁。通过lock() 和 unlock() 方法配合使用。因此也可以说是一种手动锁，使用比较灵活。但是使用这个锁时一定要注意要释放锁，不然就会造成死锁。一般配合try/finally 语句块来完成。比如： 12345678910111213141516171819public class TicketThreadSafe extends Thread{ private static int num = 5000; ReentrantLock lock = new ReentrantLock(); @Override public void run() { while(num&gt;0){ try { lock.lock(); if(num&gt;0){ System.out.println(Thread.currentThread().getName()+&quot;你的票号是&quot;+num--); } } catch (Exception e) { e.printStackTrace(); }finally { lock.unlock(); } } }} 相比 synchronized，ReentrantLock 增加了一些高级功能，主要有以下 3 项：等待可中断、可实现公平锁，以及锁可以绑定多个条件。 等待可中断是指：当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助。 公平锁是指：多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁则不保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。 12public ReentrantLock(boolean fair) sync = fair ? new FairSync() : new NonfairSync(); 锁绑定多个条件是指：一个 ReentrantLock 对象可以同时绑定多个 Condition 对象，而在 synchronized 中，锁对象的 wait() 和 notify() 或 notifyAll() 方法可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外地添加一个锁，而 ReentrantLock 则无须这样做，只需要多次调用 newCondition() 方法即可。 123final ConditionObject newCondition() { //ConditionObject是Condition的实现类 return new ConditionObject();} 方法三：使用线程本地存储ThreadLocal当多个线程操作同一个变量且互不干扰的场景下，可以使用ThreadLocal来解决。它会在每个线程中对该变量创建一个副本，即每个线程内部都会有一个该变量，且在线程内部任何地方都可以使用，线程之间互不影响，这样一来就不存在线程安全问题，也不会严重影响程序执行性能。在很多情况下，ThreadLocal比直接使用synchronized同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。通过set(T value)方法给线程的局部变量设置值；get()获取线程局部变量中的值。当给线程绑定一个 Object 内容后，只要线程不变,就可以随时取出；改变线程,就无法取出内容.。这里提供一个用法示例： 12345678910111213141516171819202122232425262728public class ThreadLocalTest { private static int a = 500; public static void main(String[] args) { new Thread(()-&gt;{ ThreadLocal&lt;Integer&gt; local = new ThreadLocal&lt;Integer&gt;(); while(true){ local.set(++a); //子线程对a的操作不会影响主线程中的a try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;子线程：&quot;+local.get()); } }).start(); a = 22; ThreadLocal&lt;Integer&gt; local = new ThreadLocal&lt;Integer&gt;(); local.set(a); while(true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;主线程：&quot;+local.get()); } }} ThreadLocal线程容器保存变量时，底层其实是通过ThreadLocalMap来实现的。它是以当前ThreadLocal变量为key ，要存的变量为value。获取的时候就是以当前ThreadLocal变量去找到对应的key，然后获取到对应的值。源码参考如下： 1234567891011121314public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);}ThreadLocalMap getMap(Thread t) { return t.threadLocals; //ThreadLocal.ThreadLocalMap threadLocals = null;Thread类中声明的}void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue);} 观察源码就会发现，其实每个线程Thread内部有一个ThreadLocal.ThreadLocalMap类型的成员变量threadLocals，这个threadLocals就是用来存储实际的变量副本的，键值为当前ThreadLocal变量，value为变量副本（即T类型的变量）。 初始时，在Thread里面，threadLocals为空，当通过ThreadLocal变量调用get()方法或者set()方法，就会对Thread类中的threadLocals进行初始化，并且以当前ThreadLocal变量为键值，以ThreadLocal要保存的副本变量为value，存到threadLocals。 然后在当前线程里面，如果要使用副本变量，就可以通过get方法在threadLocals里面查找即可。 方法四：使用乐观锁机制前面已经讲述了什么是乐观锁。这里就来描述哈在java开发中怎么使用的。 其实在表设计的时候，我们通常就需要往表里加一个version字段。每次查询时，查出带有version的数据记录，更新数据时，判断数据库里对应id的记录的version是否和查出的version相同。若相同，则更新数据并把版本号+1；若不同，则说明，该数据发生了并发，被别的线程使用了，进行递归操作，再次执行递归方法，直到成功更新数据为止。","link":"/2022/02/21/14000JAVA/2.1.2%20%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"},{"title":"","text":"2.4.1 ApplicationContextSpringBoot中获取ApplicationContext的三种方式 ApplicationContext是什么？ 简单来说就是Spring中的容器，可以用来获取容器中的各种bean组件，注册监听事件，加载资源文件等功能。 一、Application Context获取的几种方式1 直接使用Autowired注入12345678@Componentpublic class Book1 { @Autowired private ApplicationContext applicationContext; public void show() { System.out.println(applicationContext.getClass()); }} 2 利用 spring4.3 的新特性使用spring4.3新特性但是存在一定的局限性，必须满足以下两点：1 构造函数只能有一个，如果有多个，就必须有一个无参数的构造函数，此时，spring会调用无参的构造函数2 构造函数的参数，必须在spring容器中存在 1234567891011@Componentpublic class Book2 { private ApplicationContext applicationContext; public Book2(ApplicationContext applicationContext){ System.out.println(applicationContext.getClass()); this.applicationContext=applicationContext; } public void show (){ System.out.println(applicationContext.getClass()); }} 3 实现spring提供的接口 ApplicationContextAwarespring 在bean 初始化后会判断是不是ApplicationContextAware的子类，调用setApplicationContext()方法， 会将容器中ApplicationContext传入进去 1234567891011@Componentpublic class Book3 implements ApplicationContextAware { private ApplicationContext applicationContext; public void show (){ System.out.println(applicationContext.getClass()); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; }} 二、ApplicationContext的常见实现是什么？1.FileSystemXmlApplicationContext来容器从XML文件加载bean的定义。必须将XML bean配置文件的完整路径提供给构造函数。 2.ClassPathXmlApplicationContext的容器还加载从XML文件java bean的定义。在这里，您需要正确设置CLASSPATH，因为此容器将在CLASSPATH中查找bean配置XML文件。 3.WebXmlApplicationContext：容器从Web应用程序中加载的所有bean类定义的XML文件。","link":"/2021/05/13/14000JAVA/2.4.1ApplicationContext/"},{"title":"","text":"2.4 SpringBoot的启动原理1.获取并启动监听器2.环境构建3.创建容器4.前置处理5.刷新容器6.后置处理7.发出事件8.执行Runner9.返回容器 1.获取并启动监听器2.环境构建3.创建容器4.前置处理5.刷新容器Spring Framework 6.后置处理7.发出事件8.执行Runner9.返回容器","link":"/2021/05/15/14000JAVA/2.4%20SpringBoot%E7%9A%84%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86/"},{"title":"","text":"2.4.2 Spring Bean依赖注入一、什么是Spring中的依赖注入？依赖注入是控制反转（IoC）的一个方面，它是一个通用概念，它可以用许多不同的方式表达。这个概念说你不创建你的对象，而是描述它们应该如何创建。您不能在代码中直接连接组件和服务，而是描述配置文件中哪些组件需要哪些服务。然后，一个容器（IOC容器）负责将其全部挂起。 二、有哪些不同类型的IoC（依赖注入）？1.构造函数的依赖注入：当容器调用具有许多参数的类构造函数时，完成基于构造函数的DI，每个参数表示对其他类的依赖。2.Setter的依赖注入：基于Setter的DI是在调用无参数构造函数或无参数静态工厂方法来实例化bean之后，通过容器调用bean上的setter方法来完成的。 三、Spring bean是什么？Spring Beans是构成Spring应用程序主干的Java对象。它们由Spring IoC容器实例化，组装和管理。这些bean是使用提供给容器的配置元数据创建的（以XML定义、注解）。 1234567891011&lt;bean id=&quot;testController&quot; class=&quot;com.**.TestController&quot; scope=&quot;prototype&quot; init-method=&quot;init&quot; destroy-method=&quot;destory&quot;/&gt;@Controller@RequestMapping(&quot;/&quot;)@Scope(&quot;prototype&quot;) // 创建多实例public class TestController { @PostConstruct public void init() {} @PreDestory public void destory() {}} scope属性：1.singleton： 表示在spring容器中的单例，通过spring容器获得该bean时总是返回唯一的实例；2.prototype：与单例模式相反，表示为每一个bean请求都会提供一个实例；3.request：在请求范围内会为每一个来自客户端的网络请求提供一个实例，在请求完成后，bean会失效并被垃圾回收器回收；4.session：与请求范围类似，表示确保每个session中有个Bean的实例，在session过期后随之消失；5.global-session：表示在全局会话内有效。 四、有三种方式向Spring 容器提供元数据:1.XML配置文件2.基于注解配置3.基于Java的配置 基于Java类定义Bean配置元数据，其实就是通过Java类定义Spring配置元数据，且直接消除XML配置文件。首先让我们看一下基于Java类如何定义Bean配置元数据，具体步骤如下： 1.使用@Configuration注解需要作为配置的类，表示该类将定义Bean的元数据 2.使用@Bean注解相应的方法，该方法名默认就是Bean的名称，该方法返回值就是Bean的对象。 3.AnnotationConfigApplicationContext或子类进行加载基于java类的配置注意：使用bean注解的方法不能是private、final、static的。 基于Java方式的配置方式不是为了完全替代基于XML方式的配置，两者可以结合使用，因此可以有两种结合使用方式： 1.在基于Java方式的配置类中引入基于XML方式的配置文件 12345678&lt;bean id=&quot;message&quot; class=&quot;java.lang.String&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;test&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; @Configuration(&quot;ctxConfig&quot;)@ImportResource(&quot;classpath:com/jike/***/appCtx.xml&quot;)public class ApplicationContextConfig {} 可以看到在java程序中使用**@** ImportResource导入了XML的配置文件 ​ 2.在基于XML方式的配置文件中中引入基于Java方式的配置 123456789&lt;context:annotation-config/&gt;&lt;bean id=&quot;ctxConfig&quot; class=“com.jike.***..ApplicationContextConfig&quot;/&gt; //测试类public void testXmlConfig() { String configLocations[] = {&quot; classpath:com/jike/***/appCtx.xml&quot;}; ApplicationContext ctx = new ClassPathXmlApplicationContext(configLocations);} 可以看到在XML的配置文件当中将java的配置类当中Bean来声明，第一行的是开启注解驱动支持。值得注意的是必须得配置 context:annotation-config/ 在XML配置文件中。 Spring提供了一个AnnotationConfigApplicanContext类，能够直接通过标注@Configuration的Java类启动Spring容器：通过构造函数加载配置类: 1ApplicationContext ctx = new AnnotationConfigApplicationContext(AppConf.class); 通过编码方式注册配置类： 1234AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.register(DaoConfig.class);ctx.register(ServiceConfig.class);ctx.refresh(); 可以看到ctx注册了多个configuration类，然后通过refresh类来刷新容器以应用这些配置文件。可以通过代码一个个的引入配置类，当然也可以使用**@Import**注解来引入配置类 引入多个配置类： 123@Configuration@Import(DaoConfig.class)public class ServiceConfig {} 总结：不同配置方式比较 Bean XML配置文件 注解配置 Java的配置 Bean定义 @Component 标注了@Configuration的java类中，类方法上标注了@Bean Bean名称 id或name=”userDao” @Component(“userDao”) @Bean(“userDao”) Bean注入 通过子元素或通过p命名空间的动态属性注入 通过标出@Autowired，按类型匹配自动\u0010,配合@Qualifier按名称匹配 1.@Autowired方法入参绑定Bean 2.配置类的@Bean方法注入 生命过程方法 通过的init-method和destory-method属性指定Bean实现类方法名 @PostContruct和@PreDestroy @Bean(initMethod = “”, destroyMethod = “”) 作用范围 的scope属性 @Scope @Scope 延迟初始化 Lazy-init,默认为default @Lazy @Lazy","link":"/2021/05/14/14000JAVA/2.4.2%20Spring%20Bean%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/"},{"title":"","text":"2.4.3 Spring注解注解是个好东西，但好东西我们也是看见过，整理过，理解过，用过才知道好。不求我们每个都记住，但求保有印象，在需要的时候能提取出来再查找相关资料，平时工作就不会显得那么被动了。 1.@Configuration注解该类等价 与XML中配置beans，相当于Ioc容器，它的某个方法头上如果注册了@Bean，就会作为这个Spring容器中的Bean，与xml中配置的bean意思一样。 @Configuration注解的类必需使用context:component-scanbase-package=&quot;XXX&quot;扫描.如下： 1234567891011@Configuration public class MainConfig { //在properties文件里配置 @Value(&quot;${wx_appid}&quot;) public String appid; protected MainConfig(){} @Bean public WxMpService wxMpService() { WxMpService wxMpService = new WxMpServiceImpl(); wxMpService.setWxMpConfigStorage(wxMpConfigStorage()); return wxMpService; }} 定义一个MainConfig，用@Configuration注解，那MainConfig相当于xml里的beans,里面用@Bean注解的和xml里定义的bean等价，用context:component-scanbase-package=”XXX”扫描该类，最终我们可以在程序里用@AutoWired或@Resource注解取得用@Bean注解的bean，和用xml先配置bean然后在程序里自动注入一样。目的是减少xml里配置。 2.@Value注解为了简化从properties里取配置，可以使用@Value, 可以在properties文件中的配置值。在dispatcher-servlet.xml里引入properties文件。在程序里使用@Value: 12@Value(&quot;${wx_appid}&quot;)public String appid； 即使给变量赋了初值也会以配置文件的值为准。 3.@Controller,@Service,@Repository,@Component目前4种注解意思是一样，并没有什么区别，区别只是名字不同。使用方法： ​ 1.使用context:component-scanbase-package=”XXX”/扫描被注解的类​ 2.在类上写注解： 123@Controllerpublic class TestController {} 4.@PostConstruct 和 @PreDestory实现初始化和销毁bean之前进行的操作，只能有一个方法可以用此注释进行注释，方法不能有参数，返回值必需是void,方法需要是非静态的。 12345678910public class TestService { @PostConstruct public void init() { // 在构造方法和init方法(如果有的话)之间得到调用，且只会执行一次。 System.out.println(&quot;初始化&quot;); } @PreDestroy public void dostory(){ // 注解的方法在destory()方法调用后得到执行。 System.out.println(&quot;销毁&quot;); } } 流程图： 引深一点，Spring 容器中的 Bean 是有生命周期的，Spring 允许在 Bean 在初始化完成后以及 Bean 销毁前执行特定的操作，常用的设定方式有以下三种： 1.通过实现 InitializingBean/DisposableBean 接口来定制初始化之后/销毁之前的操作方法； 2.通过 元素的 init-method/destroy-method属性指定初始化之后 /销毁之前调用的操作方法； 3.在指定方法上加上@PostConstruct 或@PreDestroy注解来制定该方法是在初始化之后还是销毁之前调用 但他们之前并不等价。即使3个方法都用上了，也有先后顺序. Constructor &gt; @PostConstruct &gt;InitializingBean &gt; init-method @Primary自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常。 例如： @Component public class Apple implements Fruit{@Override public String hello() { return “我是苹果”; }} @Component @Primary public class Pear implements Fruit{@Override public String hello(String lyrics) { return “梨子”; }} public class FruitService {//Fruit有2个实例子类，因为梨子用@Primary，那么会使用Pear注入 @Autowired private Fruit fruit; public String hello(){ return fruit.hello(); }} @Lazy(true)用于指定该Bean是否取消预初始化，用于注解类，延迟初始化。 @AutowiredAutowired默认先按byType，如果发现找到多个bean，则，又按照byName方式比对，如果还有多个，则报出异常。 1.可以手动指定按byName方式注入，使用@Qualifier。 //通过此注解完成从spring配置文件中 查找满足Fruit的bean,然后按//@Qualifier指定pean @Autowired @Qualifier(“pean”) public Fruit fruit; 2.如果要允许null 值，可以设置它的required属性为false，如： @Autowired(required=false) public Fruit fruit; @Resource默认按 byName自动注入,如果找不到再按byType找bean,如果还是找不到则抛异常，无论按byName还是byType如果找到多个，则抛异常。 可以手动指定bean,它有2个属性分别是name和type，使用name属性，则使用byName的自动注入，而使用type属性时则使用byType自动注入。 @Resource(name=”bean名字”) 或 @Resource(type=”bean的class”) 这个注解是属于J2EE的，减少了与spring的耦合。 @Asyncjava里使用线程用3种方法： 继承Thread，重写run方法实现Runnable,重写run方法使用Callable和Future接口创建线程，并能得到返回值。前2种简单，第3种方式特别提示一下，例子如下： class MyCallable implements Callable { private int i = 0; // 与run()方法不同的是，call()方法具有返回值 @Override public Integer call() { int sum = 0; for (; i &lt; 100; i++) { System.out.println(Thread.currentThread().getName() + “ “ + i); sum += i; } return sum; } } main方法： public static void main(String[] args) { Callable myCallable = new MyCallable(); // 创建MyCallable对象 FutureTask ft = new FutureTask(myCallable); //使用FutureTask来包装MyCallable对象 for (int i = 0; i &lt; 100; i++) { System.out.println(Thread.currentThread().getName() + “ “ + i); if (i == 30) { Thread thread = new Thread(ft); //FutureTask对象作为Thread对象的target创建新的线程 thread.start(); //线程进入到就绪状态 } } System.out.println(“主线程for循环执行完毕..”); try { int sum = ft.get(); //取得新创建的新线程中的call()方法返回的结果 System.out.println(“sum = “ + sum); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } 而使用@Async可视为第4种方法。基于@Async标注的方法，称之为异步方法,这个注解用于标注某个方法或某个类里面的所有方法都是需要异步处理的。被注解的方法被调用的时候，会在新线程中执行，而调用它的方法会在原来的线程中执行。 application.xml形势的配置： 第一步配置XML。 参数解读： 配置参数： id：当配置多个executor时，被@Async(“id”)指定使用；也被作为线程名的前缀。 pool-size： core size：最小的线程数，缺省：1 max size：最大的线程数，缺省：Integer.MAX_VALUE queue-capacity：当最小的线程数已经被占用满后，新的任务会被放进queue里面，当这个queue的capacity也被占满之后，pool里面会创建新线程处理这个任务，直到总线程数达到了max size，这时系统会拒绝这个任务并抛出TaskRejectedException异常(缺省配置的情况下，可以通过rejection-policy来决定如何处理这种情况)。缺省值为：Integer.MAX_VALUE keep-alive：超过core size的那些线程，任务完成后，再经过这个时长(秒)会被结束掉 rejection-policy：当pool已经达到max size的时候，如何处理新任务 ABORT(缺省)：抛出TaskRejectedException异常，然后不执行DISCARD：不执行，也不抛出异常 DISCARD_OLDEST：丢弃queue中最旧的那个任务 CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 第二步在类或方法上添加@Async，当调用该方法时，则该方法即是用异常执行的方法单独开个新线程执行。 @Async(“可以指定执行器id，也可以不指定”) public static void testAsyncVoid (){ try { //让程序暂停100秒，相当于执行一个很耗时的任务 System.out.println(“异常执行打印字符串”); Thread.sleep(100000); } catch (InterruptedException e) { e.printStackTrace(); } } 当在外部调用testAsync方法时即在新线程中执行，由上面执行器去维护线程。 总结：先用context:component-scan去扫描注解，让spring能识别到@Async注解，然后task:annotation-driven去驱动@Async注解，并可以指定默认的线程执行器executor。那么当用@Async注解的方法或类得到调用时，线程执行器会创建新的线程去执行。 上面方法是无返回值的情况，还有异常方法有返回值的例子。 @Async public Future testAsyncReturn () { System.out.println(“Execute method asynchronously - “ + Thread.currentThread().getName()); try { Thread.sleep(5000); return new AsyncResult(“hello world !!!!”); } catch (InterruptedException e) { // } return null; } 返回的数据类型为Future类型，接口实现类是AsyncResult. 调用方法如下： public void test(){ Future future = cc.testAsyncReturn(); while (true) { ///这里使用了循环判断，等待获取结果信息 if (future.isDone()) { //判断是否执行完毕 System.out.println(“Result from asynchronous process - “ + future.get()); break; } System.out.println(“Continue doing something else. “); Thread.sleep(1000); } } 通过不停的检查Future的状态来获取当前的异步方法是否执行完毕 参考文章 编程的方式使用@Async: @Configuration @EnableAsync public class SpringConfig { private int corePoolSize = 10; private int maxPoolSize = 200; private int queueCapacity = 10; private String ThreadNamePrefix = “MyLogExecutor-“; @Bean public Executor logExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(corePoolSize); executor.setMaxPoolSize(maxPoolSize); executor.setQueueCapacity(queueCapacity); executor.setThreadNamePrefix(ThreadNamePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor; }} 10.@Named@Named和Spring的@Component功能相同。@Named可以有值，如果没有值生成的Bean名称默认和类名相同。比如 @Named public class Person 或 @Named(“cc”) public class Person @Inject使用@Inject需要引用javax.inject.jar，它与Spring没有关系，是jsr330规范。 与@Autowired有互换性。 @Singleton只要在类上加上这个注解，就可以实现一个单例类，不需要自己手动编写单例实现类。 13.@Valid,@Valided@Valid 网上一大片使用@Valid失效不能用的情况。为什么呢？ 1.@Valid必需使用在以@RequestBody接收参数的情况下。 2.使用ajax以POST方式提示数据，禁止用Fiddler以及浏览器直接访问的方式测试接口 3.用添加注解驱动。 4.@Valid是应用在javabean上的校验。 org.hibernate hibernate-validator 4.2.0.Final com.fasterxml.jackson.core jackson-annotations 2.5.3 com.fasterxml.jackson.core jackson-core 2.5.3 com.fasterxml.jackson.core jackson-databind 2.5.3 org.codehaus.jackson jackson-mapper-asl 1.9.8 com.fasterxml.jackson.module jackson-module-jaxb-annotations 2.5.3 这些jar包是需要的。@Valid是使用hibernate validation的时候使用，可参数下面介绍的@RequestBody 6.@Valid下后面紧跟BindingResult result，验证结果保存在result 例如： @RequestMapping(“/test”) public String testValid(@Valid User user, BindingResult result){ if (result.hasErrors()){ List errorList = result.getAllErrors(); for(ObjectError error : errorList){ System.out.println(error.getDefaultMessage()); } } return “test”; } 在入参User上添加了@Valid做校验，在User类里属性上实行实际的特定校验。 例如在User的name属性上加 @NotBlank private String name; 全部参数校验如下： 空检查 @Null 验证对象是否为null @NotNull 验证对象是否不为null, 无法查检长度为0的字符串 @NotBlank 检查约束字符串是不是Null还有被Trim的长度是否大于0,只对字符串,且会去掉前后空格. @NotEmpty 检查约束元素是否为NULL或者是EMPTY. Booelan检查 @AssertTrue 验证 Boolean 对象是否为 true @AssertFalse 验证 Boolean 对象是否为 false 长度检查 @Size(min=, max=) 验证对象(Array,Collection,Map,String)长度是否在给定的范围之内 @Length(min=, max=)验证注解的元素值长度在min和max区间内 日期检查 @Past 验证 Date 和 Calendar 对象是否在当前时间之前 @Future 验证 Date 和 Calendar 对象是否在当前时间之后 @Pattern 验证 String 对象是否符合正则表达式的规则 数值检查，建议使用在Stirng,Integer类型，不建议使用在int类型上，因为表单值为“”时无法转换为int，但可以转换为Stirng为””,Integer为null @Min(value=””) 验证 Number 和 String 对象是否大等于指定的值 @Max(value=””) 验证 Number 和 String 对象是否小等于指定的值 @DecimalMax(value=值) 被标注的值必须不大于约束中指定的最大值. 这个约束的参数是一个通过BigDecimal定义的最大值的字符串表示.小数存在精度 @DecimalMin(value=值) 被标注的值必须不小于约束中指定的最小值. 这个约束的参数是一个通过BigDecimal定义的最小值的字符串表示.小数存在精度 @Digits 验证 Number 和 String 的构成是否合法 @Digits(integer=,fraction=)验证字符串是否是符合指定格式的数字，interger指定整数精度，fraction指定小数精度。 @Range(min=, max=) 检查数字是否介于min和max之间. @Range(min=10000,max=50000,message=”range.bean.wage”) private BigDecimal wage; @Valid 递归的对关联对象进行校验, 如果关联对象是个集合或者数组,那么对其中的元素进行递归校验,如果是一个map,则对其中的值部分进行校验.(是否进行递归验证) @CreditCardNumber信用卡验证 @Email 验证是否是邮件地址，如果为null,不进行验证，算通过验证。 @ScriptAssert(lang=,script=, alias=) @URL(protocol=,host=,port=,regexp=, flags=) @Validated @Valid是对javabean的校验，如果想对使用@RequestParam方式接收参数方式校验使用@Validated 使用@Validated的步骤： 第一步：定义全局异常，让该全局异常处理器能处理所以验证失败的情况，并返回给前台失败提示数据。如下，该类不用在任何xml里配置。 import javax.validation.ValidationException; import org.springframework.context.annotation.Bean; import org.springframework.http.HttpStatus; import org.springframework.stereotype.Component; import org.springframework.validation.beanvalidation.MethodValidationPostProcessor; import org.springframework.web.bind.annotation.ControllerAdvice; import org.springframework.web.bind.annotation.ExceptionHandler; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.ResponseStatus; @ControllerAdvice @Component public class GlobalExceptionHandler { @Bean public MethodValidationPostProcessor methodValidationPostProcessor() { return new MethodValidationPostProcessor(); } @ExceptionHandler@ResponseBody@ResponseStatus(HttpStatus.BAD_REQUEST)public String handle(ValidationException exception) { System.out.println(“bad request, “ + exception.getMessage()); return “bad request, “ + exception.getMessage();}} 第二步。在XXController.java头上添加@Validated，然后在@RequestParam后台使用上面介绍的验证注解，比如@NotBlank,@Rank. 如下： @Controller @RequestMapping(“/test”) @Validated public class TestController extends BaseController {@RequestMapping(value = “testValidated”, method = RequestMethod.GET)@ResponseBody@ResponseStatus(HttpStatus.BAD_REQUEST)public Object testValidated(@RequestParam(value = “pk”, required = true) @Size(min = 1, max = 3) String pk, @RequestParam(value = “age”, required = false) @Range(min = 1, max = 3) String age) { try { return “pk:” + pk + “,age=” + age; } catch (Throwable t) { return buildFailure(“消息列表查询失败”); }}} 当入非法参数是，会被全局处理器拦截到，(Spring切面编程方式)，如果参数非法即刻给前台返回错误数据。 测试：http://127.0.0.1:8080/TestValidate/test/testValidated?pk=2&amp;age=12 返回： 注意 @Valid是使用hibernateValidation.jar做校验 @Validated是只用springValidator校验机制使用 gitHub下载地址 @Validated与@RequestBody结合使用时，在接口方法里要增加@Valid。例如： public Object edit(@Valid @RequestBody AddrRo addrRo) {…..} 14.@RequestBody@RequestBody(required=true) :有个默认属性required,默认是true,当body里没内容时抛异常。 application/x-www-form-urlencoded：窗体数据被编码为名称/值对。这是标准的编码格式。这是默认的方式 multipart/form-data：窗体数据被编码为一条消息，页上的每个控件对应消息中的一个部分。二进制数据传输方式，主要用于上传文件 注意：必需使用POST方式提交参数，需要使用ajax方式请求，用Fiddler去模拟post请求不能。 引用jar包： Spring相关jar包。 以及 com.fasterxml.jackson.core jackson-annotations 2.5.3 com.fasterxml.jackson.core jackson-core 2.5.3 com.fasterxml.jackson.core jackson-databind 2.5.3 dispatchServlet-mvc.xml配置 第一种，直接配置MappingJackson2HttpMessageCoverter： ​ 第二种：mvc:annotation-driven/ 就不用配置上面bean,默认会配好。 Ajax请求： function testRequestBody() { var o = {“status”:9}; jQuery.ajax({ type: “POST”, url: “http://127.0.0.1:8080/TestValidate/test/testValid&quot;, xhrFields:{ withCredentials:true }, data: JSON.stringify(o), contentType: “application/json”, dataType: “json”, async: false, success:function (data) { console.log(data); }, error: function(res) { console.log(res); } }); } 后台XXXcontroller.java: @RequestMapping(value=”/ testValid “,method=RequestMethod.POST) @ResponseBody public Object setOrderInfo(@RequestBody InfoVO infoVO,HttpServletRequest request, HttpServletResponse response){ InfoVO cVo = getInfoVo(infoVO); return “success”; } 开发时，不是报415，就是400错误，头都大了。还是细节没做到位，注意下面几个要点： Content-Type必需是application/json 需要jackson-databind.jar mvc:annotation-driven/要配置或直接配置bean XXXController.jar在post方式接收数据 最最重要的，使用ajax以post方式请求。不能用Fiddler模拟,不然会出错。 15.@CrossOrigin是Cross-Origin ResourceSharing(跨域资源共享)的简写 作用是解决跨域访问的问题，在Spring4.2以上的版本可直接使用。在类上或方法上添加该注解 例如： @CrossOrigin public class TestController extends BaseController {XXXX} 如果失效则可能方法没解决是GET还是POST方式，指定即可解决问题。 16.@RequestParam作用是提取和解析请求中的参数。@RequestParam支持类型转换，类型转换目前支持所有的基本Java类型 @RequestParam([value=”number”], [required=false]) String number 将请求中参数为number映射到方法的number上。required=false表示该参数不是必需的，请求上可带可不带。 17.@PathVariable，@RequestHeader，@CookieValue，@RequestParam, @RequestBody，@SessionAttributes, @ModelAttribute@PathVariable：处理requet uri部分,当使用@RequestMapping URI template 样式映射时， 即someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上 例如： @Controller @RequestMapping(“/owners/{a}”) public class RelativePathUriTemplateController { @RequestMapping(“/pets/{b}”) public void findPet(@PathVariable(“a”) String a,@PathVariable String b, Model model) { // implementation omitted } } @RequestHeader，@CookieValue: 处理request header部分的注解 将头部信息绑定到方法参数上： @RequestMapping(“/test”) public void displayHeaderInfo(@RequestHeader(“Accept-Encoding”) String encoding, @RequestHeader(“Keep-Alive”)long keepAlive) {//… } //将cookie里JSESSIONID绑定到方法参数上 @RequestMapping(“/test”) public void displayHeaderInfo(@CookieValue(“JSESSIONID”) String cookie) {//… } 18.@Scope配置bean的作用域。 @Controller @RequestMapping(“/test”) @Scope(“prototype”) public class TestController {} 默认是单例模式，即@Scope(“singleton”), singleton：单例，即容器里只有一个实例对象。 prototype：多对象，每一次请求都会产生一个新的bean实例，Spring不无法对一个prototype bean的整个生命周期负责，容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，由程序员负责销毁该对象，不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法，而对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用 request：对每一次HTTP请求都会产生一个新的bean，同时该bean仅在当前HTTP request内有效 web.xml增加如下配置： org.springframework.web.context.request.RequestContextListener session：该针对每一次HTTP请求都会产生一个新的bean，同时该bean仅在当前HTTP session内有效。也要在web.xml配置如下代码： org.springframework.web.context.request.RequestContextListener global session：作用不大，可不用管他。 19.@ResponseStatus@ResponseStatus用于修饰一个类或者一个方法，修饰一个类的时候，一般修饰的是一个异常类,当处理器的方法被调用时，@ResponseStatus指定的code和reason会被返回给前端。value属性是http状态码，比如404，500等。reason是错误信息 当修改类或方法时，只要该类得到调用，那么value和reason都会被添加到response里 例如： @ResponseStatus(value=HttpStatus.FORBIDDEN, reason=”出现了错误”) public class UserException extends RuntimeException{XXXXX} 当某处抛出UserException时，则会把value和reason返回给前端。 @RequestMapping(“/testResponseStatus”) public String testResponseStatus(int i){ if(i==0) throw new UserNotMatchException(); return “hello”; } 修改方法： @ControllerAdvice @Component public class GlobalExceptionHandler { @Bean public MethodValidationPostProcessor methodValidationPostProcessor() { return new MethodValidationPostProcessor(); } @ExceptionHandler@ResponseBody@ResponseStatus(value=HttpStatus.BAD_REQUEST,reason=”哈哈”)public String handle(ValidationException exception) {System.out.println(“bad request, “ + exception.getMessage());return “bad request, “ + exception.getMessage();}} 结果如下： 正如上面所说，该方法得到调用，不论是否抛异常，都会把value和reason添加到response里。 总结：@ResponseStatus是为了在方法或类得到调用时将指定的code和reason添加到response里返前端，就像服务器常给我们报的404错误一样，我们可以自己指定高逼格错误提示。 20.@RestController@RestController = @Controller + @ResponseBody。 是2个注解的合并效果，即指定了该controller是组件，又指定方法返回的是String或json类型数据，不会解决成jsp页面，注定不够灵活，如果一个Controller即有SpringMVC返回视图的方法，又有返回json数据的方法即使用@RestController太死板。 灵活的作法是：定义controller的时候，直接使用@Controller，如果需要返回json可以直接在方法中添加@ResponseBody 21.@ControllerAdvice官方解释是：It is typically used todefine@ExceptionHandler, @InitBinder, and@ModelAttribute methods that apply to all@RequestMapping methods 意思是：即把@ControllerAdvice注解内部使用@ExceptionHandler、@InitBinder、@ModelAttribute注解的方法应用到所有的 @RequestMapping注解的方法。非常简单，不过只有当使用@ExceptionHandler最有用，另外两个用处不大。 @ControllerAdvice public class GlobalExceptionHandler { @ExceptionHandler(SQLException.class) @ResponseStatus(value=HttpStatus.INTERNAL_SERVER_ERROR,reason=”sql查询错误”) @ResponseBody public ExceptionResponse handleSQLException(HttpServletRequest request, Exception ex) { String message = ex.getMessage(); return ExceptionResponse.create(HttpStatus.INTERNAL_SERVER_ERROR.value(), message); } } 即表示让Spring捕获到所有抛出的SQLException异常，并交由这个被注解的handleSQLException方法处理，同时使用@ResponseStatus指定了code和reason写到response上，返回给前端。 22.元注解包括 @Retention @Target @Document @Inherited四种元注解是指注解的注解，比如我们看到的ControllerAdvice注解定义如下。 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Component public @interface ControllerAdvice { XXX } @Retention: 定义注解的保留策略： @Retention(RetentionPolicy.SOURCE) //注解仅存在于源码中，在class字节码文件中不包含 @Retention(RetentionPolicy.CLASS) //默认的保留策略，注解会在class字节码文件中存在，但运行时无法获得， @Retention(RetentionPolicy.RUNTIME) //注解会在class字节码文件中存在，在运行时可以通过反射获取到 @Target：定义注解的作用目标: @Target(ElementType.TYPE) //接口、类、枚举、注解 @Target(ElementType.FIELD) //字段、枚举的常量 @Target(ElementType.METHOD) //方法 @Target(ElementType.PARAMETER) //方法参数 @Target(ElementType.CONSTRUCTOR) //构造函数 @Target(ElementType.LOCAL_VARIABLE)//局部变量 @Target(ElementType.ANNOTATION_TYPE)//注解 @Target(ElementType.PACKAGE) ///包 由以上的源码可以知道，他的elementType 可以有多个，一个注解可以为类的，方法的，字段的等等 @Document：说明该注解将被包含在javadoc中 @Inherited：说明子类可以继承父类中的该注解 比如@Valid注解定义是 表示该注解只能用在方法，属性，构造函数及方法参数上。该注意会被编译到class里可通过反射得到。 23.@RequestMapping处理映射请求的注解。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。有6个属性。 1、 value， method: value：指定请求的实际地址，指定的地址可以是URI Template 模式； method：指定请求的method类型， GET、POST、PUT、DELETE等； 比如： @RequestMapping(value = “/testValid”, method = RequestMethod.POST) @ResponseBody public Object testValid(@RequestBody @Valid Test test,BindingResult result, HttpServletRequest request, HttpServletResponse response) { XXX } value的uri值为以下三类： A) 可以指定为普通的具体值；如@RequestMapping(value =”/testValid”) B) 可以指定为含有某变量的一类值;如@RequestMapping(value=”/{day}”) C) 可以指定为含正则表达式的一类值;如@RequestMapping(value=”/{textualPart:[a-z-]+}.{numericPart:[d]+}”) 可以匹配../chenyuan122912请求。 2、 consumes，produces： consumes： 指定处理请求的提交内容类型(Content-Type)，例如@RequestMapping(value = “/test”, consumes=”application/json”)处理application/json内容类型 produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回； 3 params、headers： params： 指定request中必须包含某些参数值是，才让该方法处理。 例如： @RequestMapping(value = “/test”, method = RequestMethod.GET, params=”name=chenyuan”) public void findOrd(String name) { // implementation omitted } 仅处理请求中包含了名为“name”，值为“chenyuan”的请求. headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求。 @RequestMapping(value = “/test”, method = RequestMethod.GET, headers=”Referer=www.baidu.com&quot;) public void findOrd(String name) { // implementation omitted } 仅处理request的header中包含了指定“Refer”请求头和对应值为“www.baidu.com”的请求 @GetMapping和@PostMapping @GetMapping(value = “page”)等价于@RequestMapping(value = “page”, method = RequestMethod.GET) @PostMapping(value = “page”)等价于@RequestMapping(value = “page”, method = RequestMethod.POST)","link":"/2021/05/13/14000JAVA/2.4.3%20Spring%E6%B3%A8%E8%A7%A3/"},{"title":"","text":"阿里巴巴开源限流系统 Sentinel 全解析Sentinel 入门首先，Sentinel 不算一个特别复杂的系统 ，普通技术开发者也可以轻松理解它的原理和结构。你别看架构图上 Sentinel 的周边是一系列的其它高大山的开源中间件，这不过是一种华丽的包装，其内核 Sentinel Core 确实是非常轻巧的。 首先我们从它的 Hello World 开始，通过深入理解这段入门代码就可以洞悉其架构原理。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt; 限流分为单机和分布式两种，单机限流是指限定当前进程里面的某个代码片段的 QPS 或者 并发线程数 或者 整个机器负载指数，一旦超出规则配置的数值就会抛出异常或者返回 false。我把这里的被限流的代码片段称为「临界区」。 图片 而分布式则需要另启一个集中的发票服务器，这个服务器针对每个指定的资源每秒只会生成一定量的票数，在执行临界区的代码之前先去集中的发票服务领票，如果领成功了就可以执行，否则就会抛出限流异常。所以分布式限流代价较高，需要多一次网络读写操作。如果读者阅读了我的小册《Redis 深度历险》，里面就提到了 Redis 的限流模块，Sentinel 限流的原理和它是类似的，只不过 Sentinel 的发票服务器是自研的，使用了 Netty 框架。 Sentinel 在使用上提供了两种形式，一种是异常捕获形式，一种是布尔形式。也就是当限流被触发时，是抛出异常来还是返回一个 false。下面我们看看它的异常捕获形式，这是单机版 123456789101112131415161718192021222324252627282930313233343536import com.alibaba.csp.sentinel.Entry;import com.alibaba.csp.sentinel.SphU;import com.alibaba.csp.sentinel.slots.block.BlockException;public class SentinelTest { public static void main(String[] args) { // 配置规则 List&lt;FlowRule&gt; rules = new ArrayList&lt;&gt;(); FlowRule rule = new FlowRule(); rule.setResource(&quot;tutorial&quot;); // QPS 不得超出 1 rule.setCount(1); rule.setGrade(RuleConstant.FLOW_GRADE_QPS); rule.setLimitApp(&quot;default&quot;); rules.add(rule); // 加载规则 FlowRuleManager.loadRules(rules); // 下面开始运行被限流作用域保护的代码 while (true) { Entry entry = null; try { entry = SphU.entry(&quot;tutorial&quot;); System.out.println(&quot;hello world&quot;); } catch (BlockException e) { System.out.println(&quot;blocked&quot;); } finally { if (entry != null) { entry.exit(); } } try { Thread.sleep(500); } catch (InterruptedException e) {} } }} 使用 Sentinel 需要我们提供限流规则，在规则的基础上，将临界区代码使用限流作用域结构包裹起来。在上面的例子中限定了 tutorial 资源的单机 QPS 不得超出 1，但是实际上它的运行 QPS 是 2，这多出来的执行逻辑就会被限制，对应的 Sphu.entry() 方法就会抛出限流异常 BlockException。下面是它的运行结果 1234567891011INFO: log base dir is: /Users/qianwp/logs/csp/INFO: log name use pid is: falsehello worldblockedhello worldblockedhello worldblockedhello worldblocked... 从输出中可以看出 Sentinel 在本地文件中记录了详细的限流日志，可以将这部分日志收集起来作为报警的数据源。 我们再看看它的 bool 形式，使用也是很简单，大同小异。 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.List;import com.alibaba.csp.sentinel.SphO;import com.alibaba.csp.sentinel.slots.block.RuleConstant;import com.alibaba.csp.sentinel.slots.block.flow.FlowRule;import com.alibaba.csp.sentinel.slots.block.flow.FlowRuleManager;public class SentinelTest { public static void main(String[] args) { // 配置规则 List&lt;FlowRule&gt; rules = new ArrayList&lt;&gt;(); FlowRule rule = new FlowRule(); rule.setResource(&quot;tutorial&quot;); // QPS 不得超出 1 rule.setCount(1); rule.setGrade(RuleConstant.FLOW_GRADE_QPS); rule.setLimitApp(&quot;default&quot;); rules.add(rule); FlowRuleManager.loadRules(rules); // 运行被限流作用域保护的代码 while (true) { if (SphO.entry(&quot;tutorial&quot;)) { try { System.out.println(&quot;hello world&quot;); } finally { SphO.exit(); } } else { System.out.println(&quot;blocked&quot;); } try { Thread.sleep(500); } catch (InterruptedException e) {} } }} 规则控制上面的例子中规则都是通过代码写死的，在实际的项目中，规则应该需要支持动态配置。这就需要有一个规则配置源，它可以是 Redis、Zookeeper 等数据库，还需要有一个规则变更通知机制和规则配置后台，允许管理人员可以在后台动态配置规则并实时下发到业务服务器进行控制。 有一些规则源存储不支持事件通知机制，比如关系数据库，Sentinel 也提供了定时刷新规则，比如每隔几秒来刷新内存里面的限流规则。下面是 redis 规则源定义 12345678// redis 地址RedisConnectionConfig redisConf = new RedisConnectionConfig(&quot;localhost&quot;, 6379, 1000);// 反序列化算法Converter&lt;String, List&lt;FlowRule&gt;&gt; converter = r -&gt; JSON.parseArray(r, FlowRule.class);// 定义规则源，包含全量和增量部分// 全量是一个字符串key，增量是 pubsub channel keyReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; redisDataSource = new RedisDataSource&lt;List&lt;FlowRule&gt;&gt;(redisConf,&quot;app_key&quot;, &quot;app_pubsub_key&quot;, converter);FlowRuleManager.register2Property(redisDataSource.getProperty()); 健康状态上报与检查接入 Sentinel 的应用服务器需要将自己的限流状态上报到 Dashboard，这样就可以在后台实时呈现所有服务的限流状态。Sentinel 使用拉模型来上报状态，它在当前进程注册了一个 HTTP 服务，Dashboard 会定时来访问这个 HTTP 服务来获取每个服务进程的健康状况和限流信息。 Sentinel 需要将服务的地址以心跳包的形式上报给 Dashboard，如此 Dashboard 才知道每个服务进程的 HTTP 健康服务的具体地址。如果进程下线了，心跳包就停止了，那么对应的地址信息也会过期，如此Dashboard 就能准实时知道当前的有效进程服务列表。 当前版本开源的 Dashboard 不具备持久化能力，当管理员在后台修改了规则时，它会直接通过 HTTP 健康服务地址来同步服务限流规则直接控制具体服务进程。如果应用重启，规则将自动重置。如果你希望通过 Redis 来持久化规则源，那就需要自己定制 Dashboard。定制不难，实现它内置的持久化接口即可。 分布式限流前面我们说到分布式限流需要另起一个 Ticket Server，由它来分发 Ticket，能够获取到 Ticket 的请求才可以允许执行临界区代码，Ticket 服务器也需要提供规则输入源。 Ticket Server 是单点的，如果 Ticket Server 挂掉了，应用服务器限流将自动退化为本地模式。 框架适配Sentinel 保护的临界区是代码块，通过拓展临界区的边界就可以直接适配各种框架，比如 Dubbo、SpringBoot 、GRPC 和消息队列等。每一种框架的适配器会在请求边界处统一定义临界区作用域，用户就可以完全不必手工添加熔断保护性代码，在毫无感知的情况下就自动植入了限流保护功能。 熔断降级限流在于限制流量，也就是 QPS 或者线程的并发数，还有一种情况是请求处理不稳定或者服务损坏，导致请求处理时间过长或者老是频繁抛出异常，这时就需要对服务进行降级处理。所谓的降级处理和限流处理在形式上没有明显差异，也是以同样的形式定义一个临界区，区别是需要对抛出来的异常需要进行统计，这样才可以知道请求异常的频率，有了这个指标才会触发降级。 12345678910111213141516171819202122232425// 定义降级规则List&lt;DegradeRule&gt; rules = new ArrayList&lt;&gt;();DegradeRule rule = new DegradeRule();rule.setResource(&quot;tutorial&quot;);// 5s内异常不得超出10rule.setCount(10);rule.setGrade(RuleConstant.DEGRADE_GRADE_EXCEPTION_COUNT);rule.setLimitApp(&quot;default&quot;);rules.add(rule);DegradeRuleManager.loadRules(rules);Entry entry = null;try { entry = SphU.entry(key); // 业务代码在这里} catch (Throwable t) { // 记录异常 if (!BlockException.isBlockException(t)) { Tracer.trace(t); }} finally { if (entry != null) { entry.exit(); }} 触发限流时会抛出 FlowException，触发熔断时会抛出 DegradeException，这两个异常都继承自 BlockException。 热点限流还有一种特殊的动态限流规则，用于限制动态的热点资源。内部采用 LRU 算法计算出 topn 热点资源，然后对 topn 的资源进行限流，同时还提供特殊资源特殊对待的参数设置。比如在下面的例子中限定了同一个用户的访问频次，同时也限定了同一本书的访问频次，但是对于某个特殊用户和某个特殊的书进行了特殊的频次设置。 图片 12345678910111213141516171819202122ParamFlowRule ruleUser = new ParamFlowRule();// 同样的 userId QPS 不得超过 10ruleUser.setParamIdx(0).setCount(10);// qianwp用户特殊对待，QPS 上限是 100ParamFlowItem uitem = new ParamFlowItem(&quot;qianwp&quot;, 100, String.class);ruleUser.setParamFlowItemList(Collections.singletonList(uitem));ParamFlowRule ruleBook = new ParamFlowRule();// 同样的 bookId QPS 不得超过 20ruleBook.setParamIdx(1).setCount(20);// redis 的书特殊对待，QPS 上限是 100ParamFlowItem bitem = new ParamFlowItem(&quot;redis&quot;, 100, String.class);ruleBook.setParamFlowItemList(Collections.singletonList(item));// 加载规则List&lt;ParamFlowRule&gt; rules = new ArrayList&lt;&gt;();rules.add(ruleUser);rules.add(ruleBook);ParamFlowRuleManager.loadRules(rules)；// userId的用户访问bookId的书Entry entry = Sphu.entry(key, EntryType.IN, 1, userId, bookId); 热点限流的难点在于如何统计定长滑动窗口时间内的热点资源的访问量，Sentinel 设计了一个特别的数据结构叫 LeapArray，内部有较为复杂的算法设计后续需要单独分析。 系统自适应限流 —— 过载保护当系统的负载较高时，为了避免系统被洪水般的请求冲垮，需要对当前的系统进行限流保护。保护的方式是逐步限制 QPS，观察到系统负载恢复后，再逐渐放开 QPS，如果系统的负载又下降了，就再逐步降低 QPS。如此达到一种动态的平衡，这里面涉及到一个特殊的保持平衡的算法。系统的负载指数存在一个问题，它取自操作系统负载的 load1 参数，load1 参数更新的实时性不足，从 load1 超标到恢复的过程存在一个较长的过渡时间，如果使用一刀切方案，在这段恢复时间内阻止任何请求，待 load1 恢复后又立即放开请求，势必会导致负载的大起大落，服务处理的时断时开。为此作者将 TCP 拥塞控制算法的思想移植到这里实现了系统平滑的过载保护功能。这个算法很精巧，代码实现并不复杂，效果却是非常显著。 算法定义了一个稳态公式，稳态一旦打破，系统负载就会出现波动。算法的本质就是当稳态被打破时，通过持续调整相关参数来重新建立稳态。 图片 稳态公式很简单：ThreadNum * (1/ResponseTime) = QPS，这个公式很好理解，就是系统的 QPS 等于线程数乘以单个线程每秒可以执行的请求数量。系统会实时采样统计所有临界区的 QPS 和 ResponseTime，就可以计算出相应的稳态并发线程数。当负载超标时，通过判定当前的线程数是否超出稳态线程数就可以明确是否需要拒绝当前的请求。 定义自适应限流规则需要提供多个参数 系统的负载水平线，超过这个值时触发过载保护功能 当过载保护超标时，允许的最大线程数、最长响应时间和最大 QPS，可以不设置 12345678List&lt;SystemRule&gt; rules = new ArrayList&lt;SystemRule&gt;();SystemRule rule = new SystemRule();rule.setHighestSystemLoad(3.0);rule.setAvgRt(10);rule.setQps(20);rule.setMaxThread(10);rules.add(rule);SystemRuleManager.loadRules(Collections.singletonList(rule)); 从代码中也可以看出系统自适应限流规则不需要定义资源名称，因为它是全局的规则，会自动应用到所有的临界区。如果当负载超标时，所有临界区资源将一起勒紧裤腰带渡过难关。","link":"/2021/06/28/14000JAVA/2.4.50Sentinal/"},{"title":"","text":"2.6.2消息队列协议[TOC] 什么是协议？消息中间件负责数据的传递，存储和分发消费三个部分，数据的存储和分发的过程中肯定要遵循某种约定的规范，采取底层的TCP/IP,UDP协议还是其他的自己构建的协议。 1.计算机操作系统和应用程序通讯时共同遵循一组约定，只有遵循相同的约定和规范，系统和系统之间才能相互交流。2.和一般的网络应用程序的不同它主要负责数据的接收和传递，所以性能比较高。3.协议对数据格式和计算机之间交换数据都必须严格遵守规范。 网络协议的三要素1.语法。用户数据与控制信息的结构与格式，以及数据出现的顺序。http规定了报文和响应报文的格式。2.语义。解释控制信息每个部分的意义。post/get请求。3.时序。对时间发生顺序的详细说明。 一个请求对应一个响应。 消息中间件采用的不是http协议，常见的消息中间件协议：OpenWire，AMQP，MQTT，Kafka 面试题：为什么消息中间不直接使用http协议呢？1.因为http请求报文头和响应报文头比较复杂，包含cookie，数据加密解密，状态码，响应码等附加的功能，但是对于一个消息而言，我们并不需要这么复杂，它只负责数据传递、存储、分发，追求的高性能。2.大部分http都是短连接，在实际的交互过程中，一个请求到响应很有可能会中断，中断以后就不会进行持久化，容易造成请求的丢失。目的是为了保证消息和数据的高可靠和稳健的运行。 AMQP协议AMQP：Advanced Message Queuing Protocol高级消息队列协议。摩根大通集团联合设计。是提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端和消息中间件可传递消息，并不受客户端、中间件不同产品，不同的开发语言等条件的限制。Erlang中的实现由RabbitMQ等。特性： 1.分布式事务支持 2.消息的持久化支持 3.高性能和高可靠的消息处理优势 MQTT协议MQTT：Message Queuing Telemetry Transport 消息队列是IBM开放的一个及时通讯协议，物联网系统架构中的重要组成部分。特点： 1.轻量 2.结构简单 3.传输快，不支持事务 4.没有持久化设计应用场景： 1.适用于计算能力有限 2.低带宽 3.网络不稳定的场景支持：RabbitMQ ActiveMQ OpenMessage协议最近由阿里、雅虎和滴滴出行共同参与与创立的分布式消息中间件、流处理等领域的应用开发标准。特点： 1.结构简单 2.解析速度快 3.支持事务和持久化设计 Kafka协议Kafka协议是基于TCP/IP的二进制协议，消息内部是通过长度来分割，由一些基本数据类型组成。特点： 1.结构简单 2.解析速度快 3.无事务支持 4.有持久化设计","link":"/2021/05/16/14000JAVA/2.6.2%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%8D%8F%E8%AE%AE/"},{"title":"","text":"2.6.3消息队列的持久化持久化：数据能够永久保存，内存中的数据在机器重启后就消失了。 常见的持久化方式 ActiveMQ RabbitMQ Kafka RocketMQ 文件存储 支持 支持 支持 支持 数据库 支持 / / /","link":"/2021/05/15/14000JAVA/2.6.3%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96/"},{"title":"","text":"2.4.60中间件介绍一、分布式消息中间件1.1ActiveMQ -1.2RabbitMQ1.3Kafka - 性能最高，不支持事务，持久化1.4RocketMQ - 阿里滴滴开发，后续不一定会更新使用场景： ​ 消息中间件监控数据​ 异步数据传输场景​ 削峰填谷​ 任务调度​ 海量数据同步场景​ 分布式事务场景​ 日志管理场景​ 大数据分析场景 二、负载均衡1.Nginx 2.LVS负载均衡 3.KeepAlive 4.CDN 三、缓存中间件1.MemCache 2.Redis - 分布式架构 四、数据库中间件1.Mycat 2.ShardingJDBC 五、案例分析算法数据保存订单数据的消息分发分布式事务海量的容错分布式锁分布式会话分库分表 何谓分布式系统呢？就是一个请求由服务器端的多个服务协同处理完成。和单体架构不同的是，单体架构是一个请求发起jvm调度线程（确切的是tomcat线程池）分配线程Thread来处理请求直到释放，而分布式系统：一个请求是有多个系统共同来协同完成，jvm和环境都是独立的。 如何选择一个中间件？1.是否支持集群 - 高可用2.是否跨平台 -3.持久化功能 - 高可靠","link":"/2021/06/28/14000JAVA/2.4.60%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%BB%8B%E7%BB%8D/"},{"title":"","text":"2.6.4消息的分发策略消息的分发策略MQ消息队列有以下几个角色 1.生产者2.存储消息3.消费者那么生产者生成消息以后，MQ进行存储，消费者是2如何获取消息的呢？一般获取数据的方式无外乎推（push）或者拉（pull）两种方式，典型的git就有推拉机制，我们发送的http请求就是一种典型的拉取数据库数据返回的过程。而消息队列MQ是一种推送的过程，而这些推送机制会适用到很多的业务场景也有很多对应推机制策略。 消息的重试在发送消息的过程中可能会出现异常，或网络的抖动，故障等等。因为造成消息的无法消费，比如用户在下订单，消费MQ接受，订单系统出现故障，导致用户支付失败。那么这个时候就需要消息中间件就必须支持消息重试机制策略。 消息的分发策略的机制和对比 ActiveMQ RabbtiMQ Kafka RocketMQ 发布订阅 支持 支持 支持 支持 轮询分发 支持 支持 支持 / 公平分发 / 支持 支持 / 重发 支持 支持 / 支持 消息拉取 / 支持 支持 支持","link":"/2021/05/15/14000JAVA/2.6.4%E6%B6%88%E6%81%AF%E7%9A%84%E5%88%86%E5%8F%91%E7%AD%96%E7%95%A5/"},{"title":"","text":"2.8.2 面试官：为什么redis不能保证100%数据不丢失Redis 在以下 2 个场景下，都会导致数据丢失。AOF 持久化配置为每秒写盘，但这个写盘过程是异步的，Redis 宕机时会存在数据丢失的可能主从复制也是异步的，主从切换时，也存在丢失数据的可能（从库还未同步完成主库发来的数据，就被提成主库）基于以上原因我们可以看到，Redis 本身的无法保证严格的数据完整性。所以，如果把 Redis 当做消息队列，在这方面是有可能导致数据丢失的。 再来看那些专业的消息队列中间件是如何解决这个问题的？像 RabbitMQ 或 Kafka 这类专业的队列中间件，在使用时，一般是部署一个集群，生产者在发布消息时，队列中间件通常会写多个节点，以此保证消息的完整性。这样一来，即便其中一个节点挂了，也能保证集群的数据不丢失。也正因为如此，RabbitMQ、Kafka在设计时也更复杂。毕竟，它们是专门针对队列场景设计的。但 Redis 的定位则不同，它的定位更多是当作缓存来用，它们两者在这个方面肯定是存在差异的。 1.4.4 消息积压怎么办因为 Redis 的数据都存储在内存中，这就意味着一旦发生消息积压，则会导致 Redis 的内存持续增长，如果超过机器内存上限，就会面临被 OOM 的风险。所以，Redis 的 Stream 提供了可以指定队列最大长度的功能，就是为了避免这种情况发生。但 Kafka、RabbitMQ 这类消息队列就不一样了，它们的数据都会存储在磁盘上，磁盘的成本要比内存小得多，当消息积压时，无非就是多占用一些磁盘空间，相比于内存，在面对积压时也会更加坦然 综上，我们可以看到，把 Redis 当作队列来使用时，始终面临的 2 个问题： Redis 本身可能会丢数据 面对消息积压，Redis 内存资源紧张 到这里，Redis 是否可以用作队列，我想这个答案你应该会比较清晰了。如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。","link":"/2021/06/15/14000JAVA/2.8.2%20b%D5%98%1A:%EF%BF%BDHredis%0D%EF%BF%BD%EF%BF%BD%EF%BF%BD100%25pn%0D%221/"},{"title":"","text":"常见分布式锁实现有基于redis、基于数据库、基于zookeeper或etcd等几种实现方式，下面简单对比下这几种实现方式： 基于redis实现对于业务来说，redis使用比较轻便，性能也比较高，通过调研，有90%的业务都是基于redis实现分布式锁。但业务线基本上都是采用单个redis节点实现分布式锁，当redis节点发生failover时，主从节点基于异步复制保证不了数据的强一致性，有可能多个客户端同时获取到锁。Redis的作者提出了一个更安全的实现，叫做Redlock，要求通过N个完全独立的Redis节点基于Quorum机制实现分布式锁，但是N个节点数据同步问题也比较复杂。 另外，大神 Martin Kleppmann，在文章《How to do distributed locking》中对分布式锁原理进行论证，指出Redlock解决不了持有锁的客户端GC pause问题，如下图所示。 客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。除了GC pasue场景，时钟跳跃也会引发同样的问题，解决这个问题，Martin提出可以引入fencing token，即锁的版本号，在锁持有者放生变更时fencing token递增更新，客户端访问共享资源时携带着这个fencing token，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突），如下图所示。 Redis分布式锁引入fencing token机制，为了保证释放锁的安全性，即每个客户端只能释放自己持有的锁，必须使用lua脚本或者事务机制实现GET、比较、DEL这三步操作的原子性（类似于乐观锁机制），使用起来也不是特别方便。 总体来说，如果想要使用redis实现一套特别可靠的分布式锁服务，还是比较复杂的，对于吞吐量比较高并且有一定容错机制的服务，可以考虑直接采用单redis节点的分布式锁方案。 基于Zookeeper实现基于Zookeeper临时有序节点实现分布式锁，通过ZAB协议保证数据一致性。具有Watch功能，可以高效实现阻塞锁。不过业务需要单独部署Zookeeper集群，每个lockkey需要关联一个客户端连接session，不够灵活，性能较差； 基于ETCD实现采用etcd自带的kv存储，通过lockkey的状态值控制锁的获取与释放，基于Raft协议保证数据一致性，支持TTL与watch机制。同样，业务需要单独部署集群，客户端直连etcd的方式不太友好，需要封装客户端或代理层，单Raft group同步数据，服务端总并发能力略低。 基于数据库实现通过数据库写操作的原子特性（unique key）或数据库排他锁实现，实现简单。但不支持TTL机制，如果释放锁sql语句执行失败，有可能产生死锁，性能也比较低。","link":"/2021/06/16/14000JAVA/2.8.3%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"title":"","text":"12345&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt;","link":"/2021/05/25/14000JAVA/2.9.1%20%E5%9F%BA%E4%BA%8E%E5%88%87%E9%9D%A2%E6%A0%A1%E9%AA%8C%E5%8F%82%E6%95%B0/"},{"title":"","text":"[TOC] 从ReentrantLock的实现看AQS的原理及应用1 ReentrantLock1.1 ReentrantLock特性概览 1234567891011121314151617181920212223242526272829// **************************Synchronized的使用方式**************************// 1.用于代码块synchronized (this) {}// 2.用于对象synchronized (object) {}// 3.用于方法public synchronized void test () {}// 4.可重入for (int i = 0; i &lt; 100; i++) { synchronized (this) {}}// **************************ReentrantLock的使用方式**************************public void test () throw Exception { // 1.初始化选择公平锁、非公平锁 ReentrantLock lock = new ReentrantLock(true); // 2.可用于代码块 lock.lock(); try { try { // 3.支持多种加锁方式，比较灵活; 具有可重入特性 if(lock.tryLock(100, TimeUnit.MILLISECONDS)){ } } finally { // 4.手动释放锁 lock.unlock() } } finally { lock.unlock(); }} 1.2 ReentrantLock与AQS的关联 ReentrantLock支持公平锁和非公平锁，并且ReentrantLock的底层就是由AQS来实现的。非公平锁源码中的加锁流程如下： 12345678910111213// java.util.concurrent.locks.ReentrantLock#NonfairSync// 非公平锁static final class NonfairSync extends Sync { ... final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } ...} 这块代码的含义为： 若通过CAS设置变量State（同步状态）成功，也就是获取锁成功，则将当前线程设置为独占线程。 若通过CAS设置变量State（同步状态）失败，也就是获取锁失败，则进入Acquire方法进行后续处理。 平锁源码中获锁的方式： 12345678// java.util.concurrent.locks.ReentrantLock#FairSyncstatic final class FairSync extends Sync { ... final void lock() { acquire(1); } ...} 2.1 原理概览 AQS使用一个Volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。 2.1.1 AQS数据结构 AQS中最基本的数据结构——Node，Node即为上面CLH变体队列中的节点。 解释一下几个方法和属性值的含义： 方法和属性值 含义 waitStatus 当前节点在队列中的状态 thread 表示处于该节点的线程 prev 前驱指针 predecessor 返回前驱节点，没有的话抛出npe nextWaiter 指向下一个处于CONDITION状态的节点（由于本篇文章不讲述Condition Queue队列，这个指针不多介绍） next 后继指针 线程两种锁的模式： 模式 含义 SHARED 表示线程以共享的模式等待锁 EXCLUSIVE 表示线程正在以独占的方式等待锁 waitStatus有下面几个枚举值： 枚举 含义 0 当一个Node被初始化的时候的默认值 CANCELLED 为1，表示线程获取锁的请求已经取消了 CONDITION 为-2，表示节点在等待队列中，节点线程等待唤醒 PROPAGATE 为-3，当前线程处在SHARED情况下，该字段才会使用 SIGNAL 为-1，表示线程已经准备好了，就等资源释放了 2.1.2 同步状态State","link":"/2021/04/25/14000JAVA/2.%E9%94%81/"},{"title":"","text":"Annotations注释Java平台始终具有各种临时注释机制。例如，transient修饰符是一个临时注释，指示字段应被序列化子系统忽略，而 @deprecated javadoc标记是一个临时注释，指示该方法不再使用。从5.0版开始，该平台具有通用注释（也称为元数据）功能，该功能允许您定义和使用自己的注释类型。该工具包括用于声明注释类型的语法，用于注释声明的语法，用于读取注释的API，用于注释的类文件表示以及注释处理工具。 注释不会直接影响程序的语义，但会影响工具和库对程序的处理方式，进而会影响正在运行的程序的语义。可以从1源文件，2class文件读取注释，也可以在3运行时通过反射读取注释。 注释是对javadoc标签的补充。通常，如果标记旨在影响或产生文档，则标记可能应该是javadoc标记；否则，标记应为javadoc标记。否则，它应该是一个注释。 注释：在interface关键字之前加一个符号（@）。每个方法声明都定义了注释类型的 element。方法声明必须没有任何参数或throws子句。返回类型仅限于原语，String，enums，annotations 和 arrays。方法可以具有默认值。这是注释类型声明的示例： 123456789/** * 带注释的API进行增强请求 Request-For-Enhancement(RFE) */public @interface RequestForEnhancement { int id(); String synopsis(); String engineer() default &quot;[unassigned]&quot;; String date() default &quot;[unimplemented]&quot;; } 定义注释类型后，您可以使用它来注释声明。 注释是一种特殊的修饰符，使用方式和其他修饰符（例如 public， static或final）。 按照惯例，注释在其他修饰符之前。 注释包括一个符号（@），后跟一个注释类型和一个键-值对的括号列表。 该值必须是编译时常量。 这是带有与上面声明的注释类型相对应的注释的方法声明： 1234567@RequestForEnhancement( id = 2868724, synopsis = &quot;Enable time-travel&quot;, engineer = &quot;Mr. Peabody&quot;, date = &quot;4/1/3007&quot;)public static void travelThroughTime(Date destination) { ... } 没有元素的注释类型称为marker注释类型，例如： 12/** 指示带注释的API元素的规范是初步的，随时可能更改。 */public @interface Preliminary { } 可以省略标记注释中的括号，如下所示： 1@Preliminary public class TimeTravel { ... } 在具有单个元素的注释中，该元素应命名value，如下所示： 123public @interface Copyright { String value();} 允许在元素名称value的单元素注释中省略元素名称和等号（=），如下所示： 12@Copyright(&quot;2002 Yoyodyne Propulsion Systems&quot;)public class OscillationOverthruster { ... } 将代码联系在一起，我们将构建一个简单的基于注释的测试框架。 首先，我们需要一个标记注释类型来指示一个方法是一种测试方法，并且应该由测试工具来运行： 12345import java.lang.annotation.*;/** 指示带注释的方法是测试方法。此注释仅应在无参数静态方法上使用。*/@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Test { } 注意，注释类型声明本身是带注释的。 这种注释称为元注释。 第一个（@Retention(RetentionPolicy.RUNTIME)）表示该类型的注释将由VM保留，以便可以在运行时以反射方式读取它们。 第二个（@Target(ElementType.METHOD)）指示此注释类型只能用于方法声明。 RetentionPolicy有3个值：CLASS RUNTIME SOURCE按生命周期来划分可分为3类：1.RetentionPolicy.SOURCE：注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃；2.RetentionPolicy.CLASS：注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期；3.RetentionPolicy.RUNTIME：注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在； 12345678910111213141516171819202122232425262728293031323334public class Foo { @Test public static void m1() { } public static void m2() { } @Test public static void m3() { throw new RuntimeException(&quot;Boom&quot;); } public static void m4() { } @Test public static void m5() { } public static void m6() { } @Test public static void m7() { throw new RuntimeException(&quot;Crash&quot;); } public static void m8() { }}import java.lang.reflect.*;public class RunTests {public static void main(String[] args) throws Exception { int passed = 0, failed = 0; for (Method m : Class.forName(args[0]).getMethods()) { if (m.isAnnotationPresent(Test.class)) { try { m.invoke(null); passed++; } catch (Throwable ex) { System.out.printf(&quot;Test %s failed: %s %n&quot;, m, ex.getCause()); failed++; } } } System.out.printf(&quot;Passed: %d, Failed %d%n&quot;, passed, failed);}} 该工具将类名作为命令行参数，并遍历命名类的所有方法，以尝试调用用@Test注释类型（上面定义）进行注释的每个方法。 反映性查询以绿色突出显示以找出方法是否具有@Test注释。 如果测试方法调用引发异常，则认为测试已失败，并打印失败报告。 最后，将打印摘要，显示通过和失败的测试数量。 这是在Foo程序上运行测试工具时的外观（上图）： 1234$ java RunTests FooTest public static void Foo.m3() failed: java.lang.RuntimeException: Boom Test public static void Foo.m7() failed: java.lang.RuntimeException: Crash Passed: 2, Failed 2 尽管此测试工具显然是玩具，但它演示了注释的功能，可以轻松扩展以克服其局限性。","link":"/2021/05/25/14000JAVA/2.9.1%20%E6%B3%A8%E8%A7%A3/"},{"title":"","text":"ThreadLocal内存溢出代码演示和原因分析ThreadLocal 翻译成中文是线程本地变量的意思，也就是说它是线程中的私有变量，每个线程只能操作自己的私有变量，所以不会造成线程不安全的问题。 所谓的线程不安全是指，多个线程在同一时刻对同一个全局变量做写操作时（读操作不会涉及线程不安全问题），如果执行的结果和我们预期的结果不一致就称之为线程不安全，反之，则称为线程安全。 在 Java 语言中解决线程不安全的问题通常有两种手段： 使用锁（使用 synchronized 或 Lock）； 使用 ThreadLocal。 锁的实现方案是在多线程写入全局变量时，通过排队一个一个来写入全局变量，从而就可以避免线程不安全的问题了。比如当我们使用线程不安全的 SimpleDateFormat 对时间进行格式化时，如果使用锁来解决线程不安全的问题，实现的流程就是这样的： 从上述图片可以看出，通过加锁的方式虽然可以解决线程不安全的问题，但同时带来了新的问题，使用锁时线程需要排队执行，因此会带来一定的性能开销。然而，如果使用的是 ThreadLocal 的方式，则是给每个线程创建一个 SimpleDateFormat 对象，这样就可以避免排队执行的问题了，它的实现流程如下图所示： PS：创建 SimpleDateFormat 也会消耗一定的时间和空间，如果线程复用 SimpleDateFormat 的频率比较高的情况下，使用 ThreadLocal 的优势比较大，反之则可以考虑使用锁。 然而，在我们使用 ThreadLocal 的过程中，很容易就会出现内存溢出的问题，如下面的这个事例。 什么是内存溢出？内存溢出（Out Of Memory，简称 OOM）是指无用对象（不再使用的对象）持续占有内存，或无用对象的内存得不到及时释放，从而造成的内存空间浪费的行为就称之为内存泄露。 内存溢出代码演示在开始演示 ThreadLocal 内存溢出的问题之前，我们先使用“-Xmx50m”的参数来设置一下 Idea，它表示将程序运行的最大内存设置为 50m，如果程序的运行超过这个值就会出现内存溢出的问题，设置方法如下： 设置后的最终效果这样的： PS：因为我使用的 Idea 是社区版，所以可能和你的界面不一样，你只需要点击“Edit Configurations…”找到“VM options”选项，设置上“-Xmx50m”参数就可以了。 配置完 Idea 之后，接下来我们来实现一下业务代码。在代码中我们会创建一个大对象，这个对象中会有一个 10m 大的数组，然后我们将这个大对象存储在 ThreadLocal 中，再使用线程池执行大于 5 次添加任务，因为设置了最大运行内存是 50m，所以理想的情况是执行 5 次添加操作之后，就会出现内存溢出的问题，实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ThreadLocalOOMExample { /** * 定义一个 10m 大的类 */ static class MyTask { // 创建一个 10m 的数组（单位转换是 1M -&gt; 1024KB -&gt; 1024*1024B） private byte[] bytes = new byte[10 * 1024 * 1024]; } // 定义 ThreadLocal private static ThreadLocal&lt;MyTask&gt; taskThreadLocal = new ThreadLocal&lt;&gt;(); // 主测试代码 public static void main(String[] args) throws InterruptedException { // 创建线程池 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 5, 60, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(100)); // 执行 10 次调用 for (int i = 0; i &lt; 10; i++) { // 执行任务 executeTask(threadPoolExecutor); Thread.sleep(1000); } } /** * 线程池执行任务 * @param threadPoolExecutor 线程池 */ private static void executeTask(ThreadPoolExecutor threadPoolExecutor) { // 执行任务 threadPoolExecutor.execute(new Runnable() { @Override public void run() { System.out.println(&quot;创建对象&quot;); // 创建对象（10M） MyTask myTask = new MyTask(); // 存储 ThreadLocal taskThreadLocal.set(myTask); // 将对象设置为 null，表示此对象不在使用了 myTask = null; } }); }} 以上程序的执行结果如下： 从上述图片可看出，当程序执行到第 5 次添加对象时就出现内存溢出的问题了，这是因为设置了最大的运行内存是 50m，每次循环会占用 10m 的内存，加上程序启动会占用一定的内存，因此在执行到第 5 次添加任务时，就会出现内存溢出的问题。 原因分析内存溢出的问题和解决方案比较简单，重点在于“原因分析”，我们要通过内存溢出的问题搞清楚，为什么 ThreadLocal 会这样？是什么原因导致了内存溢出？ 要搞清楚这个问题（内存溢出的问题），我们需要从 ThreadLocal 源码入手，所以我们首先打开 set 方法的源码（在示例中使用到了 set 方法），如下所示： 12345678910public void set(T value) { // 得到当前线程 Thread t = Thread.currentThread(); // 根据线程获取到 ThreadMap 变量 ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); // 将内容存储到 map 中 else createMap(t, value); // 创建 map 并将值存储到 map 中} 从上述代码我们可以看出 Thread、ThreadLocalMap 和 set 方法之间的关系：每个线程 Thread 都拥有一个数据存储容器 ThreadLocalMap，当执行 ThreadLocal.set 方法执行时，会将要存储的值放到 ThreadLocalMap 容器中，所以接下来我们再看一下 ThreadLocalMap 的源码： 1234567891011121314151617181920212223242526272829303132static class ThreadLocalMap { // 实际存储数据的数组 private Entry[] table; // 存数据的方法 private void set(ThreadLocal&lt;?&gt; key, Object value) { Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?&gt; k = e.get(); // 如果有对应的 key 直接更新 value 值 if (k == key) { e.value = value; return; } // 发现空位插入 value if (k == null) { replaceStaleEntry(key, value, i); return; } } // 新建一个 Entry 插入数组中 tab[i] = new Entry(key, value); int sz = ++size; // 判断是否需要进行扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); } // ... 忽略其他源码} 从上述源码我们可以看出：ThreadMap 中有一个 Entry[] 数组用来存储所有的数据，而 Entry 是一个包含 key 和 value 的键值对，其中 key 为 ThreadLocal 本身，而 value 则是要存储在 ThreadLocal 中的值。 根据上面的内容，我们可以得出 ThreadLocal 相关对象的关系图，如下所示： 也就是说它们之间的引用关系是这样的：Thread -&gt; ThreadLocalMap -&gt; Entry -&gt; Key,Value，因此当我们使用线程池来存储对象时，因为线程池有很长的生命周期，所以线程池会一直持有 value 值，那么垃圾回收器就无法回收 value，所以就会导致内存一直被占用，从而导致内存溢出问题的发生。 解决方案ThreadLocal 内存溢出的解决方案很简单，我们只需要在使用完 ThreadLocal 之后，执行 remove 方法就可以避免内存溢出问题的发生了，比如以下代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class App { /** * 定义一个 10m 大的类 */ static class MyTask { // 创建一个 10m 的数组（单位转换是 1M -&gt; 1024KB -&gt; 1024*1024B） private byte[] bytes = new byte[10 * 1024 * 1024]; } // 定义 ThreadLocal private static ThreadLocal&lt;MyTask&gt; taskThreadLocal = new ThreadLocal&lt;&gt;(); // 测试代码 public static void main(String[] args) throws InterruptedException { // 创建线程池 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 5, 60, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(100)); // 执行 n 次调用 for (int i = 0; i &lt; 10; i++) { // 执行任务 executeTask(threadPoolExecutor); Thread.sleep(1000); } } /** * 线程池执行任务 * @param threadPoolExecutor 线程池 */ private static void executeTask(ThreadPoolExecutor threadPoolExecutor) { // 执行任务 threadPoolExecutor.execute(new Runnable() { @Override public void run() { System.out.println(&quot;创建对象&quot;); try { // 创建对象（10M） MyTask myTask = new MyTask(); // 存储 ThreadLocal taskThreadLocal.set(myTask); // 其他业务代码... } finally { // 释放内存 taskThreadLocal.remove(); } } }); }} 以上程序的执行结果如下： 从上述结果可以看出我们只需要在 finally 中执行 ThreadLocal 的 remove 方法之后就不会在出现内存溢出的问题了。 remove的秘密那 remove 方法为什么会有这么大的魔力呢？我们打开 remove 的源码看一下： 12345public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);} 从上述源码中我们可以看出，当调用了 remove 方法之后，会直接将 Thread 中的 ThreadLocalMap 对象移除掉，这样 Thread 就不再持有 ThreadLocalMap 对象了，所以即使 Thread 一直存活，也不会造成因为（ThreadLocalMap）内存占用而导致的内存溢出问题了。 总结本篇我们使用代码的方式演示了 ThreadLocal 内存溢出的问题，严格来讲内存溢出并不是 ThreadLocal 的问题，而是因为没有正确使用 ThreadLocal 所带来的问题。想要避免 ThreadLocal 内存溢出的问题，只需要在使用完 ThreadLocal 后调用 remove 方法即可。不过通过 ThreadLocal 内存溢出的问题，让我们搞清楚了 ThreadLocal 的具体实现，方便我们日后更好的使用 ThreadLocal，以及更好的应对面试。","link":"/2021/05/27/14000JAVA/2.9.2%20ThreadLocal/"},{"title":"","text":"4001-JDK中的ThreadPoolExecutor https://mp.weixin.qq.com/s/13G4Vs5Kcy9yrwYsX2yoiQ 一、内容概述首先描述了ThreadPoolExecutor的构造流程以及内部状态管理的机理，随后用大量篇幅深入源码探究了ThreadPoolExecutor线程分配、任务处理、拒绝策略、启动停止等过程，其中对Worker内置类进行重点分析，内容不仅包含其工作原理，更对其设计思路进行了一定分析。文章内容既包含了源码流程分析，还具有设计思路探讨和二次开发实践。 二、构造函数2.1 构造函数参数123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { ...} corePoolSize：核心线程数。提交任务时，当线程池中的线程数 小于 corePoolSize 时，会 新 创建一个核心线程执行任务。当线程数 等于 corePoolSize 时，会将任务 添加进任务队列。 maximumPoolSize：最大线程数。提交任务时，当 任务队列已满 并且线程池中的总线程数 不大于 maximumPoolSize 时，线程池会令非核心线程执行提交的任务。当 大于 maximumPoolSize 时，会执行拒绝策略。 keepAliveTime：非核心线程 空闲时 的存活时间。 unit：keepAliveTime 的单位。 workQueue：任务队列（阻塞队列）。 threadFactory：线程工厂。线程池用来新创建线程的工厂类。 handler：拒绝策略，线程池遇到无法处理的情况时会执行该拒绝策略选择抛弃或忽略任务等。 2.3 常用线程池在进入 ThreadPoolExecutor 的源码分析前，我们先介绍下常用的线程池（其实并不常用，只是JDK自带了）。这些线程池可由 Executors 这个工具类（或叫线程池工厂）来创建。 2.3.1 FixedThreadPool 固定线程数线程池的创建方式如下：其中核心线程数与最大线程数固定且相等，采用以链表为底层结构的无界阻塞队列。 123456public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);} 特点： 核心线程数与最大线程数相等，因此不会创建空闲线程。keepAliveTime 设置与否无关紧要。 采用无界队列，任务会被无限添加，直至内存溢出（OOM）。 由于无界队列不可能被占满，任务在执行前不可能被拒绝（前提是线程池一直处于运行状态）。 应用场景： 适用于线程数固定的场景 适用负载比较重的服务器 2.3.2 SingleThreadExecutor 单线程线程池的创建方式如下：其中核心线程数与最大线程数都为1，采用以链表为底层结构的无界阻塞队列。 1 特点 与 FixedThreadPool 类似，只是线程数为1而已。 应用场景 适用单线程的场景。 适用于对提交任务的处理有顺序性要求的场景。 2.3.3 CachedThreadPool 缓冲线程池的创建方式如下：其中核心线程数为0，最大线程数为Integer.MAX_VALUE（可以理解为无穷大）。采用同步阻塞队列。 - 特点： 核心线程数为0，则初始就创建空闲线程，并且空闲线程的只能等待任务60s，60s内没有提交任务，空闲线程将被销毁。 最大线程数为无穷大，这样会造成巨量线程同时运行，CPU负载过高，导致应用崩溃。 采用同步阻塞队列，即队列不存储任务。提交一个消费一个。由于最大线程数为无穷大，因此，只要提交任务就一定会被消费（应用未崩溃前）。 应用场景： 适用于耗时短、异步的小程序。 适用于负载较轻的服务器。","link":"/2021/12/29/14000JAVA/4001-JDK-ThreadPoolExecutor/"},{"title":"","text":"问题 什么是分布式消息中间件？ 消息中间件的作用是什么？ 消息中间件的使用场景是什么？ 消息中间件选型？ 分布式消息是一种通信机制，和 RPC、HTTP、RMI 等不一样，消息中间件采用分布式中间代理的方式进行通信。如图所示，采用了消息中间件之后，上游业务系统发送消息，先存储在消息中间件，然后由消息中间件将消息分发到对应的业务模块应用（分布式生产者 - 消费者模式）。这种异步的方式，减少了服务之间的耦合程度。 定义消息中间件： 利用高效可靠的消息传递机制进行平台无关的数据交流 基于数据通信，来进行分布式系统的集成 通过提供消息传递和消息排队模型，可以在分布式环境下扩展进程间的通信 在系统架构中引用额外的组件，必然提高系统的架构复杂度和运维的难度，那么在系统中使用分布式消息中间件有什么优势呢？消息中间件在系统中起的作用又是什么呢？ 解耦 冗余（存储） 扩展性 削峰 可恢复性 顺序保证 缓冲 异步通信 面试时，面试官经常会关心面试者对开源组件的选型能力，这既可以考验面试者知识的广度，也可以考验面试者对某类系统的知识的认识深度，而且也可以看出面试者对系统整体把握和系统架构设计的能力。开源分布式消息系统有很多，不同的消息系统的特性也不一样，选择怎样的消息系统，不仅需要对各消息系统有一定的了解，也需要对自身系统需求有清晰的认识。 下面是常见的几种分布式消息系统的对比：选择 答案关键字 什么是分布式消息中间件？通信，队列，分布式，生产消费者模式。 消息中间件的作用是什么？解耦、峰值处理、异步通信、缓冲。 消息中间件的使用场景是什么？异步通信，消息存储处理。 消息中间件选型？语言，协议、HA、数据可靠性、性能、事务、生态、简易、推拉模式。 Kafka 基本概念和架构问题 简单讲下 Kafka 的架构？ Kafka 是推模式还是拉模式，推拉的区别是什么？ Kafka 如何广播消息？ Kafka 的消息是否是有序的？ Kafka 是否支持读写分离？ Kafka 如何保证数据高可用？ Kafka 中 zookeeper 的作用？ 是否支持事务？ 分区数是否可以减少？ Kafka 架构中的一般概念： 架构 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其发送到 Kafka。 Consumer：消费者，也就是接受消息的一方。消费者连接到 Kafka 上并接收消息，进而进行相应的业务逻辑处理。 Consumer Group：一个消费者组可以包含一个或多个消费者。使用多分区 + 多消费者方式可以极大提高数据下游的处理速度，同一消费组中的消费者不会重复消费消息，同样的，不同消费组中的消费者消息消息时互不影响。Kafka 就是通过消费组的方式来实现消息 P2P 模式和广播模式。 Broker：服务代理节点。Broker 是 Kafka 的服务节点，即 Kafka 的服务器。 Topic：Kafka 中的消息以 Topic 为单位进行划分，生产者将消息发送到特定的 Topic，而消费者负责订阅 Topic 的消息并进行消费。 Partition：Topic 是一个逻辑的概念，它可以细分为多个分区，每个分区只属于单个主题。同一个主题下不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）。 Offset：offset 是消息在分区中的唯一标识，Kafka 通过它来保证消息在分区内的顺序性，不过 offset 并不跨越分区，也就是说，Kafka 保证的是分区有序性而不是主题有序性。 Replication：副本，是 Kafka 保证数据高可用的方式，Kafka 同一 Partition 的数据可以在多 Broker 上存在多个副本，通常只有主副本对外提供读写服务，当主副本所在 broker 崩溃或发生网络一场，Kafka 会在 Controller 的管理下会重新选择新的 Leader 副本对外提供读写服务。 Record：实际写入 Kafka 中并可以被读取的消息记录。每个 record 包含了 key、value 和 timestamp。 Kafka Topic Partitions Layout 主题 Kafka 将 Topic 进行分区，分区可以并发读写。 Kafka Consumer Offset consumer offset zookeeperzookeeper Broker 注册：Broker 是分布式部署并且之间相互独立，Zookeeper 用来管理注册到集群的所有 Broker 节点。 Topic 注册：在 Kafka 中，同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也都是由 Zookeeper 在维护 生产者负载均衡：由于同一个 Topic 消息会被分区并将其分布在多个 Broker 上，因此，生产者需要将消息合理地发送到这些分布式的 Broker 上。 消费者负载均衡：与生产者类似，Kafka 中的消费者同样需要进行负载均衡来实现多个消费者合理地从对应的 Broker 服务器上接收消息，每个消费者分组包含若干消费者，每条消息都只会发送给分组中的一个消费者，不同的消费者分组消费自己特定的 Topic 下面的消息，互不干扰。 答案关键字 简单讲下 Kafka 的架构？ Producer、Consumer、Consumer Group、Topic、Partition Kafka 是推模式还是拉模式，推拉的区别是什么？ Kafka Producer 向 Broker 发送消息使用 Push 模式，Consumer 消费采用的 Pull 模式。拉取模式，让 consumer 自己管理 offset，可以提供读取性能 Kafka 如何广播消息？ Consumer group Kafka 的消息是否是有序的？ Topic 级别无序，Partition 有序 Kafka 是否支持读写分离？ 不支持，只有 Leader 对外提供读写服务 Kafka 如何保证数据高可用？ 副本，ack，HW Kafka 中 zookeeper 的作用？ 集群管理，元数据管理 是否支持事务？ 0.11 后支持事务，可以实现”exactly once“ 分区数是否可以减少？ 不可以，会丢失数据 Kafka 使用问题 Kafka 有哪些命令行工具？你用过哪些？ Kafka Producer 的执行过程？ Kafka Producer 有哪些常见配置？ 如何让 Kafka 的消息有序？ Producer 如何保证数据发送不丢失？ 如何提升 Producer 的性能？ 如果同一 group 下 consumer 的数量大于 part 的数量，kafka 如何处理？ Kafka Consumer 是否是线程安全的？ 讲一下你使用 Kafka Consumer 消费消息时的线程模型，为何如此设计？ Kafka Consumer 的常见配置？ Consumer 什么时候会被踢出集群？ 当有 Consumer 加入或退出时，Kafka 会作何反应？ 什么是 Rebalance，何时会发生 Rebalance？ 命令行工具Kafka 的命令行工具在 Kafka 包的/bin目录下，主要包括服务和集群管理脚本，配置脚本，信息查看脚本，Topic 脚本，客户端脚本等。 kafka-configs.sh：配置管理脚本 kafka-console-consumer.sh：kafka 消费者控制台 kafka-console-producer.sh：kafka 生产者控制台 kafka-consumer-groups.sh：kafka 消费者组相关信息 kafka-delete-records.sh：删除低水位的日志文件 kafka-log-dirs.sh：kafka 消息日志目录信息 kafka-mirror-maker.sh：不同数据中心 kafka 集群复制工具 kafka-preferred-replica-election.sh：触发 preferred replica 选举 kafka-producer-perf-test.sh：kafka 生产者性能测试脚本 kafka-reassign-partitions.sh：分区重分配脚本 kafka-replica-verification.sh：复制进度验证脚本 kafka-server-start.sh：启动 kafka 服务 kafka-server-stop.sh：停止 kafka 服务 kafka-topics.sh：topic 管理脚本 kafka-verifiable-consumer.sh：可检验的 kafka 消费者 kafka-verifiable-producer.sh：可检验的 kafka 生产者 zookeeper-server-start.sh：启动 zk 服务 zookeeper-server-stop.sh：停止 zk 服务 zookeeper-shell.sh：zk 客户端 我们通常可以使用kafka-console-consumer.sh和kafka-console-producer.sh脚本来测试 Kafka 生产和消费，kafka-consumer-groups.sh可以查看和管理集群中的 Topic，kafka-topics.sh通常用于查看 Kafka 的消费组情况。 Kafka ProducerKafka producer 的正常生产逻辑包含以下几个步骤： 配置生产者客户端参数常见生产者实例。 构建待发送的消息。 发送消息。 关闭生产者实例。 Producer 发送消息的过程如下图所示，需要经过拦截器，序列化器和分区器，最终由累加器批量发送至 Broker。 producer Kafka Producer 需要以下必要参数： bootstrap.server：指定 Kafka 的 Broker 的地址 key.serializer：key 序列化器 value.serializer：value 序列化器 常见参数： batch.num.messages 默认值：200，每次批量消息的数量，只对 asyc 起作用。 request.required.acks 默认值：0，0 表示 producer 毋须等待 leader 的确认，1 代表需要 leader 确认写入它的本地 log 并立即确认，-1 代表所有的备份都完成后确认。只对 async 模式起作用，这个参数的调整是数据不丢失和发送效率的 tradeoff，如果对数据丢失不敏感而在乎效率的场景可以考虑设置为 0，这样可以大大提高 producer 发送数据的效率。 request.timeout.ms 默认值：10000，确认超时时间。 partitioner.class 默认值：kafka.producer.DefaultPartitioner，必须实现 kafka.producer.Partitioner，根据 Key 提供一个分区策略。有时候我们需要相同类型的消息必须顺序处理，这样我们就必须自定义分配策略，从而将相同类型的数据分配到同一个分区中。 producer.type 默认值：sync，指定消息发送是同步还是异步。异步 asyc 成批发送用 kafka.producer.AyncProducer， 同步 sync 用 kafka.producer.SyncProducer。同步和异步发送也会影响消息生产的效率。 compression.topic 默认值：none，消息压缩，默认不压缩。其余压缩方式还有，”gzip”、”snappy”和”lz4”。对消息的压缩可以极大地减少网络传输量、降低网络 IO，从而提高整体性能。 compressed.topics 默认值：null，在设置了压缩的情况下，可以指定特定的 topic 压缩，未指定则全部压缩。 message.send.max.retries 默认值：3，消息发送最大尝试次数。 retry.backoff.ms 默认值：300，每次尝试增加的额外的间隔时间。 topic.metadata.refresh.interval.ms 默认值：600000，定期的获取元数据的时间。当分区丢失，leader 不可用时 producer 也会主动获取元数据，如果为 0，则每次发送完消息就获取元数据，不推荐。如果为负值，则只有在失败的情况下获取元数据。 queue.buffering.max.ms 默认值：5000，在 producer queue 的缓存的数据最大时间，仅仅 for asyc。 queue.buffering.max.message 默认值：10000，producer 缓存的消息的最大数量，仅仅 for asyc。 queue.enqueue.timeout.ms 默认值：-1，0 当 queue 满时丢掉，负值是 queue 满时 block, 正值是 queue 满时 block 相应的时间，仅仅 for asyc。 Kafka ConsumerKafka 有消费组的概念，每个消费者只能消费所分配到的分区的消息，每一个分区只能被一个消费组中的一个消费者所消费，所以同一个消费组中消费者的数量如果超过了分区的数量，将会出现有些消费者分配不到消费的分区。消费组与消费者关系如下图所示： consumer group Kafka Consumer Client 消费消息通常包含以下步骤： 配置客户端，创建消费者 订阅主题 拉去消息并消费 提交消费位移 关闭消费者实例 过程 因为 Kafka 的 Consumer 客户端是线程不安全的，为了保证线程安全，并提升消费性能，可以在 Consumer 端采用类似 Reactor 的线程模型来消费数据。 消费模型 Kafka consumer 参数 bootstrap.servers：连接 broker 地址，host：port 格式。 group.id：消费者隶属的消费组。 key.deserializer：与生产者的key.serializer对应，key 的反序列化方式。 value.deserializer：与生产者的value.serializer对应，value 的反序列化方式。 session.timeout.ms：coordinator 检测失败的时间。默认 10s 该参数是 Consumer Group 主动检测 （组内成员 comsummer) 崩溃的时间间隔，类似于心跳过期时间。 auto.offset.reset：该属性指定了消费者在读取一个没有偏移量后者偏移量无效（消费者长时间失效当前的偏移量已经过时并且被删除了）的分区的情况下，应该作何处理，默认值是 latest，也就是从最新记录读取数据（消费者启动之后生成的记录），另一个值是 earliest，意思是在偏移量无效的情况下，消费者从起始位置开始读取数据。 enable.auto.commit：否自动提交位移，如果为false，则需要在程序中手动提交位移。对于精确到一次的语义，最好手动提交位移 fetch.max.bytes：单次拉取数据的最大字节数量 max.poll.records：单次 poll 调用返回的最大消息数，如果处理逻辑很轻量，可以适当提高该值。但是max.poll.records条数据需要在在 session.timeout.ms 这个时间内处理完 。默认值为 500 request.timeout.ms：一次请求响应的最长等待时间。如果在超时时间内未得到响应，kafka 要么重发这条消息，要么超过重试次数的情况下直接置为失败。 Kafka Rebalancerebalance 本质上是一种协议，规定了一个 consumer group 下的所有 consumer 如何达成一致来分配订阅 topic 的每个分区。比如某个 group 下有 20 个 consumer，它订阅了一个具有 100 个分区的 topic。正常情况下，Kafka 平均会为每个 consumer 分配 5 个分区。这个分配的过程就叫 rebalance。 什么时候 rebalance？ 这也是经常被提及的一个问题。rebalance 的触发条件有三种： 组成员发生变更（新 consumer 加入组、已有 consumer 主动离开组或已有 consumer 崩溃了——这两者的区别后面会谈到） 订阅主题数发生变更 订阅主题的分区数发生变更 如何进行组内分区分配？ Kafka 默认提供了两种分配策略：Range 和 Round-Robin。当然 Kafka 采用了可插拔式的分配策略，你可以创建自己的分配器以实现不同的分配策略。 答案关键字 Kafka 有哪些命令行工具？你用过哪些？/bin目录，管理 kafka 集群、管理 topic、生产和消费 kafka Kafka Producer 的执行过程？拦截器，序列化器，分区器和累加器 Kafka Producer 有哪些常见配置？broker 配置，ack 配置，网络和发送参数，压缩参数，ack 参数 如何让 Kafka 的消息有序？Kafka 在 Topic 级别本身是无序的，只有 partition 上才有序，所以为了保证处理顺序，可以自定义分区器，将需顺序处理的数据发送到同一个 partition Producer 如何保证数据发送不丢失？ack 机制，重试机制 如何提升 Producer 的性能？批量，异步，压缩 如果同一 group 下 consumer 的数量大于 part 的数量，kafka 如何处理？多余的 Part 将处于无用状态，不消费数据 Kafka Consumer 是否是线程安全的？不安全，单线程消费，多线程处理 讲一下你使用 Kafka Consumer 消费消息时的线程模型，为何如此设计？拉取和处理分离 Kafka Consumer 的常见配置？broker, 网络和拉取参数，心跳参数 Consumer 什么时候会被踢出集群？奔溃，网络异常，处理时间过长提交位移超时 当有 Consumer 加入或退出时，Kafka 会作何反应？进行 Rebalance 什么是 Rebalance，何时会发生 Rebalance？topic 变化，consumer 变化 高可用和性能问题 Kafka 如何保证高可用？ Kafka 的交付语义？ Replic 的作用？ 什么事 AR，ISR？ Leader 和 Flower 是什么？ Kafka 中的 HW、LEO、LSO、LW 等分别代表什么？ Kafka 为保证优越的性能做了哪些处理？ 分区与副本分区副本 在分布式数据系统中，通常使用分区来提高系统的处理能力，通过副本来保证数据的高可用性。多分区意味着并发处理的能力，这多个副本中，只有一个是 leader，而其他的都是 follower 副本。仅有 leader 副本可以对外提供服务。多个 follower 副本通常存放在和 leader 副本不同的 broker 中。通过这样的机制实现了高可用，当某台机器挂掉后，其他 follower 副本也能迅速”转正“，开始对外提供服务。 为什么 follower 副本不提供读服务？ 这个问题本质上是对性能和一致性的取舍。试想一下，如果 follower 副本也对外提供服务那会怎么样呢？首先，性能是肯定会有所提升的。但同时，会出现一系列问题。类似数据库事务中的幻读，脏读。比如你现在写入一条数据到 kafka 主题 a，消费者 b 从主题 a 消费数据，却发现消费不到，因为消费者 b 去读取的那个分区副本中，最新消息还没写入。而这个时候，另一个消费者 c 却可以消费到最新那条数据，因为它消费了 leader 副本。Kafka 通过 WH 和 Offset 的管理来决定 Consumer 可以消费哪些数据，已经当前写入的数据。 watermark 只有 Leader 可以对外提供读服务，那如何选举 Leader kafka 会将与 leader 副本保持同步的副本放到 ISR 副本集合中。当然，leader 副本是一直存在于 ISR 副本集合中的，在某些特殊情况下，ISR 副本中甚至只有 leader 一个副本。当 leader 挂掉时，kakfa 通过 zookeeper 感知到这一情况，在 ISR 副本中选取新的副本成为 leader，对外提供服务。但这样还有一个问题，前面提到过，有可能 ISR 副本集合中，只有 leader，当 leader 副本挂掉后，ISR 集合就为空，这时候怎么办呢？这时候如果设置 unclean.leader.election.enable 参数为 true，那么 kafka 会在非同步，也就是不在 ISR 副本集合中的副本中，选取出副本成为 leader。 副本的存在就会出现副本同步问题 Kafka 在所有分配的副本 (AR) 中维护一个可用的副本列表 (ISR)，Producer 向 Broker 发送消息时会根据ack配置来确定需要等待几个副本已经同步了消息才相应成功，Broker 内部会ReplicaManager服务来管理 flower 与 leader 之间的数据同步。 sync 性能优化 partition 并发 顺序读写磁盘 page cache：按页读写 预读：Kafka 会将将要消费的消息提前读入内存 高性能序列化（二进制） 内存映射 无锁 offset 管理：提高并发能力 Java NIO 模型 批量：批量读写 压缩：消息压缩，存储压缩，减小网络和 IO 开销 Partition 并发一方面，由于不同 Partition 可位于不同机器，因此可以充分利用集群优势，实现机器间的并行处理。另一方面，由于 Partition 在物理上对应一个文件夹，即使多个 Partition 位于同一个节点，也可通过配置让同一节点上的不同 Partition 置于不同的 disk drive 上，从而实现磁盘间的并行处理，充分发挥多磁盘的优势。 顺序读写Kafka 每一个 partition 目录下的文件被平均切割成大小相等（默认一个文件是 500 兆，可以手动去设置）的数据文件， 每一个数据文件都被称为一个段（segment file）, 每个 segment 都采用 append 的方式追加数据。 追加数据 答案关键字 Kafka 如何保证高可用？ 通过副本来保证数据的高可用，producer ack、重试、自动 Leader 选举，Consumer 自平衡 Kafka 的交付语义？ 交付语义一般有at least once、at most once和exactly once。kafka 通过 ack 的配置来实现前两种。 Replic 的作用？ 实现数据的高可用 什么是 AR，ISR？ AR：Assigned Replicas。AR 是主题被创建后，分区创建时被分配的副本集合，副本个 数由副本因子决定。ISR：In-Sync Replicas。Kafka 中特别重要的概念，指代的是 AR 中那些与 Leader 保 持同步的副本集合。在 AR 中的副本可能不在 ISR 中，但 Leader 副本天然就包含在 ISR 中。关于 ISR，还有一个常见的面试题目是如何判断副本是否应该属于 ISR。目前的判断 依据是：Follower 副本的 LEO 落后 Leader LEO 的时间，是否超过了 Broker 端参数 replica.lag.time.max.ms 值。如果超过了，副本就会被从 ISR 中移除。 Leader 和 Flower 是什么？ Kafka 中的 HW 代表什么？ 高水位值 (High watermark)。这是控制消费者可读取消息范围的重要字段。一 个普通消费者只能“看到”Leader 副本上介于 Log Start Offset 和 HW（不含）之间的 所有消息。水位以上的消息是对消费者不可见的。 Kafka 为保证优越的性能做了哪些处理？ partition 并发、顺序读写磁盘、page cache 压缩、高性能序列化（二进制）、内存映射 无锁 offset 管理、Java NIO 模型","link":"/2021/04/28/14000JAVA/4.Kafka/"},{"title":"","text":"[toc] 三、垃圾回收器的选择1.吞吐量还是响应时间 首先引入两个概念：吞吐量和延迟时间 吞吐量 = CPU在用户应用程序运行的时间 / (CPU在用户应用程序运行的时间 + CPU垃圾回收的时间) 延迟时间 = 平均每次的GC的耗时 通常，吞吐优先还是响应优先这个在JVM中是一个两难之选。堆内存增大，GC一次能处理的数量变大，吞吐量大；但是GC一次的时间会变长，导致后面排队的线程等待时间变长；相反，如果堆内存小，GC一次时间短，排队等待的线程等待时间变短，延迟减少，但一次请求的数量变小(并不绝对符合)，无法同时兼顾。吞吐优先VS响应优先，是JVM调优过程中需要权衡的核心问题。 2.垃圾回收器设计上的考量 垃圾回收器的底层实现机制非常复杂，但是设计者的设计目标无外乎以下几条： JVM在GC时不允许一边垃圾回收，一边还创建新对象(就像不能一边打扫卫生，还在一边扔垃圾)。 基于第一条GC时需要一段Stop the world的暂停时间，而STW会造成系统短暂停顿不能处理任何请求； 新生代收集频率高，性能优先，常用复制算法；老年代频次低，空间敏感，避免复制方式。 所有垃圾回收器的设计目标都是要让GC频率更少，时间更短，减少GC对系统影响！ 3.CMS和G1 目前主流的垃圾回收器配置是新生代采用ParNew，老年代采用CMS组合的方式，或者是完全采用G1回收器，从未来的趋势来看，G1是官方维护和更为推崇的垃圾回收器。 业务系统，延迟敏感的推荐CMS；大内存服务，要求高吞吐的，采用G1回收器！下面单独就两款回收器的工作机制和适用场景进行一下说明： 四、CMS回收器1.CMS垃圾回收器的工作机制 CMS主要是针对老年代的回收器，新生代的采用ParNew回收器，工作流程就是上文提到的经典复制算法，在三块区中进行流转回收，只不过采用多线程并行的方式加快了MinorGC速度。老年代是标记-清除，默认会在一次FullGC算法后做整理算法，清理内存碎片。 优点：并发收集、主打“低延时” 。在最耗时的两个阶段都没有发生STW，而需要STW的阶段都以很快速度完成。 缺点：1、消耗CPU；2、浮动垃圾；3、内存碎片 适用场景：重视服务器响应速度，要求系统停顿时间最短。 2.登录系统的压测前配置 调优场景以之前的登录系统为例，按照之前容量估算套路，引入性能压测环节，测试同学对登录接口压至1s内60M的对象生成速度，假设只配置了4C8G的机器配置，采用ParNew+CMS的组合回收器，堆内存分配4g，线程栈默认1M，初始配置如下： -Xms4g –Xmx4g –Xmn1536m -Xss1m -XX:+UseConcMarkSweepGC划分Eden和Surviror大小，如按照默认-XX:SurvivorRatio=8 分配规则，基于CMS的JVM运行模型粗略计算如下 基本上，可以看到20S后Eden区就满了，此时再运行的时候对象已经无法分配，会触发MinorGC，假设在这次GC后S1装入100M，马上过20S又会触发一次MinorGC，多出来的100M存活对象再加上S1区已经存在的100M，已无法顺利放入到S2区，此时就会触发JVM的动态年龄机制，将一批100M左右的对象推到老年代保存，持续运行一段时间，当老年代也满了的情况下，系统可能不到一小时候就会触发一次FullGC。 3.基于CMS的调优思路 首先采取上调Survior区容量策略：新生代划2g，维持E:S1:S2=8:1:1，此时Eden=1.6G，S=200M。60M/S速率，运行25s左右会触发一次MinorGC，回收的对象需要超过200M才触发进入老年代，对象进入老年代的几率大大降低，短命对象在几次minorGC后就释放掉了。此时的JVM配置如下： -Xms4g –Xmx4g –Xmn2g -Xss1m -XX:SurvivorRatio=8 -XX:+UseConcMarkSweepGC然后再下调晋升老年代年龄，默认为15——当躲过15次MinorGC后，可进入老年代；可适当调低改值为5~10，让长寿对象应尽快去往属于它的地方，而不是在新生代来回折腾，占用空间，这样可以优化每次MinorGC的耗时。 -Xms4g –Xmx4g –Xmn2g -Xss1m -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -XX:+UseConcMarkSweepGC再选择性的去优化老年代参数：比如老年代默认在标记清除以后会做整理，还可以在CMS的增加GC频次还是增加GC时长上做些取舍，如下是响应优先的参数调优： 那么最终我们可以得到一个比较适用于自身业务系统的、基于CMS回收器的JVM参数： -Xms4g –Xmx4g –Xmn2g -Xss1m -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=5 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch 五、G1垃圾回收器1.CMS回收器的不足 服务启动前就需要指定新生代和老年代大小，启动了就不能动态调整了！ 新生代和老年代都必须分配独立且连续的一整块内存空间！ 所有针对老年代的操作必须扫描整个老年代空间，相同的老年代对象，堆空间越大扫描耗时越长！ 2.G1回收器的设计思路 G1回收天然的适用于大内存服务器，首先G1将堆内存空间拆分为多个大小相等的Region块，Region的个数默认2048个，配置4g堆内存，每个region的大小就为2M。Region动态的属于老年代或者新生代，上一秒还是分配成新生代，经过回收以后空出来，下一秒有可能被分为老年代区。 在G1回收器这里已经不需要再提前设置新生代和老年代的大小，但是新生代仍区分Eden和Survivor区。大大降低了JVM参数的调优复杂度，只需配置-XX:MaxGCPauseMillis=n(ms)，设置最大GC停顿时间，剩下的交给G1回收器。G1会自动追踪每个region可以回收的大小和预估的时间，最后在真正垃圾回收的时候，尽量把垃圾回收控制在设置的时间范围内，在有限的时间内回收更多的对象。 所以综合来看，**G1主打高吞吐，特别适用多核、大内存服务(如Kafka/ElasticSearch)**。 3.G1的工作机制 新生代回收：对象优先分配Eden的Region，JVM不停给新生代分配更多的region，直到新生代占堆总大小的60%，触发MinorGC。 进入老年代对象的条件不变：达到晋升年龄；动态年龄判定；大对象等 Mix混合回收：当老年代的Region占堆内存的45%以后，触发MixGC，会分阶段多次混合回收新生代和老年代的Region。 Full GC：MixGC时发现无可用的新Region块了来分配复制的存活对象，立马触发FullGC，停止系统程序，单线程标记、清除和整理，空闲出一批Region，过程很缓慢。 4.G1的核心调优参数 G1收集器自身已经有一套预测和调整机制了，因此我们首先的选择是相信它，即调整-XX:MaxGCPauseMillis=N参数，这也符合G1的目的——让GC调优尽量简单！同时也不要自己显式设置新生代的大小(用-Xmn或-XX:NewRatio参数)，如果人为干预新生代的大小，会导致目标时间这个参数失效。 针对-XX:MaxGCPauseMillis来说，参数的设置带有明显的倾向性：调低↓：延迟更低，但MinorGC频繁，MixGC回收老年代区减少，增大Full GC的风险。调高↑：单次回收更多的对象，但系统整体响应时间也会被拉长。 针对InitiatingHeapOccupancyPercent来说，调参大小的效果也不一样：调低↓：更早触发MixGC，浪费cpu。调高↑：堆积过多代回收region，增大FullGC的风险。 5.G1调优在Kafka集群的应用 比如日志平台的Kafka集群每秒写入300M数据至内存，broker节点的配置为16C32G，假设堆内存给16g，新生代分配8g，每秒产生对象假设100M左右，差不多一分多钟就会产生一次MinorGC，CMS机制下需要等Eden满了以后，才一次性清理大约8g左右的垃圾对象，差不多会有秒级的STW停顿，如果是老年代的GC延时长则会有十秒级的STW停顿。 -Xms16g –Xmx16g –Xmn8g -Xss1m -XX:+UseConcMarkSweepGC假设采用了G1回收器，适当调低最大耗时，设定MaxGCPauseMillis为100ms，并且适当调低堆使用率阈值，G1就会在允许的响应时间内自动的、多批次的去进行垃圾回收，保证每个STW的时间都不会太长。 -Xms16g -Xmx16g -Xss1m -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:InitiatingHeapOccupancyPercent=40所以线上的kafka和ES集群，动辄32~64g的大内存，如果让CMS去整块回收十多G乃至几十G的垃圾对象，对于系统而言绝对不利！一般来说，堆内存超过8g的大内存服务器，都更推荐使用G1回收器！","link":"/2021/07/30/14000JAVA/GC/"},{"title":"","text":"[TOC] Object对象简介除了八大基本数据类型。当然了，八大基本数据类型也能装箱成为对象 1.equals和hashCode方法重写equals()方法，就必须重写hashCode()的方法 12345678910111213141516171819202122232425262728293031323334public boolean equals(Object anObject) { if (this == anObject) { // 地址相同 return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) // 字符相同 return false; i++; } return true; } } return false;}public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;} 2.toString方法3.clone方法protected修饰的类和属性,对于自己、本包和其子类可见 可能会想：clone()方法是定义在Object类上的(以protected来修饰)，而我们自定义的Address对象隐式继承着Object(所有的对象都是Object的子类)，那么子类调用Object以protected来修饰clone()是完全没问题的 但是，IDE现实告诉我，这编译就不通过了！ 123456789101112131415161718public class Address { private int provinceNo; private int cityNo; private int streetNo; public Address() { } public Address(int provinceNo, int cityNo, int streetNo) { this.provinceNo = provinceNo; this.cityNo = cityNo; this.streetNo = streetNo; }} Address address = new Address(1, 2, 3); address.clone(); // clone() has protected access in 'java.lang.Ojbect' 上面的代码就错在：Address与Object不是在同一个包下的，而Address直接访问了Object的clone方法。这是不行的。 4.wait和notify方法 无论是wait、notify还是notifyAll()都需要由监听器对象(锁对象)来进行调用 简单来说：他们都是在同步代码块中调用的，否则会抛出异常！ notify()唤醒的是在等待队列的某个线程(不确定会唤醒哪个)，notifyAll()唤醒的是等待队列所有线程 导致wait()的线程被唤醒可以有4种情况 该线程被中断 wait()时间到了 被notify()唤醒 被notifyAll()唤醒 调用wait()的线程会释放掉锁 notify方法调用后，被唤醒的线程不会立马获得到锁对象。而是等待notify的synchronized代码块执行完之后才会获得锁对象 Thread.sleep()与Object.wait()二者都可以暂停当前线程，释放CPU控制权。 主要的区别在于Object.wait()在释放CPU同时，释放了对象锁的控制。 而Thread.sleep()没有对锁释放 5.finalize()方法finalize()方法将在垃圾回收器清除对象之前调用，但该方法不知道何时调用，具有不定性 一个对象的finalize()方法只会被调用一次，而且finalize()被调用不意味着gc会立即回收该对象，所以有可能调用finalize()后，该对象又不需要被回收了，然后到了真正要被回收的时候，因为前面调用过一次，所以不会调用finalize()，产生问题。","link":"/2021/02/02/14000JAVA/Object%E5%AF%B9%E8%B1%A1/"},{"title":"","text":"[TOC] https://www.cnblogs.com/grey-wolf/p/13034371.html CompletableFuture这个completableFuture是JDK1.8版本新引入的类。下面是这个类。实现了俩接口。本身是个class。这个是Future的实现类。 使用completionStage接口去支持完成时触发的函数和操作。 一个completetableFuture就代表了一个任务。他能用Future的方法。还能做一些之前说的executorService配合futures做不了的。 之前future需要等待isDone为true才能知道任务跑完了。或者就是用get方法调用的时候会出现阻塞。而使用completableFuture的使用就可以用then，when等等操作来防止以上的阻塞和轮询isDone的现象出现。 初始类加载器 ​ 触发符号引用-类 定义类 ​ 双亲委派原则 类名+定义类加载器完全一样才是同一个类 Sep 123public static void main() { Count} 线上开启xdebug1234567891011SCF_BASE_PKG=&quot;-Dscf.serializer.basepakage=com.anjuke;com.bj58&quot;if [[ ${WCloud_Env} == 'Product' || ${WCloud_Env} == 'SandBox' ]];then echo &quot;switch online&quot; cp online/* config/ JAVA_OPTS=&quot;${JAVA_OPTS} -Dspring.profiles.active=online&quot; test ${WCloud_Env} == 'SandBox' &amp;&amp; JAVA_OPTS=&quot;${JAVA_OPTS} -DenabledMock=true -Xdebug -Xrunjdwp:transport=dt_socket,address=9000,server=y,suspend=n &quot;else echo &quot;switch test&quot; cp offline/* config/ JAVA_OPTS=&quot;${JAVA_OPTS_TEST} -Dspring.profiles.active=offline -DenabledMock=true -Xdebug -Xrunjdwp:transport=dt_socket,address=9000,server=y,suspend=n &quot;fi Spring1.容器的启动12BeanFactory ApplicationContext JVM线程私有的：程序计数器，虚拟机栈，本地方法栈线程共享的：堆，方法区，直接内存 1.程序计数器：是唯一不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.虚拟机栈：虚拟机栈是由一个个栈帧组成，线程在执行一个方法时，便会向栈中放入一个栈帧，每个栈帧中都拥有局部变量表、操作数栈、动态链接、方法出口信息。局部变量表存放基本数据类型（boolean、byte、char、short、int、float、long、double）和对象引用（reference)。虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError：若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。 OutOfMemoryError：若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 4.堆是Java 虚拟机所管理的内存中最大的一块，几乎所有的对象实例以及数组都在这里分配内存。也被称作GC堆，收集器基本都采用分代垃圾收集算法。分代回收”是基于这样一个事实：对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。 读写锁 ReadWriteLock stampedLock 乐观读 如何相互转换逗号分隔的字符串和List一、将逗号分隔的字符串转换为List方法 1： 利用JDK的Arrays类 12String str = &quot;a,b,c&quot;;List&lt;String&gt; result = Arrays.asList(str.split(&quot;,&quot;)); 方法 2： 利用Guava的Splitter 12String str = &quot;a, b, c&quot;;List&lt;String&gt; result = Splitter.on(&quot;,&quot;).trimResults().splitToList(str); 方法 3： 利用Apache Commons的StringUtils （只是用了split) 12String str = &quot;a,b,c&quot;;List&lt;String&gt; result = Arrays.asList(StringUtils.split(str,&quot;,&quot;)); 方法 4: 利用Spring Framework的StringUtils 123String str = &quot;a,b,c&quot;;List&lt;String&gt; str = Arrays.asList(StringUtils.commaDelimitedListToStringArray(str)); 二、将List转换为逗号分隔符方法 1： 利用JDK (好像没有很好的方法，需要一步一步实现） 方法 2： 利用Guava的Joiner 12List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;);String str = Joiner.on(&quot;,&quot;).join(list); 方法 3： 利用Apache Commons的StringUtils 12List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;);String str = StringUtils.join(list.toArray(), &quot;,&quot;); 方法 4：利用Spring Framework的StringUtils 12List&lt;String&gt; list = Arrays.asList(&quot;a&quot;, &quot;b&quot;);String str = StringUtils.collectionToDelimitedString(list, &quot;,&quot;);","link":"/2022/03/11/14000JAVA/all/"},{"title":"","text":"VisualVm下载地址： https://visualvm.github.io/?VisualVM_2.0.7 配置文件: 12# /Applications/VisualVM.app/Contents/Resources/visualvm/etc 修改成jdk安装目录visualvm_jdkhome=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_281.jdk/Contents/Home&quot;","link":"/2021/06/28/14000JAVA/visualvm/"},{"title":"离线部署docker项目方案","text":"离线部署docker项目方案1. 项目背景​ 目前我行的信用卡项目使用docker部署，由于生产环境不能联网，因此不能通过dockerfile来进行构建并部署。本文主要介绍在离线环境下运行docker项目的方法，为项目的部署和实施提供参考。 2. 相关命令（1）docker commit该命令的作用和使用方法如下： 12345678910111213[root@localhost azkaban]# docker commit --helpCreate a new image from a container's changesUsage: docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]Create a new image from a container's changes -a, --author Author (e.g., &quot;John Hannibal Smith &lt;hannibal@a-team.com&gt;&quot;) -c, --change=[] Apply Dockerfile instruction to the created image --help Print usage -m, --message Commit message -p, --pause=true Pause container during commit docker commit 命令一般用来对容器的某个状态进行备份。 （2）docker save该命令的作用和使用方法如下： 12345678910[root@localhost azkaban]# docker save --helpSave one or more images to a tar archive (streamed to STDOUT by default)Usage: docker save [OPTIONS] IMAGE [IMAGE...]Save one or more images to a tar archive (streamed to STDOUT by default) --help Print usage -o, --output Write to a file, instead of STDOUT docker save 命令主要是将容器打包成tar格式的文件，方便迁移。 （3）docker load该命令的作用和使用方法如下： 1234567891011[root@localhost azkaban]# docker load --helpLoad an image from a tar archive or STDINUsage: docker load [OPTIONS]Load an image from a tar archive or STDIN --help Print usage -i, --input Read from a tar archive file, instead of STDIN -q, --quiet Suppress the load output docker load命令的作用是docker save的逆过程，将tar镜像文件加载到本地系统中。 （4） docker-compose up|down主要用于启动docker整个项目。需要说明的是离线环境中，通过加载tar文件的方法将容器恢复到生产环境本地，不能使用 docker-compose start|stop命令来启动已经存在的容器,但可以用如下命令来替代： 12[root@localhost azkaban]# docker-compose up --no-recreate[root@localhost azkaban]# 3. 部署方案​ 离线环境下，不能联网，因此不能通过docker build去构建。但是可以先在测试环境将所有docker容器构建好，然后试运行。确认无误后，用docker save命令将容器打包成tar格式的文件，然后上传到生产环境。最后在生产环境通过docker load命令将容器文件加载到本地系统中，最后启动容器即可。 4. 具体实施信用卡项目的部署步骤如下：（1）在测试环境中，构建好容器，然后启动，并查看当前正在运行的容器 123456[root@localhost azkaban]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3dcc2a3c9f69 02azweb &quot;/bin/sh -c /root/run&quot; 16 hours ago Up 5 seconds 0.0.0.0:8443-&gt;8443/tcp azkaban_azweb_1cde60815d308 03azexe &quot;/bin/sh -c /root/run&quot; 16 hours ago Up 6 seconds 0.0.0.0:12321-&gt;12321/tcp azkaban_azexe_1291c90c34bae 01mysql &quot;/bin/sh -c /root/run&quot; 16 hours ago Up 6 seconds 0.0.0.0:3306-&gt;3306/tcp azkaban_myserver.local_1[root@localhost azkaban]# （2）使用docker save命令将正在运行的容器打包 12345[root@localhost azkaban]# docker save -o /root/01mysql.tar 01mysql[root@localhost azkaban]# docker save -o /root/02azweb.tar 02azweb[root@localhost azkaban]# docker save -o /root/03azexe.tar 03azexe （3）将所有的tar文件和启动文件(docker-compose.yml)上传到生产环境(云主机) 1234[root@cdf806db98f4474 creditcard]# ls01mysql.tar 02azweb.tar 03azexe.tar docker-compose.yml[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# （4）使用docker load命令将镜像加载到本地 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@cdf806db98f4474 creditcard]# ls01mysql.tar 02azweb.tar 03azexe.tar docker-compose.yml[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker load -i 01mysql.tarc8305edc6321: Loading layer [==================================================&gt;] 132.5 MB/132.5 MB5c42a1311e7e: Loading layer [==================================================&gt;] 15.87 kB/15.87 kBc3c1fd75be70: Loading layer [==================================================&gt;] 9.728 kB/9.728 kBcc8df16ebbaf: Loading layer [==================================================&gt;] 4.608 kB/4.608 kB4f9d527ff23f: Loading layer [==================================================&gt;] 3.072 kB/3.072 kB1b8e8c715e39: Loading layer [==================================================&gt;] 5.12 kB/5.12 kBe507afbee7f4: Loading layer [==================================================&gt;] 20.45 MB/20.45 MB39ec918b7f2f: Loading layer [==================================================&gt;] 2.048 kB/2.048 kBa4ed412d5891: Loading layer [==================================================&gt;] 2.56 kB/2.56 kBe3be9bb7b038: Loading layer [==================================================&gt;] 1.193 MB/1.193 MB6b76d21c819c: Loading layer [==================================================&gt;] 898.6 kB/898.6 kB67fcaabeec65: Loading layer [==================================================&gt;] 3.834 MB/3.834 MB2013683020bf: Loading layer [==================================================&gt;] 60.06 MB/60.06 MBde20926b4839: Loading layer [==================================================&gt;] 5.567 MB/5.567 MB97f46a45df06: Loading layer [==================================================&gt;] 348.3 MB/348.3 MB97c2201c812f: Loading layer [==================================================&gt;] 299.4 MB/299.4 MBf4f3be87c4b0: Loading layer [==================================================&gt;] 3.072 kB/3.072 kBc7d7e16e71f4: Loading layer [==================================================&gt;] 7.168 kB/7.168 kB0442b4ed1163: Loading layer [==================================================&gt;] 2.56 kB/2.56 kB68f6942b8f28: Loading layer [==================================================&gt;] 2.56 kB/2.56 kB[root@cdf806db98f4474 creditcard]# ====&gt; ] 512 B/2.56 kB[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE01mysql latest f5db485a45c1 2 weeks ago 856.3 MB[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker load -i 02azweb.tar11f4bad0bd51: Loading layer [==================================================&gt;] 15.11 MB/15.11 MBa59613308d7c: Loading layer [==================================================&gt;] 2.048 kB/2.048 kB9b739afd6122: Loading layer [==================================================&gt;] 989.2 kB/989.2 kB31c49ed4c3af: Loading layer [==================================================&gt;] 8.394 MB/8.394 MB0703bda10e6c: Loading layer [==================================================&gt;] 9.216 kB/9.216 kBa100de2c97a8: Loading layer [==================================================&gt;] 4.608 kB/4.608 kB5b6c73a56916: Loading layer [==================================================&gt;] 2.56 kB/2.56 kBe2785bf336a4: Loading layer [==================================================&gt;] 4.608 kB/4.608 kBe3ca7a8321de: Loading layer [==================================================&gt;] 2.56 kB/2.56 kB[root@cdf806db98f4474 creditcard]# ====&gt; ] 512 B/2.56 kB[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE02azweb latest 5be1ca339c00 3 hours ago 582.8 MB01mysql latest f5db485a45c1 2 weeks ago 856.3 MB[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker load -i 03azexe.tar97dc717cd760: Loading layer [==================================================&gt;] 25.28 MB/25.28 MBa66898f83caf: Loading layer [==================================================&gt;] 234 MB/234 MBd31e7bdbac8e: Loading layer [==================================================&gt;] 8.128 MB/8.128 MBda4bb674e32f: Loading layer [==================================================&gt;] 9.338 MB/9.338 MB6e532e578483: Loading layer [==================================================&gt;] 12.21 MB/12.21 MB5d15325db249: Loading layer [==================================================&gt;] 2.048 kB/2.048 kB744ea60090a4: Loading layer [==================================================&gt;] 989.2 kB/989.2 kB140f627335db: Loading layer [==================================================&gt;] 4.608 kB/4.608 kB3f4a866d3902: Loading layer [==================================================&gt;] 5.12 kB/5.12 kBb8bac96bd794: Loading layer [==================================================&gt;] 4.608 kB/4.608 kB4dfb19835bc7: Loading layer [==================================================&gt;] 2.56 kB/2.56 kB2fed8dffed35: Loading layer [==================================================&gt;] 4.608 kB/4.608 kB9a54867c948d: Loading layer [==================================================&gt;] 2.56 kB/2.56 kB[root@cdf806db98f4474 creditcard]# ====&gt; ] 512 B/2.56 kB[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE03azexe latest 7702f5f83fdc 3 hours ago 840.9 MB02azweb latest 5be1ca339c00 3 hours ago 582.8 MB01mysql latest f5db485a45c1 2 weeks ago 856.3 MB （5）项目启动，停止 12345678910111213[root@cdf806db98f4474 creditcard]# docker-compose up......[root@cdf806db98f4474 creditcard]# docker-compose downStopping creditcard_azweb_1 ... doneStopping creditcard_azexe_1 ... doneStopping creditcard_myserver.local_1 ... doneRemoving creditcard_azweb_1 ... doneRemoving creditcard_azexe_1 ... doneRemoving creditcard_myserver.local_1 ... done[root@cdf806db98f4474 creditcard]#[root@cdf806db98f4474 creditcard]# docker-compose up --no-recreate......[root@cdf806db98f4474 creditcard]# （6）查看是否启动成功使用浏览器登录azkaban-web-server的后台界面，如果无误，则部署成功了。 5. 结束语​ 本文主要介绍了离线部署docker项目的方法，通过docker的两个关键命令，可以方便地将容器迁移到云主机，而且容器的启动和停止只需要一行命令可以了，非常快捷高效。","link":"/2021/04/14/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/15001%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2docker%E9%A1%B9%E7%9B%AE%E6%96%B9%E6%A1%88/"},{"title":"Kubernetes架构与工作流程及核心概念","text":"k8s架构与工作流程及核心概念k8s的设计架构k8s集群中，有Master和Node这两种角色。Master管理Node，Node管理容器。Mater主要负责整个集群的管理控制，用于监控、编排、调度集群中的各个工作节点。通常Master会占用一台独立的服务器，基于高可用原因，可能是多台。 Node则是K8s集群中的各个工作节点。Node提供运行容器所需的各种环境，对容器进行实际的控制，而这些容器会提供实际的应用服务。 Mater的组成1.API Server(kube-apiserver)进程为各类资源对象提供了增删改查等HTTP REST接口。也可通过命令行工具kubectl客户端来访问。 2.etcd轻量级的分布式键值存储，可以再单台Master服务器上部署，可理解为k8s“数据库”，用来保存集群中所有的配置和各个对象的状态信息。只有API Server才能直接访问和操作etcd。 3.调度器(kube-scheduler)Pod资源的调度器。用来监听Pod并自动分配Node，通过API Server进程的Watch接口监听新建的Pod，将Pod绑定到目标Node上。 4.控制器管理器(Kube-controller-manager)Node控制器：负责在Node出现故障时做出响应 Replication控制器：负责对系统中的每个ReplicationController对象维护正确数量的Pod。 Endpoint控制器：负责生成和维护所有Endpoint对象的控制器。用于监听Service和对应的Pod副本的变化。 ServiceAccount及Token控制器：为新的命名空间创建默认账户和API访问令牌。 Node的组成kubelet、kube-proxy和容器运行时(container runtime) 1.kubelet每个Node上运行的主要代理进程。kubelet以PodSpec为单位来运行任务，PodSpec是一种描述Pod的YAML或JSON对象。负责维护容器的生命周期，同时也负责存储卷（volume）等资源的管理。 每个Node定期调用Master节点上API Server进程的REST接口，报告自身状态。API Server将Node状态更新到etcd中。 2.kube-proxy","link":"/2022/05/09/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/15030k8s-Master/"},{"title":"Docker Compose","text":"Docker Compose简介：管理容器，定义运行多个容器 docker-compose.yml 的配置案例如下（配置参数参考下文）： 创建 docker-compose.yml在测试目录中创建一个名为 docker-compose.yml 的文件，然后粘贴以下内容： 123456789# yaml 配置version: '3'services: web: build: . ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 该 Compose 文件定义了两个服务：web 和 redis。 web：该 web 服务使用从 Dockerfile 当前目录中构建的镜像。然后，它将容器和主机绑定到暴露的端口 5000。此示例服务使用 Flask Web 服务器的默认端口 5000 。 redis：该 redis 服务使用 Docker Hub 的公共 Redis 映像。 使用 Compose 命令构建和运行您的应用在测试目录中，执行以下命令来启动应用程序： docker-compose up 如果你想在后台执行该服务可以加上 -d 参数： docker-compose up -d Docker SwarmDocker StackDocker SecretDocker ConfigK8s","link":"/2021/11/26/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/15002Docker%20Compose/"},{"title":"","text":"Composer镜像 Dockerfile 12345FROM composer:2.0COPY --from=mlocati/php-extension-installer /usr/bin/install-php-extensions /usr/local/bin/RUN install-php-extensions apcu-5.1.20 yac-2.3.0 mbstring sockets bcmath bz2 calendar exif gettext http igbinary mcrypt msgpack mysqli pcntl pdo_mysql redis sysvmsg sysvsem sysvshm zip gmp memcache memcached shmop gd 创建Dockerfile 1234docker build -t composer .Usage: docker build [OPTIONS] PATH | URL | -生成image名为composer的镜像 docker run –rm -it -v “$PWD:/app” -v “$HOME/.ssh:/root/.ssh” composer composer “$@” install","link":"/2021/11/17/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/2151Composer/"},{"title":"","text":"Docker 中的 PHP 安装扩展插件1. PHP 源码为了保证 Docker 镜像尽量小，PHP 的源文件是以压缩包的形式存在镜像中，官方提供了 docker-php-source 快捷脚本，用于对源文件压缩包的解压（extract）及解压后的文件进行删除（delete）的操作。 示例： 1234FROM php:7.1-apacheRUN docker-php-source extract \\ # 此处开始执行你需要的操作 \\ &amp;&amp; docker-php-source delete 注意：一定要记得删除，否则解压出来的文件会大大增加镜像的文件大小。 2. 安装扩展2.1. 核心扩展 这里主要用到的是官方提供的 docker-php-ext-configure 和 docker-php-ext-install 快捷脚本，如下 12345678910111213FROM php:7.1-fpmRUN apt-get update \\ # 相关依赖必须手动安装 &amp;&amp; apt-get install -y \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ libmcrypt-dev \\ libpng-dev \\ # 安装扩展 &amp;&amp; docker-php-ext-install -j$(nproc) iconv mcrypt \\ # 如果安装的扩展需要自定义配置时 &amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\ &amp;&amp; docker-php-ext-install -j$(nproc) gd 注意：这里的 docker-php-ext-configure 和 docker-php-ext-install 已经包含了 docker-php-source 的操作，所有不需要再手动去执行。 2.2. PECL 扩展 因为一些扩展并不包含在 PHP 源码文件中，所有需要使用 PECL（PHP 的扩展库仓库，通过 PEAR 打包）。用 pecl install 安装扩展，然后再用官方提供的 docker-php-ext-enable 快捷脚本来启用扩展，如下示例 12345678FROM php:7.1-fpmRUN apt-get update \\ # 手动安装依赖 &amp;&amp; apt-get install -y libmemcached-dev zlib1g-dev \\ # 安装需要的扩展 &amp;&amp; pecl install memcached-2.2.0 \\ # 启用扩展 &amp;&amp; docker-php-ext-enable memcached 2.3. 其它扩展 一些既不在 PHP 源码包，也不再 PECL 扩展仓库中的扩展，可以通过下载扩展程序源码，编译安装的方式安装，如下示例： 1234567891011121314FROM php:5.6-apacheRUN curl -fsSL 'https://xcache.lighttpd.net/pub/Releases/3.2.0/xcache-3.2.0.tar.gz' -o xcache.tar.gz \\ &amp;&amp; mkdir -p xcache \\ &amp;&amp; tar -xf xcache.tar.gz -C xcache --strip-components=1 \\ &amp;&amp; rm xcache.tar.gz \\ &amp;&amp; ( \\ cd xcache \\ &amp;&amp; phpize \\ &amp;&amp; ./configure --enable-xcache \\ &amp;&amp; make -j$(nproc) \\ &amp;&amp; make install \\ ) \\ &amp;&amp; rm -r xcache \\ &amp;&amp; docker-php-ext-enable xcache 注意：官方提供的 docker-php-ext-* 脚本接受任意的绝对路径（不支持相对路径，以便与系统内置的扩展程序进行区分），所以，上面的例子也可以这样写： 12345678FROM php:5.6-apacheRUN curl -fsSL 'https://xcache.lighttpd.net/pub/Releases/3.2.0/xcache-3.2.0.tar.gz' -o xcache.tar.gz \\ &amp;&amp; mkdir -p /tmp/xcache \\ &amp;&amp; tar -xf xcache.tar.gz -C /tmp/xcache --strip-components=1 \\ &amp;&amp; rm xcache.tar.gz \\ &amp;&amp; docker-php-ext-configure /tmp/xcache --enable-xcache \\ &amp;&amp; docker-php-ext-install /tmp/xcache \\ &amp;&amp; rm -r /tmp/xcache 将以下代码保存为一份xxx.sh 并执行，即可扩展mysql、gd、phalcon 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#! /usr/binPHP_VERSION=7.0.10docker run --name php \\-v /home/wwwroot:/home/wwwroot \\-v ~/php_config/php.ini:/usr/local/etc/php/php.ini \\-p 9000:9000 \\-d php:${PHP_VERSION}-fpmdocker exec -it php sed -i &quot;s/33/2016/g&quot; /etc/passwddocker exec -it php sed -i &quot;s/33/2016/g&quot; /etc/groupdocker exec -it php bash -c &quot;set -ex \\&amp;&amp; cd ~ \\&amp;&amp; mv /etc/apt/sources.list /etc/apt/sources.list.bak \\&amp;&amp; { \\echo deb http://mirrors.aliyun.com/debian/ jessie main non-free contrib; \\echo deb http://mirrors.aliyun.com/debian/ jessie-proposed-updates main non-free contrib; \\echo deb-src http://mirrors.aliyun.com/debian/ jessie main non-free contrib; \\echo deb-src http://mirrors.aliyun.com/debian/ jessie-proposed-updates main non-free contrib; \\} | tee /etc/apt/sources.list \\&amp;&amp; apt-get update \\&amp;&amp; apt-get install -y git \\libpcre3-dev \\libfreetype6-dev \\libjpeg62-turbo-dev \\libmcrypt-dev \\libpng12-dev \\&amp;&amp; docker-php-ext-install -j$(nproc) iconv mcrypt \\&amp;&amp; docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\&amp;&amp; docker-php-ext-install -j$(nproc) gd \\&amp;&amp; docker-php-ext-install mysql \\&amp;&amp; docker-php-ext-install pdo_mysql \\&amp;&amp; curl -o /usr/local/etc/php/php.ini https://raw.githubusercontent.com/php/php-src/PHP-${PHP_VERSION}/php.ini-production \\&amp;&amp; git clone -b 2.1.x --depth=1 git://github.com/phalcon/cphalcon.git ~/cphalcon \\&amp;&amp; cd ~/cphalcon/ext \\&amp;&amp; export CFLAGS=\\&quot;-O2 -finline-functions -fvisibility=hidden\\&quot; \\&amp;&amp; phpize \\&amp;&amp; ./configure --enable-phalcon \\&amp;&amp; make \\&amp;&amp; make install \\&amp;&amp; docker-php-ext-enable phalcon \\&amp;&amp; rm -rf ~/cphalcon&quot;docker commit -a &quot;technofiend &lt;2281551151@qq.com&gt;&quot; -m &quot;install gd、 phalcon、pdo_mysql、mysql extsions&quot; php phalcon:${PHP_VERSION}-fpmdocker rm -f phpdocker run --name php \\-v /home/wwwroot:/home/wwwroot \\-v ~/php_config/php.ini:/usr/local/etc/php/php.ini \\-p 9000:9000 \\-d phalcon:${PHP_VERSION}-fpmdocker exec -it php sed -i &quot;s/33/2016/g&quot; /etc/passwddocker exec -it php sed -i &quot;s/33/2016/g&quot; /etc/group docker 中安装PHP扩展12345678910可以通过两种方式实现1.pecl pdo_msql 方式二：docker-php-ext-install pdo pdo_mysql如果报 /usr/local/bin/docker-php-ext-enable: cannot create /usr/local/etc/php/conf.d/docker-php-ext-pdo_mysql.ini: Directory nonexistent 解决方案： 直接在/usr/local/etc/php目录下面新建 conf.d目录和对应的docker-php-ext-pdo_msql.ini文件其中docker-php-ext-pdo_msql.ini的内容为：extension=pdo_mysql.so","link":"/2021/09/02/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/Docker%E5%AE%89%E8%A3%85PHP%E6%89%A9%E5%B1%952/"},{"title":"","text":"Install PHP extensions on DockerI’m developing PHP applications on Docker containers. But there are some points you will struggle to solve even though you use official Docker images. For this time, I will introduce how we can install extensions for PHP on Docker. Core extensionsAccording to official Docker image, some docker-php-ext-* commands are prepared for users to install extensions easily on the containers. For example, you add the following line to Dockerfile to install curl extension. 1RUN docker-php-ext-install curl Pecl extensionsPecl extensions will be also easy to be put on the containers. If you want to install memcached extension, you can do that like below. 12RUN pecl install memcached \\ &amp;&amp; docker-php-ext-enable memcached Above all, it looks like very easy steps to deal with that. But you will be in trouble when you try to install extensions which need PHP compiling options. Pecl extensions with Compiling optionsActually, I consumed a long time to solve this problem when I install mailparse extension which requires --enable-mailparse option to build PHP. Finally, I got the solution. In Dockerfile of offical image, I found $PHP_EXTRA_CONFIGURE_ARGS . https://github.com/docker-library/php/blob/9abc1efe542b56aa93835e4987d5d4a88171b232/7.0/alpine/Dockerfile#L121 From Docker version 1.9, --build-arg option is provided to define environmental variables when we build Dockerfile. So you can add compiling options like this. 12docker build . \\ --build-arg PHP_EXTRA_CONFIGURE_ARGS=&quot;--enable-mailparse&quot; Then you put pecl installation on Dockerfile. 12RUN pecl install mailparse \\ &amp;&amp; docker-php-ext-enable mailparse","link":"/2021/09/02/15000%E5%AE%B9%E5%99%A8%E8%BF%90%E7%BB%B4/Docker%E5%AE%89%E8%A3%85PHP%E6%89%A9%E5%B1%95/"},{"title":"","text":"Inverted File","link":"/2021/08/23/207ElasticSearch/%E6%96%87%E6%A1%A3/"},{"title":"","text":"[toc] 1、什么是 Mybatis？1.Mybatis 是⼀个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发 时只需要关注 SQL 语句本身，不需要 花费精⼒去处理加载驱动、创建连接、 创 建 statement 等繁杂的过程。程序员直 接编写原⽣态 sql，可以严格控制 sql执⾏性能，灵活度⾼。2.MyBatis 可以使⽤ XML 或注解来配 置和映射原⽣信息，将 POJO 映射成数据库中的记 录 ，避免了⼏乎所有的JDBC代码和⼿动设置参数以及获取结果集。3.通过 xml ⽂件或注解的⽅式将要执⾏的各种 statement 配置起来，并通过 java 对象和 statement 中 sql 的动态参数进⾏映射⽣成最终执⾏的 sql 语句，最后由 mybatis 框架执⾏ sql 并将结果映射为 java 对象并返回。（从执⾏ sql 到返回result 的过程）。 2.MyBatis的优点1.基于 SQL 语句编程，相当灵活，不会对应⽤程序或者数据库的现有设计造成任何影响，SQL 写在 XML ⾥，解除 sql 与程序代码的耦合，便于统⼀管理；提供 XML 标签，⽀持编写动态 SQL 语 句，并可重⽤。2.与 JDBC 相⽐，减少了 50%以上的 代码量，消除了 JDBC ⼤量冗余的代 码， 不需要⼿动开关连接；3.很 好 的 与 各 种 数 据 库 兼 容 （ 因 为 MyBatis 使⽤ JDBC 来连接数据库，所 以 只要 JDBC ⽀持的数据库 MyBatis 都⽀持）。4.能够与 Spring 很好的集成；5.提供映射标签，⽀持对象与数据库的 ORM 字段关系映射；提供对象关系映 射标签，⽀持对象关系组件维护。 3.MyBatis 框架的缺点：1.SQL 语句的编写⼯作量较⼤，尤其当 字段多、关联表多时，对开发⼈员编写 SQL 语句的功底有⼀定要求。2.SQL 语句依赖于数据库，导致数据库 移植性差，不能随意更换数据库。 4、MyBatis 框架适⽤场合：1.MyBatis 专注于 SQL 本身，是⼀个⾜够灵活的 DAO 层解决⽅案。2.对性能的要求很⾼，或者需求变化较 多的项⽬，如互联⽹项⽬，MyBatis 将 是不错的选择。 5、MyBatis 与 Hibernate 有哪些不同？ 1.Mybatis 和 hibernate 不同，它不完全是 2.MYBATIS之MAPPER接口的注册过程Mapper接口用于定义执行SQL语句相关的方法，方法名一般和Mapper XML配置文件中&lt;select|update|delete|insert&gt;标签的id属性相同，接口的完全限定名一般对应Mapper XML配置文件的命名空间。 可以看一下Mapper XML，如下面的UserMapper.xml 1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.blog4java.mybatis.example.mapper.UserMapper&quot;&gt; &lt;sql id=&quot;userAllField&quot;&gt; id,create_time, name, password, phone, nick_name &lt;/sql&gt; &lt;select id=&quot;listAllUser&quot; resultType=&quot;com.blog4java.mybatis.example.entity.UserEntity&quot; &gt; select &lt;include refid=&quot;userAllField&quot;/&gt; from user &lt;/select&gt; &lt;select id=&quot;getUserByEntity&quot; resultType=&quot;com.blog4java.mybatis.example.entity.UserEntity&quot;&gt; select &lt;include refid=&quot;userAllField&quot;/&gt; from user &lt;where&gt; &lt;if test=&quot;id != null&quot;&gt; AND id = #{id} &lt;/if&gt; &lt;if test=&quot;name != null&quot;&gt; AND name = #{name} &lt;/if&gt; &lt;if test=&quot;phone != null&quot;&gt; AND phone = #{phone} &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; &lt;select id=&quot;getUserByPhone&quot; resultType=&quot;com.blog4java.mybatis.example.entity.UserEntity&quot;&gt; select &lt;include refid=&quot;userAllField&quot;/&gt; from user where phone = ${phone} &lt;/select&gt;&lt;/mapper&gt; UserMapper的代码 123456789public interface UserMapper { List&lt;UserEntity&gt; listAllUser(); List&lt;UserEntity&gt; getUserByEntity( UserEntity user); UserEntity getUserByPhone(@Param(&quot;phone&quot;) String phone);} 看一下如何执行这个UserMapper.xml，参考下面的代码 123456789101112// 获取配置文件输入流InputStream inputStream = Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;);// 通过SqlSessionFactoryBuilder的build()方法创建SqlSessionFactory实例SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);// 调用openSession()方法创建SqlSession实例SqlSession sqlSession = sqlSessionFactory.openSession();// 获取UserMapper代理对象UserMapper userMapper = sqlSession.getMapper(UserMapper.class);// 执行Mapper方法，获取执行结果List&lt;UserEntity&gt; userList = userMapper.listAllUser();System.out.println(JSON.toJSONString(userList)); 如上面的代码所示，在创建SqlSession实例后，需要调用SqlSession的getMapper()方法获取一个UserMapper的引用，然后通过该引用调用Mapper接口中定义的方法，UserMapper是一个接口，我们调用SqlSession对象getMapper()返回的到底是什么呢？ 我们知道，接口中定义的方法必须通过某个类实现该接口，然后创建该类的实例，才能通过实例调用方法。所以SqlSession对象的getMapper()方法返回的一定是某个类的实例。具体是哪个类的实例呢？实际上getMapper()方法返回的是一个动态代理对象。 我们一步步解析 DefaultSqlSession实现了SqlSession接口，可以直接看一下DefaultSqlSession对getMapper()的具体实现 123456789101112// SqlSession对象getMapper()具体的实现@Overridepublic &lt;T&gt; T getMapper(Class&lt;T&gt; type) { return configuration.&lt;T&gt;getMapper(type, this); // this:sqlSession}// 通过调用configuration类的getMapper获取UserMapper的实列，继续看看getMapper的实现public class Configuration { // configuration类的getMapper的实现 public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) { return mapperRegistry.getMapper(type, sqlSession); }} configuration类的getMapper的是通过调用mapperRegistry的getMapper方法来实现的 这里的mapperRegistry用于注册Mapper接口信息，建立Mapper接口的Class对象和MapperProxyFactory对象之间的关系，其中MapperProxyFactory对象用于创建Mapper动态代理对象 继续下一步 根据Mapper接口Class对象获取Mapper动态代理对象 Mapper动态代理对象是通过MapperProxyFactory创建的。 重点来了，MapperProxyFactory如何通过动态代理来创建Mapper对象的 MapperProxy使用的是JDK内置的动态代理 MapperProxy使用的是JDK内置的动态代理，实现了InvocationHandler接口，invoke()方法中为通用的拦截逻辑，具体内容在介绍Mapper方法调用过程时再做介绍。使用JDK内置动态代理，通过MapperProxy类实现InvocationHandler接口，定义方法执行拦截逻辑后，还需要调用java.lang.reflect.Proxy类的newProxyInstance()方法创建代理对象。 MyBatis对这一过程做了封装，使用MapperProxyFactory创建Mapper动态代理对象。 MapperProxyFactory类对jdk动态代理做了进一步的封装 如上面的代码所示，MapperProxyFactory类的工厂方法newInstance()是非静态的。也就是说，使用MapperProxyFactory创建Mapper动态代理对象首先需要创建MapperProxyFactory实例。MapperProxyFactory实例是什么时候创建的呢？ Configuration对象中有一个mapperRegistry属性，创建Configuration对象过程中，具体看这篇文章，解析标签的时候会调用mapperRegistry中的addMapper(Class type)，创建MapperProxyFactory实例。 MyBatis通过mapperRegistry属性注册Mapper接口与MapperProxyFactory对象之间的对应关系。下面是MapperRegistry类的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class MapperRegistry { // Configuration对象引用 private final Configuration config; // 用于注册Mapper接口Class对象，和MapperProxyFactory对象对应关系 private final Map&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt; knownMappers = new HashMap&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt;(); public MapperRegistry(Configuration config) { this.config = config; } // 根据Mapper接口Class对象获取Mapper动态代理对象 @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) { final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); if (mapperProxyFactory == null) { throw new BindingException(&quot;Type &quot; + type + &quot; is not known to the MapperRegistry.&quot;); } try { return mapperProxyFactory.newInstance(sqlSession); } catch (Exception e) { throw new BindingException(&quot;Error getting mapper instance. Cause: &quot; + e, e); } } public &lt;T&gt; boolean hasMapper(Class&lt;T&gt; type) { return knownMappers.containsKey(type); } // 根据Mapper接口Class对象，创建MapperProxyFactory对象，并注册到knownMappers属性中 public &lt;T&gt; void addMapper(Class&lt;T&gt; type) { if (type.isInterface()) { if (hasMapper(type)) { throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); } boolean loadCompleted = false; try { knownMappers.put(type, new MapperProxyFactory&lt;T&gt;(type)); // It's important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won't try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; } finally { if (!loadCompleted) { knownMappers.remove(type); } } } } /** * @since 3.2.2 */ public Collection&lt;Class&lt;?&gt;&gt; getMappers() { return Collections.unmodifiableCollection(knownMappers.keySet()); } /** * @since 3.2.2 */ public void addMappers(String packageName, Class&lt;?&gt; superType) { ResolverUtil&lt;Class&lt;?&gt;&gt; resolverUtil = new ResolverUtil&lt;Class&lt;?&gt;&gt;(); resolverUtil.find(new ResolverUtil.IsA(superType), packageName); Set&lt;Class&lt;? extends Class&lt;?&gt;&gt;&gt; mapperSet = resolverUtil.getClasses(); for (Class&lt;?&gt; mapperClass : mapperSet) { addMapper(mapperClass); } } /** * @since 3.2.2 */ public void addMappers(String packageName) { addMappers(packageName, Object.class); } } 如上面的代码所示，MapperRegistry类有一个knownMappers属性，用于注册Mapper接口对应的Class对象和MapperProxyFactory对象之间的关系。另外，MapperRegistry提供了addMapper()方法，用于向knownMappers属性中注册Mapper接口信息。在addMapper()方法中，为每个Mapper接口对应的Class对象创建一个MapperProxyFactory对象，然后添加到knownMappers属性中。 MapperRegistry还提供了getMapper()方法，能够根据Mapper接口的Class对象获取对应的MapperProxyFactory对象，然后就可以使用MapperProxyFactory对象创建Mapper动态代理对象了。 MyBatis框架在应用启动时会解析所有的Mapper接口，然后调用MapperRegistry对象的addMapper()方法将Mapper接口信息和对应的MapperProxyFactory对象注册到MapperRegistry对象中。 3.从源码层面解释：为什么执行MYBATIS接口就可以执行SQL？1:场景分析在我们使用SpringBoot+MyBatis的时候，我们一般是先引入依赖，然后配置 1234mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: com.coco.pojo复制代码 当然还要在启动类上加上一个注解 这时候，就可以编写一个接口，然后调用这个方法就可以执行配置文件中对应的SQL语句了 那么底层原理到底是怎么实现的呢？？ 2:万事开头难分析一个框架源码的时候最难的就是不知道该从哪开始，我是这样想的，既然我们只要写一个这样的接口，那么就可以调用对应的SQL语句，那么肯定是在哪个环节对这个接口做一些特殊的处理 我们在启动类上加了一个注解，而且注解中的包路径正是我们接口的路径，这时候我们就有点眉目了。 进入 @MapperScan(“com.coco.mapper”) 这个注解中 我们看到除了注解的基本三个注解之外，还有一个注解就是 @Import({ MapperScannerRegistrar.class}) ，很多小伙伴可能不知道这个注解有什么用，我们先解释一下 3: SPRINGBOOT中@IMPORT注解的作用在SpringBoot中当我们要声明一个Bean的时候，我们可以在该类上加上 @Service，@Compont 等，或者是在配置类中加上 @Bean 这个注解，除此之外还有一种方法，就是 @Import @Import注解中会标明一个类，而且在SpringBoot启动的时候会处理也就是会实例化这个Bean，也就是会对这个Bean做一些处理 4: MAPPERSCANNERREGISTRAR.CLASS的作用即然知道了 @Import 注解的作用，那现在我们进入到这个类中看看，这个类实现了** ImportBeanDefinitionRegistrar** 这个接口 这个接口有什么用呢？？简单的来说就是MyBatis通过这个入口可以让Spring扫描到某些Bean，并且这些Bean会被Spring所管理，也就是说这些Bean会被Spring进行初始化。 所以我们自定义的Mapper接口会被Spring扫描到，然后会被Spring进行加载 ImportBeanDefinitionRegistrar这个接口就代表着当把Bean生成了对应的 BeanDefinition 的时候，就会调用这个接口的方法，我们看下这个接口中定义的方法 这个方法做什么的呢？？ Spring在加载Bean的时候，首先会将Bean生成一个个的对应的 BeanDefinition ，后续就会通过这些一个个的 BeanDefinition 来进行初始化，也就是生成对应的Bean。 简而言之：Spring会通过MyBatis提供的 @MapperScan(“com.coco.mapper”) 这个注解会扫描我们自定义的Mapper接口，然后Spring就会为这些Mapper接口生成对应的 BeanDefinition 5: DEBUG模式进入源码 然后debug模式启动SpringBoot项目，当然前提是整合了MyBatis哈，这个方法我进行了截取，其实只需要关注下面这几行代码就行 12345678910111213141516171819@Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { //获取到MapperScan注解 AnnotationAttributes annoAttrs = AnnotationAttributes.fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName())); ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); // 获取MapperScan注解中basePackages的属性值 for (String pkg : annoAttrs.getStringArray(&quot;basePackages&quot;)) { if (StringUtils.hasText(pkg)) { basePackages.add(pkg); } } // 真正开始处理这个包路径下的接口，也就是我们的Mapper接口 scanner.doScan(StringUtils.toStringArray(basePackages)); } 复制代码 这里就可以获取到我们自定义mapper接口的包的全路径了 6: 开始处理MAPPER接口我们进入到上面的 scanner.doScan(StringUtils.toStringArray(basePackages)); 这个方法 然后进入 Set beanDefinitions = super.doScan(basePackages); 发现好多代码，其实这个方法的返回值是一个 BeanDefinitionHolder 的集合，而 BeanDefinitionHolder 就是bean的名称和该bean的BeanDefinition的组成 其实到这里我们应该能明白，这个方法的作用就是：扫描我们自定义的Mapper接口，然后为每一个接口生成一个对应的BeanDefinition，然后将其返回 123456789101112131415161718192021222324252627protected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) { Assert.notEmpty(basePackages, &quot;At least one base package must be specified&quot;); Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;(); for (String basePackage : basePackages) { Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage); for (BeanDefinition candidate : candidates) { ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate); candidate.setScope(scopeMetadata.getScopeName()); String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry); if (candidate instanceof AbstractBeanDefinition) { postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName); } if (candidate instanceof AnnotatedBeanDefinition) { AnnotationConfigUtils.processCommonDefinitionAnnotations((AnnotatedBeanDefinition) candidate); } if (checkCandidate(beanName, candidate)) { BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); beanDefinitions.add(definitionHolder); registerBeanDefinition(definitionHolder, this.registry); } } } return beanDefinitions; }复制代码 我们debug到这一步可以看到返回值，也证实了我们之前说的 7: 拿到BEANDEFINITION之后的处理现在我们来看下 processBeanDefinitions(beanDefinitions); 这个方法，因为之前我们已经拿到了Mapper接口的BeanDefinition了，所以接下来就要进一步的处理 这个方法的代码依旧很多，我这里就不贴出来了，这里我先说一下这个方法是干什么的。 Spring在初始化Bean之前，我们是可以改变Bean的BeanDefinition的属性值得，而这个方法做的事情就是这个，经过这个方法处理之后，我们之前得到的BeanDefiniton会发生一些改变。我这里贴出二张图进行对比一下 这是之前的： 这是经过该方法处理之后的： 可以发现该Bean的 beanClass 属性变了，已经不再是我们自定义的Bean的class了 改变之后有什么问题呢？？ Spring在初始化Bean的时候，会拿到该Bean的BenDefinition，然后就是根据 beanClass 这个属性值初始化Bean，本来我们Mapper接口初始化之后应该就是我们自己定义的Bean，也就是我们执行a.getClass的值应该是 com.xxx.a 这种形式的 但是现在变了，也就是说我们自定义的Mapper接口在被Spring初始化之后，再执行a.getClass会变成** org.mybatis.spring.mapper.MapperFactoryBean** 8: 初始化BEAN经过上面的步骤之后，我们是拿到了Mapper接口的BeanDefinition，现在Spring就要开始初始化这些Bean了 因为此时就涉及到了Spring的源码了，我这里就不细说了 大致的流程： 1: Spring在初始化bean的时候，会根据Bean的scope属性进行初始化，而我们自定义的Mapper接口由于BeanDefinition的beanClass属性被修改了，所以在初始化的时候，经过一系列的判断最终会由MyBatis中的 MapperProxy 生成一个代理类，底层是通过jdk动态代理实现的 2: 然后当我们调用Mapper接口方法的时候就会执行 invoke 方法，因为是jdk动态代理生成的代理类。 3: 这时候，MyBatis是可以拿到该方法所在的类和该类的全路径的，比如我们在 com.coco.mapper 包下自定义了一个 TestMapper 接口，然后里面有一个 test() 方法，这时候我们可以通过一系列的方法得到一个值，该值就是:** com.coco.mapper.TestMapper.test** , 也就是该Mapper接口的全路径+方法名 4: MyBatis在解析xml配置文件的时候，有一个 namespace 的属性，它的值就是Mapper接口的全路径名，然后加上 id 的值，MyBatis底层会将所有的这种路径全都保存在一个Map中，然后执行接口方法的时候就会根据第3步生成的值去匹配，就能拿到对应的SQL语句了 12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.coco.mapper.TestMapper&quot;&gt; &lt;select id=&quot;test&quot;&gt; select * from test &lt;/select&gt; &lt;/mapper&gt;复制代码","link":"/2022/02/07/208MyBatis/2.8.1.%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%97%AE%E9%A2%98/"},{"title":"","text":"[toc] Mybatis 接口 Mapper 内的方法为啥不能重载吗？动态代理的功能：通过拦截器方法回调，对目标target方法进行增强。 言外之意就是为了增强目标target方法。上面这句话没错，但也不要认为它就是真理，殊不知，动态代理还有投鞭断流的霸权，连目标target都不要的科幻模式。 注：本文默认认为，读者对动态代理的原理是理解的，如果不明白target的含义，难以看懂本篇文章，建议先理解动态代理。 1. 自定义JDK动态代理之投鞭断流实现自动映射器Mapper首先定义一个pojo。 1234567891011public class User { private Integer id; private String name; private int age; public User(Integer id, String name, int age) { this.id = id; this.name = name; this.age = age; }} 再定义一个接口UserMapper.java。 123public interface UserMapper { public User getUserById(Integer id); } 接下来我们看看如何使用动态代理之投鞭断流，实现实例化接口并调用接口方法返回数据的。 自定义一个InvocationHandler。 123456789101112131415161718192021222324import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class MapperProxy implements InvocationHandler { @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T newInstance(Class&lt;T&gt; clz) { return (T) Proxy.newProxyInstance(clz.getClassLoader(), new Class[] { clz }, this); } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (Object.class.equals(method.getDeclaringClass())) { try { // 诸如hashCode()、toString()、equals()等方法，将target指向当前对象this return method.invoke(this, args); } catch (Throwable t) { } } // 投鞭断流 return new User((Integer) args[0], &quot;zhangsan&quot;, 18); }} 上面代码中的target，在执行Object.java内的方法时，target被指向了this，target已经变成了傀儡、象征、占位符。在投鞭断流式的拦截时，已经没有了target。 写一个测试代码： 1234567891011121314151617public static void main(String[] args) { MapperProxy proxy = new MapperProxy(); UserMapper mapper = proxy.newInstance(UserMapper.class); User user = mapper.getUserById(1001); System.out.println(&quot;ID:&quot; + user.getId()); System.out.println(&quot;Name:&quot; + user.getName()); System.out.println(&quot;Age:&quot; + user.getAge()); System.out.println(mapper.toString());}============================ID:1001Name:zhangsanAge:18x.y.MapperProxy@6bc7c054 这便是Mybatis自动映射器Mapper的底层实现原理。 可能有读者不禁要问：你怎么把代码写的像初学者写的一样？没有结构，且缺乏美感。 必须声明，作为一名经验老道的高手，能把程序写的像初学者写的一样，那必定是高手中的高手。这样可以让初学者感觉到亲切，舒服，符合自己的Style，让他们或她们，感觉到大牛写的代码也不过如此，自己甚至写的比这些大牛写的还要好，从此自信满满，热情高涨，认为与大牛之间的差距，仅剩下三分钟。 2. Mybatis自动映射器Mapper的源码分析首先编写一个测试类： 123456789101112public static void main(String[] args) { SqlSession sqlSession = MybatisSqlSessionFactory.openSession(); try { StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); List&lt;Student&gt; students = studentMapper.findAllStudents(); for (Student student : students) { System.out.println(student); } } finally { sqlSession.close(); } } Mapper长这个样子： 12345public interface StudentMapper { List&lt;Student&gt; findAllStudents(); Student findStudentById(Integer id); void insertStudent(Student student);} org.apache.ibatis.binding.MapperProxy.java部分源码。 123456789101112131415161718192021222324252627public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable { private static final long serialVersionUID = -6424540398559729838L; private final SqlSession sqlSession; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache; public MapperProxy(SqlSession sqlSession, Class&lt;T&gt; mapperInterface, Map&lt;Method, MapperMethod&gt; methodCache) { this.sqlSession = sqlSession; this.mapperInterface = mapperInterface; this.methodCache = methodCache; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (Object.class.equals(method.getDeclaringClass())) { try { return method.invoke(this, args); } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } } // 投鞭断流 final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); } // ... org.apache.ibatis.binding.MapperProxyFactory.java部分源码。 12345678public class MapperProxyFactory&lt;T&gt; { private final Class&lt;T&gt; mapperInterface; @SuppressWarnings(&quot;unchecked&quot;) protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) { return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy);} 这便是Mybatis使用动态代理之投鞭断流 3. 接口Mapper内的方法能重载（overLoad）吗？（重要类似下面： 12public User getUserById(Integer id);public User getUserById(Integer id, String name); Answer：不能。 原因：在投鞭断流时，Mybatis使用package+Mapper+method全限名作为key，去xml内寻找唯一sql来执行的。类似：key=x.y.UserMapper.getUserById，那么，重载方法时将导致矛盾。对于Mapper接口，Mybatis禁止方法重载（overLoad）。","link":"/2021/06/13/208MyBatis/2.8.2.Mapper%E6%96%B9%E6%B3%95%E4%B8%8D%E8%83%BD%E9%87%8D%E8%BD%BD/"},{"title":"","text":"配置 appid wxf50efe73475465fb secret 9e05b081d22377ad7152f40ba23a3dee Token lilaiqun EncodingAESKey jjCJ8xOsNBad23m4N0l4weST0cOR4UQevNkWjn2mfG2 第三方sdk https://www.easywechat.com/ 12345# thinkphp5.1composer create-project topthink/think=5.1.* tp5 # easywechat.comcomposer require overtrue/wechat:~4.0 -vvv 消息格式 12345678&lt;xml&gt; &lt;ToUserName&gt;&lt;![CDATA[toUser]]&gt;&lt;/ToUserName&gt; &lt;FromUserName&gt;&lt;![CDATA[fromUser]]&gt;&lt;/FromUserName&gt; &lt;CreateTime&gt;1348831860&lt;/CreateTime&gt; &lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt; &lt;Content&gt;&lt;![CDATA[this is a test]]&gt;&lt;/Content&gt; &lt;MsgId&gt;1234567890123456&lt;/MsgId&gt;&lt;/xml&gt; 参数 描述 ToUserName 开发者微信号 FromUserName 发送方帐号（一个OpenID） CreateTime 消息创建时间 （整型） MsgType 消息类型，文本为text Content 文本消息内容 MsgId 消息id，64位整型","link":"/2021/04/14/211%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/note/"},{"title":"","text":"[TOC] PECL - php扩展安装apcu安装12345$ sudo apt-get install php7.x-dev$ pecl channel-update pecl.php.net$ pecl install apcu$ php -i // 查看加载的php.ini位置 Composer 安装安装指定版本123456789101112$ php -r &quot;copy('https://getcomposer.org/installer', 'composer-setup.php');&quot;$ php -r &quot;if (hash_file('sha384', 'composer-setup.php') === '756890a4488ce9024fc62c56153228907f1545c228516cbf63f885e036d37e9a59d27d63f46af1d4d07ee0f76181c7d3') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;&quot;$ php composer-setup.php --version=1.7.1All settings correct for using ComposerDownloading...Composer (version 1.7.1) successfully installed to: //composer.pharUse it: php composer.phar $ php -r &quot;unlink('composer-setup.php');&quot;$ mv composer.phar /usr/bin/composer$ apt-get install php-pear composer 中版本号使用~和^的意思很接近，在x.y的情况下是一样的都是代表x.y &lt;= 版本号 &lt; (x+1).0，但在版本号是x.y.z的情况下有区别，举个例子： ~1.2.3 代表 1.2.3 &lt;= 版本号 &lt; 1.3.0 ^1.2.3 代表 1.2.3 &lt;= 版本号 &lt; 2.0.0 执行ssh-add时出现Could not open a connection to your authentication agent先执行 eval ssh-agent （是～键上的那个`） 再执行 ssh-add ~/.ssh/rsa成功ssh-add -l 就有新加的rsa了","link":"/2021/03/25/212PHP/all/"},{"title":"","text":"通过npm安装Vue CLI 123456npm install -g @vue/clivue create electron-vue&gt; Vue CLI v4.5.15&gt; Vue 3.0 Installing Electron into your Vue ApplicationAfter creating our Vue project, let’s now install Electron in our Vue project using the following commands: 1$ npm install --save-dev electron@latest By using the --save-dev switch, Electron will be installed as a development dependency in your project. &quot;electron&quot;: &quot;^16.0.4&quot; Bootstrapping the Electron AppAfter installing Electron, you need to add some code to bootstrap your Electron app and create a GUI window where the Vue app will be opened. Go ahead and create a main.js file inside your Vue project and add the following code: https://ee.58corp.com/detail/iwork/v4/issue/detail/USER-124092 16hhttps://ee.58corp.com/detail/iwork/v4/issue/detail/USER-127197 8hhttps://ee.58corp.com/detail/iwork/v4/issue/detail/USER-127198 8h","link":"/2021/12/09/217Node/Electron-Vue/"},{"title":"","text":"平滑增加字段","link":"/2021/05/15/22000Mysql%E4%BC%98%E5%8C%96/22001%20Mysql%E5%A2%9E%E5%8A%A0%E5%AD%97%E6%AE%B5/"},{"title":"","text":"2.2.2 Mysql索引从数据结构角度 1、B+树索引(O(log(n)))：关于B+树索引 2、hash索引：a 仅仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询b 其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引c 只有Memory存储引擎显示支持hash索引 3、FULLTEXT索引（现在MyISAM和InnoDB引擎都支持了） 4、R-Tree索引（用于对GIS数据类型创建SPATIAL索引） 从物理存储角度 1、聚集索引（clustered index） 2、非聚集索引（non-clustered index） 从逻辑角度 1、主键索引：主键索引是一种特殊的唯一索引，不允许有空值 2、普通索引或者单列索引 3、复合索引：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合 4、唯一索引或者非唯一索引 5、空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。MYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MYISAM的表中创建 12CREATE TABLE table_name[col_name data type][unique|fulltext|spatial][index|key][index_name](col_name[length])[asc|desc] 1、unique|fulltext|spatial为可选参数，分别表示唯一索引、全文索引和空间索引； 2、index和key为同义词，两者作用相同，用来指定创建索引 3、col_name为需要创建索引的字段列，该列必须从数据表中该定义的多个列中选择； 4、index_name指定索引的名称，为可选参数，如果不指定，MYSQL默认col_name为索引值； 5、length为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度； 6、asc或desc指定升序或降序的索引值存储","link":"/2021/05/15/22000Mysql%E4%BC%98%E5%8C%96/22002%20Mysql%E7%B4%A2%E5%BC%95/"},{"title":"","text":"数据库优化[TOC] 一.索引优化 「覆盖索引」，减少「回表」所消耗的时间。意味着，我们在 select 的时候，一定要指明对应的列，而不是 select *； 「联合索引」。如果组建「联合索引」，尽量将区分度最高的放在最左边，并且需要考虑「最左匹配原则」； 索引列不参与计算，索引失效：3.1 对索引进行函数操作或者表达式计算会导致索引失效；3.2 不符合最左匹配原则3.3 字段进行了隐形数据类型转换3.4 走索引没有走全表扫描效率高 子查询优化超多分页场景。比如 limit offset , n 在 MySQL 是获取 offset + n 的记录，再返回 n 条。而利用子查询则是查出 n 条，通过 ID 检索对应的记录出来，提高查询效率； 通过 explain 命令来查看 SQL 的执行计划，看看自己写的 SQL 是否走了索引，走了什么索引。通过 show profile 来查看 SQL 对系统资源的损耗情况（不过一般还是比较少用到的）； 在开启事务后，在事务内尽可能只操作数据库，并有意识地减少锁的持有时间。比如在事务内需要插入并修改数据，那可以先插入后修改。因为修改是更新操作，会加行锁。如果先更新，那并发下可能会导致多个事务的请求等待行锁释放。 即便走对了索引，线上查询还是慢？表的数据量实在是太大了。1.考虑能不能把「旧的数据」给”删掉”。对于我们公司而言，我们都会把数据同步到 Hive。2.走一层缓存（Redis）要看业务能不能忍受读取的「非真正实时」的数据（毕竟 Redis 和 MySQL 的数据一致性需要保证）。如果查询条件相对复杂且多变的话（涉及各种group by 和 sum），那走缓存也不是一种好的办法，维护起来就不方便了…… 再看看是不是有「字符串」检索的场景导致查询低效，如果是的话，可以考虑把表的数据导入至 Elasticsearch 类的搜索引擎，后续的线上查询就直接走 Elasticsearch 了。MySQL-&gt;Elasticsearch 需要有对应的同步程序（一般就是监听 MySQL 的 binlog，解析 binlog 后导入到 Elasticsearch)。 如果还不是的话，那考虑要不要根据查询条件的维度，做相对应的聚合表。线上的请求就查询聚合表的数据，不走原表。比如，用户下单后有一份订单明细，而订单明细表的量级太大。但在产品侧（前台）透出的查询功能是以「天」维度来展示的，那就可以将每个用户的每天数据聚合起来，在聚合表就是一个用户一天只有一条汇总后的数据。查询走聚合后的表，那速度肯定杠杠的（聚合后的表数据量肯定比原始表要少很多）。思路大致的就是「以空间换时间」，相同的数据换别的地方也存储一份，提高查询效率。 除了读之外，写性能同样有瓶颈，怎么办？在 MySQL 读写都有瓶颈，那首先看下目前 MySQL 的架构是怎么样的。 1.如果是单库的，那是不是可以考虑升级至主从架构，实现读写分离。简单理解就是主库接收写请求，从库接收读请求。从库的数据由主库发送的 binlog 进而更新，实现主从数据一致（在一般场景下，主从的数据是通过异步来保证最终一致性的）。 2.主从架构下，读写仍存在瓶颈，那就要考虑是否要分库分表了。用户维度分表 分库分表后的 ID 是怎么生成的？ MySQL 自增的 2.借助 Redis 自增的 3.基于「雪花算法」自增的 数据迁移采取「双写」的方式来进行迁移 增量的消息各自往新表和旧表写一份； 将旧表的数据迁移至新库； 迟早新表的数据都会追得上旧表（在某个节点上数据是同步的）； 校验新表和老表的数据是否正常（主要看能不能对得上）； 开启双读（一部分流量走新表，一部分流量走老表），相当于灰度上线的过程； 读流量全部切新表，停止老表的写入； 提前准备回滚机制，临时切换失败能恢复正常业务以及有修数据的相关程序。 1.Explain字段 字段 含义 示例 id select_type table partitions type 访问方法 const、ref、ref_or_null、index Possible_keys Type访问方法 const 索引一次命中，匹配一行数据 system 只有一行 eq_ref ref 非唯一索引，二级索引列与常数等值比较，采用二级索引来执行查询的访问方法 range index 遍历索引树 all 全表 Extra： using filesort 对结果进行外部排序 using index 覆盖索引扫描 using temporary 临时表排序 using where where过滤，效率高 2.ACID是靠什么保证的？原子性A：由undolog日志来保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql 一致性C：由其他三大特性保证，程序保证业务上的一致性 隔离性I：由MVCC保证 持久性D：由redolog来保证，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍不会丢失。","link":"/2022/03/11/22000Mysql%E4%BC%98%E5%8C%96/22000%20%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96/"},{"title":"","text":"MyISAM 和 InnoDB 不支持事务，每次查询都是原子的表级锁，每次操作对全表加锁存储表的总行数 索引文件、表结构文件、数据文件 采用非聚集索引，索引文件的数据存储指向数据文件的指针。辅索引和主索引基本一致，但辅助索引不能保证唯一性。 支持事务，4种隔离级别 行锁，支持并发写不存总行数 索引类型及对数据库的性能的影响 普通、唯一、主键、联合、全文、","link":"/2021/05/31/22000Mysql%E4%BC%98%E5%8C%96/22003MyISAM%E5%92%8CInnodb/"},{"title":"","text":"主从延迟一、常见的主从架构 一主一从 一主多从 多主一从 双主复制 级联复制 二、主从同步原理 binlog（二进制日志文件） relay log（中继日志文件） 主从同步过程 从库生成两个线程，一个I/O线程，一个SQL线程； i/o线程去请求主库的binlog，并将得到的binlog日志写到relay log（中继日志） 文件中； 主库会生成一个 log dump 线程，用来给从库 i/o线程传binlog SQL 线程，会读取relay log文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致； 三、如何判断主从是否延时通过监控 show slave status 命令输出的Seconds_Behind_Master参数的值来判断： NULL，表示io_thread或是sql_thread有任何一个发生故障 0，该值为零，表示主从复制良好 正值，表示主从已经出现延时，数字越大表示从库延迟越严重 四、主从延迟原因随机重放MySQL的主从复制都是单线程的操作，主库对所有DDL和DML产生的日志写进binlog，由于binlog是顺序写，所以效率很高。Slave的SQL Thread线程将主库的DDL和DML操作事件在slave中重放。DML和DDL的IO操作是随机的，不是顺序的，成本高很多。所以SQL Thread线程的速度赶不上主库学binlog的速度，就会产生主从延迟 锁等待另一方面，由于SQL Thread也是单线程的，当主库的并发较高时，产生的DML数量超过slave的SQL Thread所能处理的速度，或者当slave中有大型query语句产生了锁等待那么延时就产生了。 五、主从延迟解决办法并行复制既然 SQL 单线程进行重放时速度有限，那么能不能采用多线程的方式来进行重放呢？MySQL 5.6 版本后，提供了一种并行复制的方式，通过将 SQL 线程转换为多个 work 线程来进行重放，这样就解决了主从延迟的问题 降低并发如果你理解了随机重放这个导致主从延迟的原因，那么就比较好理解了，控制主库写入的速度，主从延迟发生的概率自然就小了。 读主库如果你做的是类似支付这种对实时性要求非常高的业务，那么最直接的方法就是直接读主库。","link":"/2022/02/16/22000Mysql%E4%BC%98%E5%8C%96/22004%E4%B8%BB%E4%BB%8E%E5%BB%B6%E8%BF%9F/"},{"title":"","text":"分布式事务2PC：第一阶段（prepare）：每个参与者执行本地事务不提交，进入ready状态，并通知协调者已经准备就绪 第二阶段（commit）：协调者确认每个参与者都ready后，通知参与者进行commit操作；如果参与者fail，则发送rollback命令，各参与者做回滚。 问题： 单点故障：一旦事务管理器TM出现故障，整个系统不可用 数据不一致：在阶段二，如果事务管理器TM只发送了部分commit消息，此时网络异常，那么只有部分参与者接收到commit消息，使得数据不一致 相应时间长：参与者与协调资源都被锁住，提交或回滚后才能释放 不确定性：TM发送commit之后，此时只有一个参与者收到commit，TM和参与者同时宕机后，重新选举的事务管理器无法确定该条消息是否提交成功。 三阶段协议：解决2PC单点故障的问题，但是性能问题和不一致问题没有根本解决。 阶段一：发送CanCommit消息，确认数据库环境正常 阶段二：发送PreCommit消息，完成sql语句的操作 阶段三： TCC补偿事务：Try、Confirm、Cancel针对每个操作，都要注册一个与其对应的确认和补偿操作 TCC模型对业务的侵入性较强，改造难度大，每个操作都需要try、confirm、cancel三个接口实现 confirm和cancel接口都必须实现幂等性。 消息队列的事务消息： 发送prepare消息到消息中间件 发送成功后，执行本地事务 如果事务执行成功，则commit，消息中间件将消息下发到消费端（commit前，消息不会被消费） 如果事务执行失败，则回滚，消息中间件将这条prepare消息删除 消费端接收到消息进行消费，如果消费失败，则不断重试","link":"/2022/04/29/22000Mysql%E4%BC%98%E5%8C%96/22005%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"","text":"[TOC] 1.网络： 局域网 城域网 广域网 2.计算机网络体系结构​ 各层的关系：每一层建立在低一层的服务上，并为高一层提供服务 应用层 传输层 网络层 链路层 TCP/IP协议族 3.端口号FTP=21,SSH=22,SMTP=25,HTTP服务器=80,FTP=2000,HTTP客户端=2001总共65535个端口号 4.什么是TCPTCP基本特性： Transmission Control Protocol 面向连接 可靠性 RTT和RTO (Round trip time传输来回的时间 RTO重传时间) 数据排序 流量控制 全双工 5.SYN洪泛攻击发送伪造的攻击报文，造成服务端上的半开连接队列被占满，阻止其他用户进行访问。 原理：第一次握手，而服务端的响应（第二次握手）的报文将永远发送不到真实的客户端，客户端在等待第三次握手，服务器在这种半开的连接中消耗了资源。 解决方案： 无效连接监控释放 延缓TCB分配方法 防火墙","link":"/2022/03/10/22000Mysql%E4%BC%98%E5%8C%96/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"","text":"DDD","link":"/2022/02/15/220%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8/DDD/"},{"title":"","text":"[toc] 「DUBBO系列」责任链模式实现原理与源码分析1 文章概述责任链模式将请求发送和接收解耦，让多个接收对象都有机会处理这个请求。这些接收对象串成一条链路并沿着这条链路传递这个请求，直到链路上某个接收对象能够处理它。本文我们介绍责任链模式两种应用场景和四种代码实现方式，最后介绍了DUBBO如何应用责任链构建过滤器链路。 2 应用场景2.1 命中立即中断我们实现一个关键词过滤功能。系统设置三个关键词过滤器，输入内容命中任何一个过滤器规则就返回校验不通过，链路立即中断无需继续进行。 (1) 实现方式一123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface ContentFilter { public boolean filter(String content);}public class AaaContentFilter implements ContentFilter { private final static String KEY_CONTENT = &quot;aaa&quot;; @Override public boolean filter(String content) { boolean isValid = Boolean.FALSE; if (StringUtils.isEmpty(content)) { return isValid; } isValid = !content.contains(KEY_CONTENT); return isValid; }}public class BbbContentFilter implements ContentFilter { private final static String KEY_CONTENT = &quot;bbb&quot;; @Override public boolean filter(String content) { boolean isValid = Boolean.FALSE; if (StringUtils.isEmpty(content)) { return isValid; } isValid = !content.contains(KEY_CONTENT); return isValid; }}public class CccContentFilter implements ContentFilter { private final static String KEY_CONTENT = &quot;ccc&quot;; @Override public boolean filter(String content) { boolean isValid = Boolean.FALSE; if (StringUtils.isEmpty(content)) { return isValid; } isValid = !content.contains(KEY_CONTENT); return isValid; }} 具体过滤器已经编写完成，接下来构造过滤器责任链路 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Servicepublic class FilterHandlerChain { private FilterHandler head = null; private FilterHandler tail = null; @PostConstruct public void init() { FilterHandler aaaHandler = new AaaContentFilter(); FilterHandler bbbHandler = new BbbContentFilter(); addHandler(aaaHandler); addHandler(bbbHandler); } public void addHandler(FilterHandler handler) { if (head == null) { head = tail = handler; head.setSuccessor(tail); return; } /** 设置当前tail继任者 **/ tail.setSuccessor(handler); /** 指针重新指向tail **/ tail = handler; } public boolean filter(String content) { if (null == head) { throw new RuntimeException(&quot;FilterHandlerChain is empty&quot;); } /** head发起调用 **/ return head.filter(content); }}public class Test { public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] { &quot;classpath*:META-INF/chain/spring-core.xml&quot; }); FilterHandlerChain chain = (FilterHandlerChain) context.getBean(&quot;filterHandlerChain&quot;); System.out.println(context); boolean result1 = chain.filter(&quot;ccc&quot;); boolean result2 = chain.filter(&quot;ddd&quot;); System.out.println(&quot;校验结果1=&quot; + result1); System.out.println(&quot;校验结果2=&quot; + result2); }} (2) 实现方式二1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class FilterHandler {/** 下一个节点 **/ protected FilterHandler successor = null; public void setSuccessor(FilterHandler successor) { this.successor = successor; } public final boolean filter(String content) { /** 执行自身方法 **/ boolean isValid = doFilter(content); if (!isValid) { System.out.println(&quot;校验不通过&quot;); return isValid; } /** 执行下一个节点链路 **/ if (successor != null &amp;&amp; this != successor) { isValid = successor.filter(content); } return isValid; } /** 每个节点过滤方法 **/ protected abstract boolean doFilter(String content);}public class AaaContentFilterHandler extends FilterHandler { private final static String KEY_CONTENT = &quot;aaa&quot;; @Override protected boolean doFilter(String content) { boolean isValid = Boolean.FALSE; if (StringUtils.isEmpty(content)) { return isValid; } isValid = !content.contains(KEY_CONTENT); return isValid; }}// 省略其它过滤器代码 具体过滤器已经编写完成，接下来构造过滤器责任链路 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Servicepublic class FilterHandlerChain { private FilterHandler head = null; private FilterHandler tail = null; @PostConstruct public void init() { FilterHandler aaaHandler = new AaaContentFilterHandler(); FilterHandler bbbHandler = new BbbContentFilterHandler(); FilterHandler cccHandler = new CccContentFilterHandler(); addHandler(aaaHandler); addHandler(bbbHandler); addHandler(cccHandler); } public void addHandler(FilterHandler handler) { if (head == null) { head = tail = handler; } /** 设置当前tail继任者 **/ tail.setSuccessor(handler); /** 指针重新指向tail **/ tail = handler; } public boolean filter(String content) { if (null == head) { throw new RuntimeException(&quot;FilterHandlerChain is empty&quot;); } /** head发起调用 **/ return head.filter(content); }}public class Test { public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] { &quot;classpath*:META-INF/chain/spring-core.xml&quot; }); FilterHandlerChain chain = (FilterHandlerChain) context.getBean(&quot;filterHandlerChain&quot;); System.out.println(context); boolean result1 = chain.filter(&quot;ccc&quot;); boolean result2 = chain.filter(&quot;ddd&quot;); System.out.println(&quot;校验结果1=&quot; + result1); System.out.println(&quot;校验结果2=&quot; + result2); }} 2.2 全链路执行我们实现一个考题生成功能。在线考试系统根据不同年级生成不同考题。系统设置三个考题生成器，每个生成器都会执行，根据学生年级决定是否生成考题，无需生成则执行下一个生成器。 (1) 实现方式一12345678910111213141516171819public interface QuestionGenerator { public Question generateQuestion(String gradeInfo);}public class AaaQuestionGenerator implements QuestionGenerator { @Override public Question generateQuestion(String gradeInfo) { if (!gradeInfo.equals(&quot;一年级&quot;)) { return null; } Question question = new Question(); question.setId(&quot;aaa&quot;); question.setScore(10); return question; }}// 省略其它生成器代码 具体生成器已经编写完成，接下来构造生成器责任链路 123456789101112131415161718192021222324252627282930313233343536373839@Servicepublic class QuestionChain { private List&lt;QuestionGenerator&gt; generators = new ArrayList&lt;QuestionGenerator&gt;(); @PostConstruct public void init() { QuestionGenerator aaaQuestionGenerator = new AaaQuestionGenerator(); QuestionGenerator bbbQuestionGenerator = new BbbQuestionGenerator(); QuestionGenerator cccQuestionGenerator = new CccQuestionGenerator(); generators.add(aaaQuestionGenerator); generators.add(bbbQuestionGenerator); generators.add(cccQuestionGenerator); } public List&lt;Question&gt; generate(String gradeInfo) { if (CollectionUtils.isEmpty(generators)) { throw new RuntimeException(&quot;QuestionChain is empty&quot;); } List&lt;Question&gt; questions = new ArrayList&lt;Question&gt;(); for (QuestionGenerator generator : generators) { Question question = generator.generateQuestion(gradeInfo); if (null == question) { continue; } questions.add(question); } return questions; }}public class Test { public static void main(String[] args) { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] { &quot;classpath*:META-INF/chain/spring-core.xml&quot; }); System.out.println(context); QuestionChain chain = (QuestionChain) context.getBean(&quot;questionChain&quot;); List&lt;Question&gt; questions = chain.generate(&quot;一年级&quot;); System.out.println(questions); }} (2) 实现方式二12345678910111213141516171819202122232425262728293031323334353637383940414243444546public abstract class GenerateHandler { /** 下一个节点 **/ protected GenerateHandler successor = null; public void setSuccessor(GenerateHandler successor) { this.successor = successor; } public final List&lt;Question&gt; generate(String gradeInfo) { List&lt;Question&gt; result = new ArrayList&lt;Question&gt;(); /** 执行自身方法 **/ Question question = doGenerate(gradeInfo); if (null != question) { result.add(question); } /** 执行下一个节点链路 **/ if (successor != null &amp;&amp; this != successor) { List&lt;Question&gt; successorQuestions = successor.generate(gradeInfo); if (null != successorQuestions) { result.addAll(successorQuestions); } } return result; } /** 每个节点生成方法 **/ protected abstract Question doGenerate(String gradeInfo);}public class AaaGenerateHandler extends GenerateHandler { @Override protected Question doGenerate(String gradeInfo) { if (!gradeInfo.equals(&quot;一年级&quot;)) { return null; } Question question = new Question(); question.setId(&quot;aaa&quot;); question.setScore(10); return question; }}// 省略其它生成器代码 具体生成器已经编写完成，接下来构造生成器责任链路 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Servicepublic class GenerateChain { private GenerateHandler head = null; private GenerateHandler tail = null; @PostConstruct public void init() { GenerateHandler aaaHandler = new AaaGenerateHandler(); GenerateHandler bbbHandler = new BbbGenerateHandler(); GenerateHandler cccHandler = new CccGenerateHandler(); addHandler(aaaHandler); addHandler(bbbHandler); addHandler(cccHandler); } public void addHandler(GenerateHandler handler) { if (head == null) { head = tail = handler; } /** 设置当前tail继任者 **/ tail.setSuccessor(handler); /** 指针重新指向tail **/ tail = handler; } public List&lt;Question&gt; generate(String gradeInfo) { if (null == head) { throw new RuntimeException(&quot;GenerateChain is empty&quot;); } /** head发起调用 **/ return head.generate(gradeInfo); }}public class Test { public static void main(String[] args) { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] { &quot;classpath*:META-INF/chain/spring-core.xml&quot; }); GenerateChain chain = (GenerateChain) context.getBean(&quot;generateChain&quot;); System.out.println(context); List&lt;Question&gt; result = chain.generate(&quot;一年级&quot;); System.out.println(result); }} 3 DUBBO构建过滤器链生产者和消费者最终执行对象都是过滤器链路最后一个节点，整个链路包含多个过滤器进行业务处理。我们看看生产者和消费者默认过滤器链路。 123456# 生产者过滤器链路EchoFilter &gt; ClassloaderFilter &gt; GenericFilter &gt; ContextFilter &gt; TraceFilter &gt; TimeoutFilter &gt; MonitorFilter &gt; ExceptionFilter &gt; AbstractProxyInvoker# 消费者过滤器链路ConsumerContextFilter &gt; FutureFilter &gt; MonitorFilter &gt; DubboInvoker ProtocolFilterWrapper作为链路生成核心通过匿名类方式构建过滤器链路，我们以消费者构建过滤器链路为例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class ProtocolFilterWrapper implements Protocol { private static &lt;T&gt; Invoker&lt;T&gt; buildInvokerChain(final Invoker&lt;T&gt; invoker, String key, String group) { // invoker = DubboInvoker Invoker&lt;T&gt; last = invoker; // 查询符合条件过滤器列表 List&lt;Filter&gt; filters = ExtensionLoader.getExtensionLoader(Filter.class).getActivateExtension(invoker.getUrl(), key, group); if (!filters.isEmpty()) { for (int i = filters.size() - 1; i &gt;= 0; i--) { final Filter filter = filters.get(i); final Invoker&lt;T&gt; next = last; // 构造一个简化Invoker last = new Invoker&lt;T&gt;() { @Override public Class&lt;T&gt; getInterface() { return invoker.getInterface(); } @Override public URL getUrl() { return invoker.getUrl(); } @Override public boolean isAvailable() { return invoker.isAvailable(); } @Override public Result invoke(Invocation invocation) throws RpcException { // 构造过滤器链路 Result result = filter.invoke(next, invocation); if (result instanceof AsyncRpcResult) { AsyncRpcResult asyncResult = (AsyncRpcResult) result; asyncResult.thenApplyWithContext(r -&gt; filter.onResponse(r, invoker, invocation)); return asyncResult; } else { return filter.onResponse(result, invoker, invocation); } } @Override public void destroy() { invoker.destroy(); } @Override public String toString() { return invoker.toString(); } }; } } return last; } @Override public &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException { // RegistryProtocol不构造过滤器链路 if (Constants.REGISTRY_PROTOCOL.equals(url.getProtocol())) { return protocol.refer(type, url); } Invoker&lt;T&gt; invoker = protocol.refer(type, url); return buildInvokerChain(invoker, Constants.REFERENCE_FILTER_KEY, Constants.CONSUMER); }} 4 文章总结面向对象设计有一个重要原则：对扩展开放对修改关闭，我认为这是最重要的面向对象设计原则，责任链模式非常好得体现了这个原则，有助于代码维护和处理复杂场景。","link":"/2021/07/02/24000Dubbo/2.14.1%E8%B4%A3%E4%BB%BB%E9%93%BE/"},{"title":"","text":"服务发现服务发现 RPC 通信协议描述 Dubbo3 支持的通信协议 服务流量管理通过 Dubbo 定义的路由规则，实现对流量分布的控制 配置管理描述 Dubbo 支持的配置，Dubbo 的动态配置能力。 部署架构（注册中心 配置中心 元数据中心）了解 Dubbo 的三大中心化组件，它们各自的职责、工作方式。 如何扩展 DubboDubbo 通过 SPI 机制提供了非常灵活的可扩展性 12345ServiceBean extends ServiceConfig 1.服务端：将ref/Interface/","link":"/2022/03/19/24000Dubbo/24001/"},{"title":"","text":"Dubbo的服务请求失败怎么处理？默认重试机制-重试2次 容错策略：1.快速失败 - 适合非密等，直接抛出错误2.失败安全策略 - 把异常 3.失败重试机制 - 4.并行调用多个 - 有一个返回成功则成功 5.广播 - 有一个失败则失败","link":"/2022/03/28/24000Dubbo/24002%E5%A4%B1%E8%B4%A5%E5%AE%B9%E9%94%99/"},{"title":"","text":"问题一：RabbitMQ 中的 broker 是指什么？cluster 又是指什么？答：broker 是指一个或多个 erlang node 的逻辑分组，且 node 上运行着 RabbitMQ 应用程序。cluster 是在 broker 的基础之上，增加了 node 之间共享元数据的约束。 问题二：什么是元数据？元数据分为哪些类型？包括哪些内容？与 cluster 相关的元数据有哪些？元数据是如何保存的？元数据在 cluster 中是如何分布的？答：在非 cluster 模式下，元数据主要分为 Queue 元数据（queue 名字和属性等）、Exchange 元数据（exchange 名字、类型和属性等）、Binding 元数据（存放路由关系的查找表）、Vhost 元数据（vhost 范围内针对前三者的名字空间约束 和安全属性设置）。在cluster 模式下，还包括 cluster 中 node 位置信息和 node 关系信息。元数据按照 erlang node 的类型确定是仅保存于 RAM 中，还是同时保存在 RAM 和 disk 上。元数据在cluster 中是全 node 分布的。 下图所示为 queue 的元数据在单 node 和 cluster 两种模式下的分布图。 问题三：RAM node 和 disk node 的区别？答：RAM node 仅将 fabric（即 queue、exchange 和 binding 等 RabbitMQ 基础构件）相关元数据保存到内存中，但 disk node 会在内存和磁盘中均进行存储。RAM node 上唯一会存储到磁盘上的元数据是 cluster 中使用的 disk node 的地址。要求在 RabbitMQ cluster中至少存在一个 disk node 。 问题四：RabbitMQ 上的一个 queue 中存放的 message 是否有数量限制？答：可以认为是无限制，因为限制取决于机器的内存，但是消息过多会导致处理效率的下降。 问题五：RabbitMQ 概念里的 channel、exchange 和 queue 这些东东是逻辑概念，还是对应着进程实体？这些东东分别起什么作用？答：queue 具有自己的 erlang 进程；exchange 内部实现为保存 binding 关系的查找表；channel 是实际进行路由工作的实体，即负责按照 routing_key 将 message 投递给queue 。由 AMQP 协议描述可知，channel 是真实 TCP 连接之上的虚拟连接，所有AMQP 命令都是通过 channel 发送的，且每一个 channel 有唯一的 ID。一个 channel 只能被单独一个操作系统线程使用，故投递到特定 channel 上的 message 是有顺序的。但一个操作系统线程上允许使用多个 channel 。channel 号为 0 的 channel 用于处理所有对于当前 connection 全局有效的帧，而 1-65535 号 channel 用于处理和特定 channel 相关的帧。AMQP 协议给出的 channel 复用模型如下 其中每一个 channel 运行在一个独立的线程上，多线程共享同一个 socket。 问题六：vhost 是什么？起什么作用？答：vhost 可以理解为虚拟 broker ，即 mini-RabbitMQ server。其内部均含有独立的queue、exchange 和 binding 等，但最最重要的是，其拥有独立的权限系统，可以做到vhost 范围的用户控制。当然，从 RabbitMQ 的全局角度，vhost 可以作为不同权限隔离的手段（一个典型的例子就是不同的应用可以跑在不同的 vhost 中）。 【cluster 相关】 问题七：在单 node 系统和多 node 构成的 cluster 系统中声明 queue、exchange ，以及 进行 binding 会有什么不同？ 答：当你在单 node 上声明 queue 时，只要该 node 上相关元数据进行了变更，你就会 得到 Queue.Declare-ok 回应；而在 cluster 上声明 queue ，则要求 cluster 上的全部 node 都要进行元数据成功更新，才会得到 Queue.Declare-ok 回应。另外，若 node 类型 为 RAM node 则变更的数据仅保存在内存中，若类型为 disk node 则还要变更保存在磁盘 上的数据。 问题八：客户端连接到 cluster 中的任意 node 上是否都能正常工作？ 答：是的。客户端感觉不到有何不同。 问题九：若 cluster 中拥有某个 queue 的 owner node 失效了，且该 queue 被声明具有 durable 属性，是否能够成功从其他 node 上重新声明该 queue ？ 答：不能，在这种情况下，将得到 404 NOT_FOUND 错误。只能等 queue 所属的 node 恢复后才能使用该 queue 。但若该 queue 本身不具有 durable 属性，则可在其他 node 上重新声明。 问题十：cluster 中 node 的失效会对 consumer 产生什么影响？若是在 cluster 中创建了 mirrored queue ，这时 node 失效会对 consumer 产生什么影响？ 答：若是 consumer 所连接的那个 node 失效（无论该 node 是否为 consumer 所订阅 queue 的 owner node），则 consumer 会在发现 TCP 连接断开时，按标准行为执行重连 逻辑，并根据“Assume Nothing”原则重建相应的 fabric 即可。若是失效的 node 为 consumer 订阅 queue 的 owner node，则 consumer 只能通过 Consumer Cancellation Notification 机制来检测与该 queue 订阅关系的终止，否则会出现傻等却没有任何消息来 到的问题。 问题十一：能够在地理上分开的不同数据中心使用 RabbitMQ cluster 么？ 答：不能。第一，你无法控制所创建的 queue 实际分布在 cluster 里的哪个 node 上（一 般使用 HAProxy + cluster 模型时都是这样），这可能会导致各种跨地域访问时的常见问 题；第二，Erlang 的 OTP 通信框架对延迟的容忍度有限，这可能会触发各种超时，导致 业务疲于处理；第三，在广域网上的连接失效问题将导致经典的“脑裂”问题，而 RabbitMQ 目前无法处理（该问题主要是说 Mnesia）。 【综合问题】 问题十二：为什么 heavy RPC 的使用场景下不建议采用 disk node ？ 答：heavy RPC 是指在业务逻辑中高频调用 RabbitMQ 提供的 RPC 机制，导致不断创建、 销毁 reply queue ，进而造成 disk node 的性能问题（因为会针对元数据不断写盘）。所以 在使用 RPC 机制时需要考虑自身的业务场景。 问题十三：向不存在的 exchange 发 publish 消息会发生什么？向不存在的 queue 执行","link":"/2021/06/21/25000%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/2.5.1Rocket/"},{"title":"","text":"RabbitMq一、Docker安装拉取镜像1docker pull docker.io/macintoshplus/rabbitmq-management 运行镜像1docker run -d --hostname my-rabbit --name rabbit -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin -p 15672:15672 -p 5672:5672 -p 25672:25672 -p 61613:61613 -p 1883:1883 rabbitmq:management -e RABBITMQ_DEFAULT_USER=admin 默认用户名 -e RABBITMQ_DEFAULT_PASS=admin 默认密码 http://localhost:15672 访问地址+端口 查看rabbitmq运行状况12345678910docker logs rabbit2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; node : rabbit@my-rabbit2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; home dir : /var/lib/rabbitmq2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; config file(s) : /etc/rabbitmq/conf.d/10-default-guest-user.conf2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; cookie hash : 9RiYHrEtDfFYMX2hWoP1/A==2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; log(s) : /var/log/rabbitmq/rabbit@my-rabbit_upgrade.log2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; : &lt;stdout&gt;2021-11-11 09:32:53.479856+00:00 [info] &lt;0.222.0&gt; database dir : /var/lib/rabbitmq/mnesia/rabbit@my-rabbit 二、支持消息的模式 https://rabbitmq.com/getstarted.html 2.1.简单模式Simple2.2.工作模式Work queues 2.3.发布订阅模式fanout fanout-发布订阅模式，是一种广播机制，它没有路由key的模式 2.4.路由模式direct 有routing-key的匹配模式 2.5.主题Topic模式topic 模糊的routing-key的匹配模式 2.6.参数模式类型：headers 特点：参数匹配模式 轮训分发 公平分发：需要手动应答，能者多劳 123finalChannel.basicQos(1); // 一次取多少条数据finalChannel.basicConsume(&quot;queue&quot;, false)","link":"/2021/11/24/25000%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/2052/"},{"title":"","text":"概述RabbitMq： spring同一个开发团队，开源 Kafka： 场景： 1","link":"/2022/03/15/25000%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/25000%E6%A6%82%E8%BF%B0/"},{"title":"","text":"","link":"/2021/11/29/25000%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/25002%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/"},{"title":"","text":"[TOC] 1.如何使用Redis来实现简单限流策略？首先我们来看一个常见 的简单的限流策略。系统要限定用户的某个行为在指定的时间里只能允许发生 N 次，如何使用 Redis 的数据结构来实现这个限流的功能？ 我们先定义这个接口，理解了这个接口的定义，读者就应该能明白我们期望达到的功能。 12345678910# 指定用户 user_id 的某个行为 action_key 在特定的时间内 period 只允许发生一定的次数max_countdef is_action_allowed(user_id, action_key, period, max_count): return True# 调用这个接口 , 一分钟内只允许最多回复 5 个帖子can_reply = is_action_allowed(&quot;laoqian&quot;, &quot;reply&quot;, 60, 5) if can_reply: do_reply() else: raise ActionThresholdOverflow() 问题：1.迁移前后Redis过期时间不一致。​ 我们将expire/pexpire/setex/psetex 命令在复制到从库的时候转换成时间戳的方式，比如expire 转成expireat命令，setex转换成set和expireat命令 2.迁移前后Redis key 数量不一致​ 1、主库在做RDB快照文件的时候，发现key已经过期了，则此时不会将过期时间写在RDB中​ 2、从库在load RDB 文件到内存中的时候，发现key已经过期了，则此时不会将过期的key load进去​ 针对上述问题，*目前我们将以上两个步骤都改为不忽略过期key，过期key的删除统一由主库触发删除，然后将删除命令传送到从库中。这样key的数量就完全一致了。*","link":"/2021/02/26/26000REDIS/1%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%20Redis%20%E6%9D%A5%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E9%99%90%E6%B5%81%E7%AD%96%E7%95%A5%EF%BC%9F/"},{"title":"","text":"2.6.1.分布式锁分布式锁超时问题可重入性 分布式应用进行逻辑处理时经常会遇到并发问题。 比如一个操作要修改用户的状态，修改状态需要先读出用户的状态，在内存里进行修改，改完了再存回去。如果这样的操作同时进行了，就会出现并发问题，因为读取和保存状态这两个操作不是原子的。（Wiki 解释：所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch 线程切换。） 这个时候就要使用到分布式锁来限制程序的并发执行。Redis 分布式锁使用非常广泛，它是面试的重要考点之一，很多同学都知道这个知识，也大致知道分布式锁的原理，但是具体到细节的使用上往往并不完全正确。 分布式锁分布式锁本质上要实现的目标就是在 Redis 里面占一个“茅坑”，当别的进程也要来占时，发现已经有人蹲在那里了，就只好放弃或者稍后再试。占坑一般是使用 setnx(set if not exists) 指令，只允许被一个客户端占坑。先来先占， 用完了，再调用 del 指令释放茅坑。 123456// 这里的冒号:就是一个普通的字符，没特别含义，它可以是任意其它字符，不要误解&gt; setnx lock:codehole trueOK... do something critical ...&gt; del lock:codehole(integer) 1 ​ 但是有个问题，如果逻辑执行到中间出现异常了，可能会导致 del 指令没有被调用，这样就会陷入死锁，锁永远得不到释放。 ​ 于是我们在拿到锁之后，再给锁加上一个过期时间，比如 5s，这样即使中间出现异常也可以保证 5 秒之后锁会自动释放。 123456&gt; setnx lock:codehole trueOK&gt; expire lock:codehole 5... do something critical ...&gt; del lock:codehole(integer) 1 ​ 但是以上逻辑还有问题。如果在 setnx 和 expire 之间服务器进程突然挂掉了，可能是因为机器掉电或者是被人为杀掉的，就会导致 expire 得不到执行，也会造成死锁。 ​ 这种问题的根源就在于 setnx 和 expire 是两条指令而不是原子指令。如果这两条指令可以一起执行就不会出现问题。也许你会想到用 Redis 事务来解决。但是这里不行，因为 expire 是依赖于 setnx 的执行结果的，如果 setnx 没抢到锁，expire 是不应该执行的。事务里没有 if\u0002-else 分支逻辑，事务的特点是一口气执行，要么全部执行要么一个都不执行。 ​ 为了解决这个疑难，Redis 开源社区涌现了一堆分布式锁的 library，专门用来解决这个问题。实现方法极为复杂，小白用户一般要费很大的精力才可以搞懂。如果你需要使用分布式锁，意味着你不能仅仅使用 Jedis 或者 redis-py 就行了，还得引入分布式锁的 library。 ​ 为了治理这个乱象，Redis 2.8 版本中作者加入了 set 指令的扩展参数，使得 setnx 和expire 指令可以一起执行，彻底解决了分布式锁的乱象。从此以后所有的第三方分布式锁library 可以休息了。 123&gt; set lock:codehole true ex 5 nx OK ... do something critical ... &gt; del lock:codehole 上面这个指令就是 setnx 和 expire 组合在一起的原子指令，它就是分布式锁的奥义所在。 超时问题Redis 的分布式锁不能解决超时问题，如果在加锁和释放锁之间的逻辑执行的太长，以至于超出了锁的超时限制，就会出现问题。因为这时候锁过期了，第二个线程重新持有了这把锁，但是紧接着第一个线程执行完了业务逻辑，就把锁给释放了，第三个线程就会在第二个线程逻辑执行完之间拿到了锁。 为了避免这个问题，Redis 分布式锁不要用于较长时间的任务。如果真的偶尔出现了，数据出现的小波错乱可能需要人工介入解决。 1234tag = random.nextint() # 随机数if redis.set(key, tag, nx=True, ex=5): do_something()redis.delifequals(key, tag) # 不存在的 delifequals 指令 有一个更加安全的方案是为 set 指令的 value 参数设置为一个随机数，释放锁时先匹配随机数是否一致，然后再删除 key。但是匹配 value 和删除 key 不是一个原子操作，Redis 也没有提供类似于 delifequals 这样的指令，这就需要使用 Lua 脚本来处理了，因为 Lua 脚本可以保证连续多个指令的原子性执行。 123456# delifequalsif redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end 可重入性可重入性是指线程在持有锁的情况下再次请求加锁，如果一个锁支持同一个线程的多次加锁，那么这个锁就是可重入的。比如 Java 语言里有个 ReentrantLock 就是可重入锁。Redis 分布式锁如果要支持可重入，需要对客户端的 set 方法进行包装，使用线程的 Threadlocal 变量存储当前持有锁的计数。 1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 import redis import threading locks = threading.local() locks.redis = {} def key_for(user_id): return &quot;account_{}&quot;.format(user_id) def _lock(client, key): return bool(client.set(key, True, nx=True, ex=5)) def _unlock(client, key): client.delete(key) def lock(client, user_id): key = key_for(user_id) if key in locks.redis: locks.redis[key] += 1 return True ok = _lock(client, key) if not ok: return False locks.redis[key] = 1 return True def unlock(client, user_id): key = key_for(user_id) if key in locks.redis: locks.redis[key] -= 1 if locks.redis[key] &lt;= 0: del locks.redis[key] return True return Falseclient = redis.StrictRedis() print &quot;lock&quot;, lock(client, &quot;codehole&quot;) print &quot;lock&quot;, lock(client, &quot;codehole&quot;) print &quot;unlock&quot;, unlock(client, &quot;codehole&quot;) print &quot;unlock&quot;, unlock(client, &quot;codehole&quot;) 以上还不是可重入锁的全部，精确一点还需要考虑内存锁计数的过期时间，代码复杂度将会继续升高。老钱不推荐使用可重入锁，它加重了客户端的复杂性，在编写业务方法时注意在逻辑结构上进行调整完全可以不使用可重入锁。下面是 Java 版本的可重入锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class RedisWithReentrantLock { private ThreadLocal&lt;Map&gt; lockers = new ThreadLocal&lt;&gt;(); private Jedis jedis; public RedisWithReentrantLock(Jedis jedis) { this.jedis = jedis; } private boolean _lock(String key) { return jedis.set(key, &quot;&quot;, &quot;nx&quot;, &quot;ex&quot;, 5L) != null; } private void _unlock(String key) { jedis.del(key); } private Map &lt;String, Integer&gt; currentLockers() { Map &lt;String, Integer&gt; refs = lockers.get(); if (refs != null) { return refs; } lockers.set(new HashMap&lt;&gt;()); return lockers.get(); } public boolean lock(String key) { Map refs = currentLockers(); Integer refCnt = refs.get(key); if (refCnt != null) { refs.put(key, refCnt + 1); return true; } boolean ok = this._lock(key); if (!ok) { return false; } refs.put(key, 1); return true; } public boolean unlock(String key) { Map refs = currentLockers(); Integer refCnt = refs.get(key); if (refCnt == null) { return false; } refCnt -= 1; if (refCnt &gt; 0) { refs.put(key, refCnt); } else { refs.remove(key); this ._unlock(key); } return true; } public static void main(String[] args) { Jedis jedis = new Jedis(); RedisWithReentrantLock redis = new RedisWithReentrantLock(jedis); System.out.println(redis.lock(&quot;codehole&quot;)); System.out.println(redis.lock(&quot;codehole&quot;)); System.out.println(redis.unlock(&quot;codehole&quot;)); System.out.println(redis.unlock(&quot;codehole&quot;)); } }","link":"/2021/06/08/26000REDIS/2.6.1%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"title":"","text":"2.6.2延时队列异步消息队列队列空了怎么办？队列延迟空闲连接自动断开锁冲突处理延时队列的实现 ​ 我们平时习惯于使用 Rabbitmq 和 Kafka 作为消息队列中间件，来给应用程序之间增加异步消息传递功能。这两个中间件都是专业的消息队列中间件，特性之多超出了大多数人的理解能力。 ​ 使用过 Rabbitmq 的同学知道它使用起来有多复杂，发消息之前要创建 Exchange，再创建 Queue，还要将 Queue 和 Exchange 通过某种规则绑定起来，发消息的时候要指定 routing\u0002-key，还要控制头部信息。消费者在消费消息之前也要进行上面一系列的繁琐过程。但是绝大多数情况下，虽然我们的消息队列只有一组消费者，但还是需要经历上面这些繁琐的过程。 ​ 有了 Redis，它就可以让我们解脱出来，对于那些只有一组消费者的消息队列，使用 Redis 就可以非常轻松的搞定。Redis 的消息队列不是专业的消息队列，它没有非常多的高级特性，没有 ack 保证，如果对消息的可靠性有着极致的追求，那么它就不适合使用。 异步消息队列Redis 的 list(列表) 数据结构常用来作为异步消息队列使用，使用rpush/lpush操作入队列，使用 lpop 和 rpop 来出队列。 123456789101112131415161718&gt; rpush notify-queue apple banana pear (integer) 3 &gt; llen notify-queue (integer) 3 &gt; lpop notify-queue &quot;apple&quot;&gt; llen notify-queue (integer) 2 &gt; lpop notify-queue &quot;banana&quot;&gt; llen notify-queue (integer) 1 &gt; lpop notify-queue &quot;pear&quot;&gt; llen notify-queue (integer) 0 &gt; lpop notify-queue (nil) 队列空了怎么办？​ 客户端是通过队列的 pop 操作来获取消息，然后进行处理。处理完了再接着获取消息，再进行处理。如此循环往复，这便是作为队列消费者的客户端的生命周期。 ​ 可是如果队列空了，客户端就会陷入 pop 的死循环，不停地 pop，没有数据，接着再 pop，又没有数据。这就是浪费生命的空轮询。空轮询不但拉高了客户端的 CPU，redis 的 QPS 也会被拉高，如果这样空轮询的客户端有几十来个，Redis 的慢查询可能会显著增多。 ​ 通常我们使用 sleep 来解决这个问题，让线程睡一会，睡个 1s 钟就可以了。不但客户端的 CPU 能降下来，Redis 的 QPS 也降下来了。 1Thread.sleep(1000) # java 睡 1s 队列延迟用上面睡眠的办法可以解决问题。但是有个小问题，那就是睡眠会导致消息的延迟增大。如果只有 1 个消费者，那么这个延迟就是 1s。如果有多个消费者，这个延迟会有所下降，因为每个消费者的睡觉时间是岔开来的。 ​ 有没有什么办法能显著降低延迟呢？你当然可以很快想到：那就把睡觉的时间缩短点。这种方式当然可以，不过有没有更好的解决方案呢？当然也有，那就是 blpop/brpop。 ​ 这两个指令的前缀字符 b 代表的是 blocking，也就是阻塞读。 ​ 阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为零。用 blpop/brpop 替代前面的 lpop/rpop，就完美解决了上面的问题。… 空闲连接自动断开​ 你以为上面的方案真的很完美么？先别急着开心，其实他还有个问题需要解决。 ​ 什么问题？—— 空闲连接的问题。 ​ 如果线程一直阻塞在哪里，Redis 的客户端连接就成了闲置连接，闲置过久，服务器一般会主动断开连接，减少闲置资源占用。这个时候 blpop/brpop 会抛出异常来。 ​ 所以编写客户端消费者的时候要小心，注意捕获异常，还要重试。… 锁冲突处理​ 上节课我们讲了分布式锁的问题，但是没有提到客户端在处理请求时加锁没加成功怎么办。一般有 3 种策略来处理加锁失败： ​ 1、直接抛出异常，通知用户稍后重试；​ 2、sleep 一会再重试；​ 3、将请求转移至延时队列，过一会再试； 直接抛出特定类型的异常 这种方式比较适合由用户直接发起的请求，用户看到错误对话框后，会先阅读对话框的内容，再点击重试，这样就可以起到人工延时的效果。如果考虑到用户体验，可以由前端的代码替代用户自己来进行延时重试控制。它本质上是对当前请求的放弃，由用户决定是否重新发起新的请求。 sleep sleep 会阻塞当前的消息处理线程，会导致队列的后续消息处理出现延迟。如果碰撞的比较频繁或者队列里消息比较多，sleep 可能并不合适。如果因为个别死锁的 key 导致加锁不成功，线程会彻底堵死，导致后续消息永远得不到及时处理。 延时队列 这种方式比较适合异步消息处理，将当前冲突的请求扔到另一个队列延后处理以避开冲突。 延时队列的实现延时队列可以通过 Redis 的 zset(有序列表) 来实现。我们将消息序列化成一个字符串作为 zset 的 value，这个消息的到期处理时间作为 score，然后用多个线程轮询 zset 获取到期的任务进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处理。因为有多个线程，所以需要考虑并发争抢任务，确保任务不能被多次执行。 123456789101112131415161718def delay(msg): msg.id = str(uuid.uuid4()) # 保证 value 值唯一 value = json.dumps(msg) retry_ts = time.time() + 5 # 5 秒后重试 redis.zadd(&quot;delay-queue&quot;, retry_ts, value) def loop(): while True: # 最多取 1 条 values = redis.zrangebyscore(&quot;delay-queue&quot;, 0, time.time(), start=0, num=1) if not values: time.sleep(1) # 延时队列空的，休息 1s continue value = values[0] # 拿第一条，也只有一条 success = redis.zrem(&quot;delay-queue&quot;, value) # 从消息队列中移除该消息 if success: # 因为有多进程并发的可能，最终只会有一个进程可以抢到消息 msg = json.loads(value) handle_msg(msg) ​ Redis 的 zrem 方法是多线程多进程争抢任务的关键，它的返回值决定了当前实例有没有抢到任务，因为 loop 方法可能会被多个线程、多个进程调用，同一个任务可能会被多个进程线程抢到，通过 zrem来决定唯一的属主。 ​ 同时，我们要注意一定要对 handle_msg 进行异常捕获，避免因为个别任务处理问题导致循环异常退出。以下是 Java 版本的延时队列实现，因为要使用到 Json 序列化，所以还需要 fastjson 库的支持。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.lang.reflect.Type;import java.util.Set;import java.util.UUID;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.TypeReference;import redis.clients.jedis.Jedis;public class RedisDelayingQueue&lt;T&gt; { static class TaskItem&lt;T&gt; { public String id; public T msg; } // fastjson 序列化对象中存在 generic 类型时，需要使用 TypeReference private Type TaskType = new TypeReference&lt;TaskItem&lt;T&gt;&gt;() { }.getType(); private Jedis jedis; private String queueKey; public RedisDelayingQueue(Jedis jedis, String queueKey) { this.jedis = jedis; this.queueKey = queueKey; } public void delay(T msg) { TaskItem task = new TaskItem(); task.id = UUID.randomUUID().toString(); // 分配唯一的 uuid task.msg = msg; String s = JSON.toJSONString(task); // fastjson 序列化 jedis.zadd(queueKey, System.currentTimeMillis() + 5000, s); // 塞入延时队列 ,5s 后再试 } public void loop() { while (!Thread.interrupted()) {// 只取一条 Set values = jedis.zrangeByScore(queueKey, 0, System.currentTimeMillis(), 0, 1); if (values.isEmpty()) { try { Thread.sleep(500); // 歇会继续 } catch (InterruptedException e) { break; } continue; } String s = values.iterator().next(); if (jedis.zrem(queueKey, s) &gt; 0) { // 抢到了 TaskItem task = JSON.parseObject(s, TaskType); // fastjson 反序列化 this.handleMsg(task.msg); } } } public void handleMsg(T msg) { System.out.println(msg); } public static void main(String[] args) { Jedis jedis = new Jedis(); RedisDelayingQueue queue = new RedisDelayingQueue&lt;&gt;(jedis, &quot;q-demo&quot;); Thread producer = new Thread() { public void run() { for (int i = 0; i &lt; 10; i++) { queue.delay(&quot;codehole&quot; + i); } } }; Thread consumer = new Thread() { public void run() { queue.loop(); } }; producer.start(); consumer.start(); try { producer.join(); Thread.sleep(6000); consumer.interrupt(); consumer.join(); } catch (InterruptedException e) { } }} 进一步优化 上面的算法中同一个任务可能会被多个进程取到之后再使用 zrem 进行争抢，那些没抢到的进程都是白取了一次任务，这是浪费。可以考虑使用 lua scripting 来优化一下这个逻辑，将zrangebyscore 和 zrem 一同挪到服务器端进行原子化操作，这样多个进程之间争抢任务时就不会出现这种浪费了。 思考 1、Redis 作为消息队列为什么不能保证 100% 的可靠性？ 2、使用 Lua Scripting 来优化延时队列的逻辑。","link":"/2021/06/08/26000REDIS/2.6.2%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/"},{"title":"","text":"2.6.30 签到功能常见的场景如下： 给一个 userId ，判断用户登陆状态； 显示用户某个月的签到次数和首次签到时间； 两亿用户最近 7 天的签到情况，统计 7 天内连续签到的用户总数； 通常情况下，我们面临的用户数量以及访问量都是巨大的，比如百万、千万级别的用户数量，或者千万级别、甚至亿级别的访问信息。 所以，我们必须要选择能够非常高效地统计大量数据（例如亿级）的集合类型。 如何选择合适的数据集合，我们首先要了解常用的统计模式，并运用合理的数据类型来解决实际问题。 四种统计类型： 二值状态统计； 聚合统计； 排序统计； 基数统计。 本文将由二值状态统计类型作为实战篇系列的开篇，文中将用到 String、Set、Zset、List、hash 以外的拓展数据类型 Bitmap 来实现。 文章涉及到的指令可以通过在线 Redis 客户端运行调试，地址：https://try.redis.io/，超方便的说。 寄语 ❝ 多分享多付出，前期多给别人创造价值并且不计回报，从长远来看，这些付出都会成倍的回报你。 特别是刚开始跟别人合作的时候，不要去计较短期的回报，没有太大意义，更多的是锻炼自己的视野、视角以及解决问题的能力。 二值状态统计 ❝ 码哥，什么是二值状态统计呀？ 也就是集合中的元素的值只有 0 和 1 两种，在签到打卡和用户是否登陆的场景中，只需记录签到(1)或 未签到(0)，已登录(1)或未登陆(0)。 假如我们在判断用户是否登陆的场景中使用 Redis 的 String 类型实现（key -&gt; userId，value -&gt; 0 表示下线，1 - 登陆），假如存储 100 万个用户的登陆状态，如果以字符串的形式存储，就需要存储 100 万个字符串了，内存开销太大。 ❝ 码哥，为什么 String 类型内存开销大？ String 类型除了记录实际数据以外，还需要额外的内存记录数据长度、空间使用等信息。 当保存的数据包含字符串，String 类型就使用简单动态字符串（SDS）结构体来保存，如下图所示： SDS len：占 4 个字节，表示 buf 的已用长度。 alloc：占 4 个字节，表示 buf 实际分配的长度，通常 &gt; len。 buf：字节数组，保存实际的数据，Redis 自动在数组最后加上一个 “\\0”，额外占用一个字节的开销。 所以，在 SDS 中除了 buf 保存实际的数据， len 与 alloc 就是额外的开销。 另外，还有一个 RedisObject 结构的开销，因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等）。 所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。 对于二值状态场景，我们就可以利用 Bitmap 来实现。比如登陆状态我们用一个 bit 位表示，一亿个用户也只占用 一亿 个 bit 位内存 ≈ （100000000 / 8/ 1024/1024）12 MB。 1大概的空间占用计算公式是：($offset/8/1024/1024) MB ❝ 什么是 Bitmap 呢？ Bitmap 的底层数据结构用的是 String 类型的 SDS 数据结构来保存位数组，Redis 把每个字节数组的 8 个 bit 位利用起来，每个 bit 位 表示一个元素的二值状态（不是 0 就是 1）。 可以将 Bitmap 看成是一个 bit 为单位的数组，数组的每个单元只能存储 0 或者 1，数组的下标在 Bitmap 中叫做 offset 偏移量。 为了直观展示，我们可以理解成 buf 数组的每个字节用一行表示，每一行有 8 个 bit 位，8 个格子分别表示这个字节中的 8 个 bit 位，如下图所示： Bitmap 8 个 bit 组成一个 Byte，所以 Bitmap 会极大地节省存储空间。 这就是 Bitmap 的优势。 判断用户登陆态 ❝ 怎么用 Bitmap 来判断海量用户中某个用户是否在线呢？ Bitmap 提供了 GETBIT、SETBIT 操作，通过一个偏移值 offset 对 bit 数组的 offset 位置的 bit 位进行读写操作，需要注意的是 offset 从 0 开始。 只需要一个 key = login_status 表示存储用户登陆状态集合数据， 将用户 ID 作为 offset，在线就设置为 1，下线设置 0。通过 GETBIT判断对应的用户是否在线。50000 万 用户只需要 6 MB 的空间。 SETBIT 命令 1SETBIT &lt;key&gt; &lt;offset&gt; &lt;value&gt; 设置或者清空 key 的 value 在 offset 处的 bit 值（只能是 0 或者 1）。 GETBIT 命令 1GETBIT &lt;key&gt; &lt;offset&gt; 获取 key 的 value 在 offset 处的 bit 位的值，当 key 不存在时，返回 0。 假如我们要判断 ID = 10086 的用户的登陆情况： 第一步，执行以下指令，表示用户已登录。 1SETBIT login_status 10086 1 第二步，检查该用户是否登陆，返回值 1 表示已登录。 1GETBIT login_status 10086 第三步，登出，将 offset 对应的 value 设置成 0。 1SETBIT login_status 10086 0 用户每个月的签到情况在签到统计中，每个用户每天的签到用 1 个 bit 位表示，一年的签到只需要 365 个 bit 位。一个月最多只有 31 天，只需要 31 个 bit 位即可。 ❝ 比如统计编号 89757 的用户在 2021 年 5 月份的打卡情况要如何进行？ key 可以设计成 uid:sign:{userId}:{yyyyMM}，月份的每一天的值 - 1 可以作为 offset（因为 offset 从 0 开始，所以 offset = 日期 - 1）。 第一步，执行下面指令表示记录用户在 2021 年 5 月 16 号打卡。 1SETBIT uid:sign:89757:202105 15 1 第二步，判断编号 89757 用户在 2021 年 5 月 16 号是否打卡。 1GETBIT uid:sign:89757:202105 15 第三步，统计该用户在 5 月份的打卡次数，使用 BITCOUNT 指令。该指令用于统计给定的 bit 数组中，值 = 1 的 bit 位的数量。 1BITCOUNT uid:sign:89757:202105 这样我们就可以实现用户每个月的打卡情况了，是不是很赞。 ❝ 如何统计这个月首次打卡时间呢？ Redis 提供了 BITPOS key bitValue [start] [end]指令，返回数据表示 Bitmap 中第一个值为 bitValue 的 offset 位置。 在默认情况下， 命令将检测整个位图， 用户可以通过可选的 start 参数和 end 参数指定要检测的范围。 所以我们可以通过执行以下指令来获取 userID = 89757 在 2021 年 5 月份首次打卡日期： 1BITPOS uid:sign:89757:202105 1 需要注意的是，我们需要将返回的 value + 1 ，因为 offset 从 0 开始。 连续签到用户总数 ❝ 在记录了一个亿的用户连续 7 天的打卡数据，如何统计出这连续 7 天连续打卡用户总数呢？ 我们把每天的日期作为 Bitmap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置成 1。 key 对应的集合的每个 bit 位的数据则是一个用户在该日期的打卡记录。 一共有 7 个这样的 Bitmap，如果我们能对这 7 个 Bitmap 的对应的 bit 位做 『与』运算。 同样的 UserID offset 都是一样的，当一个 userID 在 7 个 Bitmap 对应对应的 offset 位置的 bit = 1 就说明该用户 7 天连续打卡。 结果保存到一个新 Bitmap 中，我们再通过 BITCOUNT 统计 bit = 1 的个数便得到了连续打卡 7 天的用户总数了。 Redis 提供了 BITOP operation destkey key [key …]这个指令用于对一个或者多个 键 = key 的 Bitmap 进行位元操作。 opration 可以是 and、OR、NOT、XOR。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0 。空的 key 也被看作是包含 0 的字符串序列。 便于理解，如下图所示： BITOP 3 个 Bitmap，对应的 bit 位做「与」操作，结果保存到新的 Bitmap 中。 操作指令表示将 三个 bitmap 进行 AND 操作，并将结果保存到 destmap 中。接着对 destmap 执行 BITCOUNT 统计。 1234// 与操作BITOP AND destmap bitmap:01 bitmap:02 bitmap:03// 统计 bit 位 = 1 的个数BITCOUNT destmap 简单计算下 一个一亿个位的 Bitmap占用的内存开销，大约占 12 MB 的内存（10^8/8/1024/1024），7 天的 Bitmap 的内存开销约为 84 MB。同时我们最好给 Bitmap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存。 小结思路才是最重要，当我们遇到的统计场景只需要统计数据的二值状态，比如用户是否存在、 ip 是否是黑名单、以及签到打卡统计等场景就可以考虑使用 Bitmap。 只需要一个 bit 位就能表示 0 和 1。在统计海量数据的时候将大大减少内存占用。","link":"/2021/05/27/26000REDIS/2.6.30%20%E7%AD%BE%E5%88%B0%E5%8A%9F%E8%83%BD/"},{"title":"","text":"Redis集群的5种使用方式，及各自优缺点对比分析本文主要针对 Redis 常见的几种使用方式及其优缺点展开分析。 一、常见使用方式Redis 的几种常见使用方式包括： Redis 单副本 Redis 多副本(主从) Redis Sentinel(哨兵) Redis Cluster Redis 自研 二、各种使用方式的优缺点1、Redis 单副本 Redis 单副本，采用单个 Redis 节点部署架构，没有备用节点实时同步数据，不提供数据持久化和备份策略，适用于数据可靠性要求不高的纯缓存业务场景。 优点： 架构简单，部署方便; 高性价比：缓存使用时无需备用节点(单实例可用性可以用 supervisor 或 crontab 保证)，当然为了满足业务的高可用性，也可以牺牲一个备用节点，但同时刻只有一个实例对外提供服务; 高性能。 缺点： 不保证数据的可靠性; 在缓存使用，进程重启后，数据丢失，即使有备用的节点解决高可用性，但是仍然不能解决缓存预热问题，因此不适用于数据可靠性要求高的业务; 高性能受限于单核 CPU 的处理能力(Redis 是单线程机制)，CPU 为主要瓶颈，所以适合操作命令简单，排序、计算较少的场景。也可以考虑用 Memcached 替代。 2、Redis 多副本(主从) Redis 多副本，采用主从(replication)部署结构，相较于单副本而言最大的特点就是主从实例间数据实时同步，并且提供数据持久化和备份策略。主从实例部署在不同的物理服务器上，根据公司的基础环境配置，可以实现同时对外提供服务和读写分离策略。 优点： 高可靠性：一方面，采用双机主备架构，能够在主库出现故障时自动进行主备切换，从库提升为主库提供服务，保证服务平稳运行;另一方面，开启数据持久化功能和配置合理的备份策略，能有效的解决数据误操作和数据异常丢失的问题; 读写分离策略：从节点可以扩展主库节点的读能力，有效应对大并发量的读操作。 缺点： 故障恢复复杂，如果没有 RedisHA 系统(需要开发)，当主库节点出现故障时，需要手动将一个从节点晋升为主节点，同时需要通知业务方变更配置，并且需要让其它从库节点去复制新主库节点，整个过程需要人为干预，比较繁琐; 主库的写能力受到单机的限制，可以考虑分片; 主库的存储能力受到单机的限制，可以考虑 Pika; 原生复制的弊端在早期的版本中也会比较突出，如：Redis 复制中断后，Slave 会发起 psync，此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时可能会造成毫秒或秒级的卡顿;又由于 COW 机制，导致极端情况下的主库内存溢出，程序异常退出或宕机;主库节点生成备份文件导致服务器磁盘 IO 和 CPU(压缩)资源消耗;发送数 GB 大小的备份文件导致服务器出口带宽暴增，阻塞请求，建议升级到最新版本。 3、Redis Sentinel(哨兵) Redis Sentinel 是社区版本推出的原生高可用解决方案，其部署架构主要包括两部分：Redis Sentinel 集群和 Redis 数据集群。 其中 Redis Sentinel 集群是由若干 Sentinel 节点组成的分布式集群，可以实现故障发现、故障自动转移、配置中心和客户端通知。Redis Sentinel 的节点数量要满足 2n+1(n&gt;=1)的奇数个。 优点： Redis Sentinel 集群部署简单; 能够解决 Redis 主从模式下的高可用切换问题; 很方便实现 Redis 数据节点的线形扩展，轻松突破 Redis 自身单线程瓶颈，可极大满足 Redis 大容量或高性能的业务需求; 可以实现一套 Sentinel 监控一组 Redis 数据节点或多组数据节点。 缺点： 部署相对 Redis 主从模式要复杂一些，原理理解更繁琐; 资源浪费，Redis 数据节点中 slave 节点作为备份节点不提供服务; Redis Sentinel 主要是针对 Redis 数据节点中的主节点的高可用切换，对 Redis 的数据节点做失败判定分为主观下线和客观下线两种，对于 Redis 的从节点有对节点做主观下线操作，并不执行故障转移。 不能解决读写分离问题，实现起来相对复杂。 建议： 如果监控同一业务，可以选择一套 Sentinel 集群监控多组 Redis 数据节点的方案，反之选择一套 Sentinel 监控一组 Redis 数据节点的方案。 sentinel monitor配置中的建议设置成 Sentinel 节点的一半加 1，当 Sentinel 部署在多个 IDC 的时候，单个 IDC 部署的 Sentinel 数量不建议超过(Sentinel 数量 – quorum)。 合理设置参数，防止误切，控制切换灵敏度控制： a. quorum b. down-after-milliseconds 30000 c. failover-timeout 180000 d. maxclient e. timeout 部署的各个节点服务器时间尽量要同步，否则日志的时序性会混乱。 Redis 建议使用 pipeline 和 multi-keys 操作，减少 RTT 次数，提高请求效率。 自行搞定配置中心(zookeeper)，方便客户端对实例的链接访问。 4、Redis Cluster Redis Cluster 是社区版推出的 Redis 分布式集群解决方案，主要解决 Redis 分布式方面的需求，比如，当遇到单机内存，并发和流量等瓶颈的时候，Redis Cluster 能起到很好的负载均衡的目的。 Redis Cluster 集群节点最小配置 6 个节点以上(3 主 3 从)，其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。 Redis Cluster 采用虚拟槽分区，所有的键根据哈希函数映射到 0～16383 个整数槽内，每个节点负责维护一部分槽以及槽所印映射的键值数据。 优点： 无中心架构; 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布; 可扩展性：可线性扩展到 1000 多个节点，节点可动态添加或删除; 高可用性：部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升; 降低运维成本，提高系统的扩展性和可用性。 缺点： Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅 JedisCluster 相对成熟，异常处理部分还不完善，比如常见的“max redirect exception”。 节点会因为某些原因发生阻塞(阻塞时间大于 clutser-node-timeout)，被判断下线，这种 failover 是没有必要的。 数据通过异步复制，不保证数据的强一致性。 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。 Slave 在集群中充当“冷备”，不能缓解读压力，当然可以通过 SDK 的合理设计来提高 Slave 资源的利用率。 Key 批量操作限制，如使用 mset、mget 目前只支持具有相同 slot 值的 Key 执行批量操作。对于映射为不同 slot 值的 Key 由于 Keys 不支持跨 slot 查询，所以执行 mset、mget、sunion 等操作支持不友好。 Key 事务操作支持有限，只支持多 key 在同一节点上的事务操作，当多个 Key 分布于不同的节点上时无法使用事务功能。 Key 作为数据分区的最小粒度，不能将一个很大的键值对象如 hash、list 等映射到不同的节点。 不支持多数据库空间，单机下的 redis 可以支持到 16 个数据库，集群模式下只能使用 1 个数据库空间，即 db 0。 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。 避免产生 hot-key，导致主库节点成为系统的短板。 避免产生 big-key，导致网卡撑爆、慢查询等。 重试时间应该大于 cluster-node-time 时间。 Redis Cluster 不建议使用 pipeline 和 multi-keys 操作，减少 max redirect 产生的场景。 5、Redis 自研 Redis 自研的高可用解决方案，主要体现在配置中心、故障探测和 failover 的处理机制上，通常需要根据企业业务的实际线上环境来定制化。 优点： 高可靠性、高可用性; 自主可控性高; 贴切业务实际需求，可缩性好，兼容性好。 缺点： 实现复杂，开发成本高; 需要建立配套的周边设施，如监控，域名服务，存储元数据信息的数据库等; 维护成本高。","link":"/2021/05/28/26000REDIS/2.6.31%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/"},{"title":"","text":"2.6.3位图基本使用统计和查找魔术指令bitfield ​ 在我们平时开发过程中，会有一些 bool 型数据需要存取，比如用户一年的签到记录，签了是 1，没签是 0，要记录 365 天。如果使用普通的 key/value，每个用户要记录 365 个，当用户上亿的时候，需要的存储空间是惊人的。 ​ 为了解决这个问题，Redis 提供了位图数据结构，这样每天的签到记录只占据一个位，365 天就是 365 个位，46 个字节 (一个稍长一点的字符串) 就可以完全容纳下，这就大大节约了存储空间。 ​ 位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 byte 数组。我们可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。 ​ 以老钱的经验，在面试中有 Redis 位图使用经验的同学很少，如果你对 Redis 的位图有所了解，它将会是你的面试加分项。 基本使用Redis 的位数组是自动扩展，如果设置了某个偏移位置超出了现有的内容范围，就会自动将位数组进行零扩充。 接下来我们使用位操作将字符串设置为 hello (不是直接使用 set 指令)，首先我们需要得到 hello 的 ASCII 码，用 Python 命令行可以很方便地得到每个字符的 ASCII 码的二进制值。 12345678910&gt;&gt;&gt; bin(ord('h')) '0b1101000' # 高位 -&gt; 低位&gt;&gt;&gt; bin(ord('e'))'0b1100101'&gt;&gt;&gt; bin(ord('l')) '0b1101100'&gt;&gt;&gt; bin(ord('l'))'0b1101100'&gt;&gt;&gt; bin(ord('o'))'0b1101111' 接下来我们使用 redis-cli 设置第一个字符，也就是位数组的前 8 位，我们只需要设置值为 1 的位，如上图所示，h 字符只有 1/2/4 位需要设置，e 字符只有 9/10/13/15 位需要设置。值得注意的是位数组的顺序和字符的位顺序是相反的。 12345678910111213141516127.0.0.1:6379&gt; setbit s 1 1 (integer) 0 127.0.0.1:6379&gt; setbit s 2 1 (integer) 0 127.0.0.1:6379&gt; setbit s 4 1 (integer) 0 127.0.0.1:6379&gt; setbit s 9 1 (integer) 0 127.0.0.1:6379&gt; setbit s 10 1 (integer) 0 127.0.0.1:6379&gt; setbit s 13 1(integer) 0 127.0.0.1:6379&gt; setbit s 15 1 (integer) 0 127.0.0.1:6379&gt; get s &quot;he&quot; 上面这个例子可以理解为「零存整取」，同样我们还也可以「零存零取」，「整存零取」。「零存」就是使用 setbit 对位值进行逐个设置，「整存」就是使用字符串一次性填充所有位数组，覆盖掉旧值。 零存零取 1234567891011121314127.0.0.1:6379&gt; setbit w 1 1(integer) 0127.0.0.1:6379&gt; setbit w 2 1(integer) 0127.0.0.1:6379&gt; setbit w 4 1(integer) 0127.0.0.1:6379&gt; getbit w 1 # 获取某个具体位置的值 0/1(integer) 1127.0.0.1:6379&gt; getbit w 2(integer) 1127.0.0.1:6379&gt; getbit w 4(integer) 1127.0.0.1:6379&gt; getbit w 5(integer) 0 如果对应位的字节是不可打印字符，redis-cli 会显示该字符的 16 进制形式。 123456127.0.0.1:6379&gt; setbit x 0 1(integer) 0127.0.0.1:6379&gt; setbit x 1 1(integer) 0127.0.0.1:6379&gt; get x&quot;\\xc0&quot; 统计和查找​ Redis 提供了位图统计指令 bitcount 和位图查找指令 bitpos，bitcount 用来统计指定位置范围内 1 的个数，bitpos 用来查找指定范围内出现的第一个 0 或 1。 ​ 比如我们可以通过 bitcount 统计用户一共签到了多少天，通过 bitpos 指令查找用户从哪一天开始第一次签到。如果指定了范围参数[start, end]，就可以统计在某个时间范围内用户签到了多少天，用户自某天以后的哪天开始签到。 ​ 遗憾的是， start 和 end 参数是字节索引，也就是说指定的位范围必须是 8 的倍数，而不能任意指定。这很奇怪，我表示不是很能理解 Antirez 为什么要这样设计。因为这个设计，我们无法直接计算某个月内用户签到了多少天，而必须要将这个月所覆盖的字节内容全部取出来 (getrange 可以取出字符串的子串) 然后在内存里进行统计，这个非常繁琐。 接下来我们简单试用一下 bitcount 指令和 bitpos 指令: 12345678910111213141516127.0.0.1:6379&gt; set w helloOK127.0.0.1:6379&gt; bitcount w(integer) 21127.0.0.1:6379&gt; bitcount w 0 0 # 第一个字符中 1 的位数(integer) 3127.0.0.1:6379&gt; bitcount w 0 1 # 前两个字符中 1 的位数(integer) 7127.0.0.1:6379&gt; bitpos w 0 # 第一个 0 位(integer) 0127.0.0.1:6379&gt; bitpos w 1 # 第一个 1 位(integer) 1127.0.0.1:6379&gt; bitpos w 1 1 1 # 从第二个字符算起，第一个 1 位(integer) 9127.0.0.1:6379&gt; bitpos w 1 2 2 # 从第三个字符算起，第一个 1 位(integer) 17 魔术指令bitfield前文我们设置 (setbit) 和获取 (getbit) 指定位的值都是单个位的，如果要一次操作多个位，就必须使用管道来处理。 不过 Redis 的 3.2 版本以后新增了一个功能强大的指令，有了这条指令，不用管道也可以一次进行多个位的操作。 bitfield 有三个子指令，分别是get/set/incrby，它们都可以对指定位片段进行读写，但是最多只能处理 64 个连续的位，如果超过 64 位，就得使用多个子指令，bitfield 可以一次执行多个子指令。 接下来我们对照着上面的图看个简单的例子: 12345678910127.0.0.1:6379&gt; set w helloOK127.0.0.1:6379&gt; bitfield w get u4 0 # 从第一个位开始取 4 个位，结果是无符号数 (u)(integer) 6127.0.0.1:6379&gt; bitfield w get u3 2 # 从第三个位开始取 3 个位，结果是无符号数 (u)(integer) 5127.0.0.1:6379&gt; bitfield w get i4 0 # 从第一个位开始取 4 个位，结果是有符号数 (i)(integer) 6127.0.0.1:6379&gt; bitfield w get i3 2 # 从第三个位开始取 3 个位，结果是有符号数 (i)(integer) -3","link":"/2021/06/09/26000REDIS/2.6.3%E4%BD%8D%E5%9B%BE/"},{"title":"","text":"2.6.4HyperLogLog​ 在开始这一节之前，我们先思考一个常见的业务问题：如果你负责开发维护一个大型的网站，有一天老板找产品经理要网站每个网页每天的 UV 数据，然后让你来开发这个统计模块，你会如何实现？ ​ 如果统计 PV 那非常好办，给每个网页一个独立的 Redis 计数器就可以了，这个计数器的 key 后缀加上当天的日期。这样来一个请求，incrby 一次，最终就可以统计出所有的 PV 数据。 ​ 但是 UV 不一样，它要去重，同一个用户一天之内的多次访问请求只能计数一次。这就要求每一个网页请求都需要带上用户的 ID，无论是登陆用户还是未登陆用户都需要一个唯一ID 来标识。 ​ 你也许已经想到了一个简单的方案，那就是为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。当一个请求过来时，我们使用 sadd 将用户 ID 塞进去就可以了。通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。没错，这是一个非常简单的方案。 ​ 但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。如果这样的页面很多，那所需要的存储空间是惊人的。为这样一个去重功能就耗费这样多的存储空间，值得么？其实老板需要的数据又不需要太精确，105w 和 106w 这两个数字对于老板们来说并没有多大区别，So，有没有更好的解决方案呢？ ​ 这就是本节要引入的一个解决方案，Redis 提供了 HyperLogLog 数据结构就是用来解决这种统计问题的。HyperLogLog 提供不精确的去重计数方案，虽然不精确但是也不是非常不精确，标准误差是 0.81%，这样的精确度已经可以满足上面的 UV 统计需求了。 ​ HyperLogLog 数据结构是 Redis 的高级数据结构，它非常有用，但是令人感到意外的是，使用过它的人非常少。 使用方法HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。pfadd 用法和 set 集合的 sadd 是一样的，来一个用户 ID，就将用户 ID 塞进去就是。pfcount 和 scard 用法是一样的，直接获取计数值。 12345678910111213141516171819202122232425262728127.0.0.1:6379&gt; pfadd codehole user1(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 1127.0.0.1:6379&gt; pfadd codehole user2(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 2127.0.0.1:6379&gt; pfadd codehole user3(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 3127.0.0.1:6379&gt; pfadd codehole user4(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 4127.0.0.1:6379&gt; pfadd codehole user5(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 5127.0.0.1:6379&gt; pfadd codehole user6(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 6127.0.0.1:6379&gt; pfadd codehole user7 user8 user9 user10(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 10 12345678910111213public class PfTest { public static void main(String[] args) { Jedis jedis = new Jedis(); for (int i = 0; i &lt; 1000; i++) { jedis.pfadd(&quot;codehole&quot;, &quot;user&quot; + i); long total = jedis.pfcount(&quot;codehole&quot;); if (total != i + 1) { System.out.printf(&quot;%d %d\\n&quot;, total, i + 1); break; } } jedis.close();} } 们将数据增加到 10w 差了 277 个，按百分比是 0.277%，对于上面的 UV 统计需求来说，误差率也不算高。然后我们把上面的脚本再跑一边，也就相当于将数据重复加入一边，查看输出，可以发现，pfcount 的结果没有任何改变，还是 99723，说明它确实具备去重功能。 pfmerge适合什么场合用？HyperLogLog 除了上面的 pfadd 和 pfcount 之外，还提供了第三个指令 pfmerge，用于将多个 pf 计数值累加在一起形成一个新的 pf 值。比如在网站中我们有两个内容差不多的页面，运营说需要这两个页面的数据进行合并。其中页面的 UV 访问量也需要合并，那这个时候 pfmerge 就可以派上用场了。","link":"/2021/06/09/26000REDIS/2.6.4HyperLogLog/"},{"title":"","text":"布隆过滤器​ 上一节我们学会了使用 HyperLogLog 数据结构来进行估数，它非常有价值，可以解决很多精确度不高的统计需求。 ​ 但是如果我们想知道某一个值是不是已经在 HyperLogLog 结构里面了，它就无能为力了，它只提供了 pfadd 和 pfcount 方法，没有提供 pfcontains 这种方法。 ​ 布隆过滤器 (Bloom Filter) 闪亮登场了，它就是专门用来解决这种去重问题的。它在起到去重的同时，在空间上还能节省 90% 以上，只是稍微有那么点不精确，也就是有一定的误判概率。 布隆过滤器是什么？​ 布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。 ​ 当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。打个比方，当它说不认识你时，肯定就不认识；当它说见过你时，可能根本就没见过面，不过因为你的脸跟它认识的人中某脸比较相似 (某些熟脸的系数组合)，所以误判以前见过你。","link":"/2021/06/24/26000REDIS/2.6.5%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"title":"","text":"主从同步[1.CAP原理](#1.CAP 原理)2.最终一致3.主从同步4.增量同步5.快照同步6.增加从节点7.无盘复制8.Wait指令9.小结[10.集群1—— Sentinel](#10.集群1—— Sentinel)11.消息丢失12.Sentinel基本使用 很多企业都没有使用到 Redis 的集群，但是至少都做了主从。有了主从，当 master 挂掉的时候，运维让从库过来接管，服务就可以继续，否则 master 需要经过数据恢复和重启的过程，这就可能会拖很长的时间，影响线上业务的持续服务。 在了解 Redis 的主从复制之前，让我们先来理解一下现代分布式系统的理论基石——CAP 原理。 1.CAP 原理CAP 原理就好比分布式领域的牛顿定律，它是分布式存储的理论基石。自打 CAP 的论文发表之后，分布式存储中间件犹如雨后春笋般一个一个涌现出来。理解这个原理其实很简单，本节我们首先对这个原理进行一些简单的讲解。 C - Consistent ，一致性A - Availability ，可用性P - Partition tolerance ，分区容忍性 分布式系统的节点往往都是分布在不同的机器上进行网络隔离开的，这意味着必然会有网络断开的风险，这个网络断开的场景的专业词汇叫着「网络分区」。在网络分区发生时，两个分布式节点之间无法进行通信，我们对一个节点进行的修改操作将无法同步到另外一个节点，所以数据的「一致性」将无法满足，因为两个分布式节点的数据不再保持一致。除非我们牺牲「可用性」，也就是暂停分布式节点服务，在网络分区发生时，不再提供修改数据的功能，直到网络状况完全恢复正常再继续对外提供服务。 一句话概括 CAP 原理就是——网络分区发生时，一致性和可用性两难全。 2.最终一致​ Redis 的主从数据是异步同步的，所以分布式的 Redis 系统并不满足「一致性」要求。当客户端在 Redis 的主节点修改了数据后，立即返回，即使在主从网络断开的情况下，主节点依旧可以正常对外提供修改服务，所以 Redis 满足「可用性」。​ Redis 保证「最终一致性」，从节点会努力追赶主节点，最终从节点的状态会和主节点的状态将保持一致。如果网络断开了，主从节点的数据将会出现大量不一致，一旦网络恢复，从节点会采用多种策略努力追赶上落后的数据，继续尽力保持和主节点一致。 3.主从同步Redis 同步支持主从同步和从从同步，从库同步功能是 Redis 后续版本增加的功能，为了减轻主库的同步负担。后面为了描述上的方便，统一理解为主从同步。 4.增量同步Redis 同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一遍向主节点反馈自己同步到哪里了 (偏移量)。因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer中。Redis 的复制内存 buffer 是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。 如果因为网络状况不好，从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis 的主节点中那些没有同步的指令在 buffer 中有可能已经被后续的指令覆盖掉了，从节点将无法直接通过指令流来进行同步，这个时候就需要用到更加复杂的同步机制 — — 快照同步。 5.快照同步快照同步是一个非常耗费资源的操作，它首先需要在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空。加载完毕后通知主节点继续进行增量同步。 在整个快照同步进行的过程中，主节点的复制 buffer 还在不停的往前移动，如果快照同步的时间过长或者复制 buffer 太小，都会导致同步期间的增量指令在复制 buffer 中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。 所以务必配置一个合适的复制 buffer 大小参数，避免快照复制的死循环。 6.增加从节点当从节点刚刚加入到集群时，它必须先要进行一次快照同步，同步完成后再继续进行增量同步。 7.无盘复制主节点在进行快照同步时，会进行很重的文件 IO 操作，特别是对于非 SSD 磁盘存储时，快照会对系统的负载产生较大影响。特别是当系统正在进行 AOF 的 fsync 操作时如果发生快照，fsync 将会被推迟执行，这就会严重影响主节点的服务效率。 所以从 Redis 2.8.18 版开始支持无盘复制。所谓无盘复制是指主服务器直接通过套接字将快照内容发送到从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一遍将序列化的内容发送到从节点，从节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。 8.Wait指令Redis 的复制是异步进行的，wait 指令可以让异步复制变身同步复制，确保系统的强一致性 (不严格)。wait 指令是 Redis3.0 版本以后才出现的。 1234&gt; set key valueOK&gt; wait 1 0(integer) 1 wait 提供两个参数，第一个参数是从库的数量 N，第二个参数是时间 t，以毫秒为单位。它表示等待 wait 指令之前的所有写操作同步到 N 个从库 (也就是确保 N 个从库的同步没有滞后)，最多等待时间 t。如果时间 t=0，表示无限等待直到 N 个从库同步完成达成一致。 假设此时出现了网络分区，wait 指令第二个参数时间 t=0，主从同步无法继续进行，wait 指令会永远阻塞，Redis 服务器将丧失可用性。 9.小结主从复制是 Redis 分布式的基础，Redis 的高可用离开了主从复制将无从进行。后面的章节我们会开始讲解 Redis 的集群模式，这几种集群模式都依赖于本节所讲的主从复制。不过复制功能也不是必须的，如果你将 Redis 只用来做缓存，跟 memcache 一样来对待，也就无需要从库做备份，挂掉了重新启动一下就行。但是只要你使用了 Redis 的持久化功能，就必须认真对待主从复制，它是系统数据安全的基础保障。","link":"/2021/05/31/26000REDIS/2.6.8%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"title":"","text":"集群1—— Sentinel目前我们讲的 Redis 还只是主从方案，最终一致性。读者们可思考过，如果主节点凌晨3 点突发宕机怎么办？就坐等运维从床上爬起来，然后手工进行从主切换，再通知所有的程序把地址统统改一遍重新上线么？毫无疑问，这样的人工运维效率太低，事故发生时估计得至少 1 个小时才能缓过来。如果是一个大型公司，这样的事故足以上新闻了。 所以我们必须有一个高可用方案来抵抗节点故障，当故障发生时可以自动进行从主切换，程序可以不用重启，运维可以继续睡大觉，仿佛什么事也没发生一样。Redis 官方提供了这样一种方案 —— Redis Sentinel(哨兵)。 我们可以将 Redis Sentinel 集群看成是一个 ZooKeeper 集群，它是集群高可用的心脏，它一般是由 3～5 个节点组成，这样挂了个别节点集群还可以正常运转。它负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将最新的主节点地址告诉客户端。如此应用程序将无需重启即可自动完成节点切换。比如上图的主节点挂掉后，集群将可能自动调整为下图所示结构。 从这张图中我们能看到主节点挂掉了，原先的主从复制也断开了，客户端和损坏的主节点也断开了。从节点被提升为新的主节点，其它从节点开始和新的主节点建立复制关系。客户端通过新的主节点继续进行交互。Sentinel 会持续监控已经挂掉了主节点，待它恢复后，集群会调整为下面这张图。 此时原先挂掉的主节点现在变成了从节点，从新的主节点那里建立复制关系。 1.消息丢失Redis 主从采用异步复制，意味着当主节点挂掉时，从节点可能没有收到全部的同步消息，这部分未同步的消息就丢失了。如果主从延迟特别大，那么丢失的数据就可能会特别多。Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。它有两个选项可以限制主从延迟过大。 min-slaves-to-write 1 min-slaves-max-lag 10 第一个参数表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务，丧失可用性。何为正常复制，何为异常复制？这个就是由第二个参数控制的，它的单位是秒，表示如果 10s 没有收到从节点的反馈，就意味着从节点同步不正常，要么网络断开了，要么一直没有给反馈。 2.Sentinel基本使用接下来我们看看客户端如何使用 sentinel，标准的流程应该是客户端可以通过 sentinel发现主从节点的地址，然后在通过这些地址建立相应的连接来进行数据存取操作。我们来看看 Python 客户端是如何做的。 123456&gt;&gt;&gt; from redis.sentinel import Sentinel&gt;&gt;&gt; sentinel = Sentinel([('localhost', 26379)], socket_timeout=0.1)&gt;&gt;&gt; sentinel.discover_master('mymaster')('127.0.0.1', 6379)&gt;&gt;&gt; sentinel.discover_slaves('mymaster')[('127.0.0.1', 6380)] sentinel 的默认端口是 26379，不同于 Redis 的默认端口 6379，通过 sentinel 对象的 discover_master discover_slaves 方法可以发现主从地址，主地址只有一个，从地址可以有多个。 12345&gt;&gt;&gt; master = sentinel.master_for('mymaster', socket_timeout=0.1)&gt;&gt;&gt; slave = sentinel.slave_for('mymaster', socket_timeout=0.1)&gt;&gt;&gt; master.set('foo', 'bar')&gt;&gt;&gt; slave.get('foo')'bar' 通过 xxx_for 方法可以从连接池中拿出一个连接来使用，因为从地址有多个，redis 客户端对从地址采用轮询方案，也就是 RoundRobin 轮着来。 有个问题是，但 sentinel 进行主从切换时，客户端如何知道地址变更了 ? 通过分析源码，我发现 redis-py 在建立连接的时候进行了主库地址变更判断。连接池建立新连接时，会去查询主库地址，然后跟内存中的主库地址进行比对，如果变更了，就断开所有连接，重新使用新地址建立新连接。如果是旧的主库挂掉了，那么所有正在使用的连接都会被关闭，然后在重连时就会用上新地址。 但是这样还不够，如果是 sentinel 主动进行主从切换，主库并没有挂掉，而之前的主库连接已经建立了在使用了，没有新连接需要建立，那这个连接是不是一致切换不了？继续深入研究源码，我发现 redis-py 在另外一个点也做了控制。那就是在处理命令的时候捕获了一个特殊的异常 ReadOnlyError，在这个异常里将所有的旧连接全部关闭了，后续指令就会进行重连。主从切换后，之前的主库被降级到从库，所有的修改性的指令都会抛出 ReadonlyError。 如果没有修改性指令，虽然连接不会得到切换，但是数据不会被破坏，所以即使不切换也没关系。 作业 1、尝试自己搭建一套 redis-sentinel 集群； 2、使用 Python 或者 Java 的客户端对集群进行一些常规操作； 3、试试主从切换，主动切换和被动切换都试一试，看看客户端能否正常切换连接；","link":"/2021/05/31/26000REDIS/2.6.9%E9%9B%86%E7%BE%A41-Sentinel/"},{"title":"","text":"[TOC] 三种过期策略： 定时删除 含义：在设置key的过期时间的同时，为该key创建一个定时器，让定时器在key的过期时间来临时，对key进行删除 优点：保证内存被尽快释放 缺点：若过期key很多，删除这些key会占用很多的CPU时间，在CPU时间紧张的情况下，CPU不能把所有的时间用来做要紧的事儿，还需要去花时间删除这些key定时器的创建耗时，若为每一个设置过期时间的key创建一个定时器（将会有大量的定时器产生），性能影响严重 懒汉式删除 含义：key过期的时候不删除，每次通过key获取值的时候去检查是否过期，若过期，则删除，返回null（用的时候再检查 删除）。 优点：删除操作只发生在通过key取值的时候发生，而且只删除当前key，所以对CPU时间的占用是比较少的，而且此时的删除是已经到了非做不可的地步（如果此时还不删除的话，我们就会获取到了已经过期的key了） 缺点：若大量的key在超出超时时间后，很久一段时间内，都没有被获取过，那么可能发生内存泄露（无用的垃圾占用了大量的内存） 定期删除 含义：每隔一段时间执行一次删除过期key操作 优点：通过限制删除操作的时长和频率，来减少删除操作对CPU时间的占用–处理”定时删除”的缺点定期删除过期key–处理”懒汉式删除”的缺点缺点：在内存友好方面，不如”定时删除”（会造成一定的内存占用，但是没有懒汉式那么占用内存） 在CPU时间友好方面，不如”懒汉式删除”（会定期的去进行比较和删除操作，cpu方面不如懒汉式，但是比定时好） 难点：合理设置删除操作的执行时长（每次删除执行多长时间）和执行频率（每隔多长时间做一次删除）（这个要根据服务器运行情况来定了），每次执行时间太长，或者执行频率太高对cpu都是一种压力。每次进行定期删除操作执行之后，需要记录遍历循环到了哪个标志位，以便下一次定期时间来时，从上次位置开始进行循环遍历","link":"/2022/03/11/26000REDIS/26001%E6%9D%82/"},{"title":"","text":"[TOC] Redis 线程IO模型Redis 是个单线程程序！这点必须铭记。也许你会怀疑高并发的 Redis 中间件怎么可能是单线程。很抱歉，它就是单线程，你的怀疑暴露了你基础知识的不足。莫要瞧不起单线程，除了 Redis 之外，Node.js 也是单线程，Nginx 也是单线程，但是它们都是服务器高性能的典范。 Redis 单线程为什么还能这么快？因为它所有的数据都在内存中，所有的运算都是内存级别的运算。正因为 Redis 是单线程，所以要小心使用 Redis 指令，对于那些时间复杂度为 O(n) 级别的指令，一定要谨慎使用，一不小心就可能会导致 Redis 卡顿。 Redis 单线程如何处理那么多的并发客户端连接？这个问题，有很多中高级程序员都无法回答，因为他们没听过多路复用这个词汇，不知道 select 系列的事件轮询 API，没用过非阻塞 IO。 非阻塞 IO当我们调用套接字的读写方法，默认它们是阻塞的，比如 read 方法要传递进去一个参数n，表示读取这么多字节后再返回，如果没有读够线程就会卡在那里，直到新的数据到来或者连接关闭了，read 方法才可以返回，线程才能继续处理。而 write 方法一般来说不会阻塞，除非内核为套接字分配的写缓冲区已经满了，write 方法就会阻塞，直到缓存区中有空闲空间挪出来了。 ​ 非阻塞 IO 在套接字对象上提供了一个选项 Non_Blocking，当这个选项打开时，读写方法不会阻塞，而是能读多少读多少，能写多少写多少。能读多少取决于内核为套接字分配的读缓冲区内部的数据字节数，能写多少取决于内核为套接字分配的写缓冲区的空闲空间字节数。读方法和写方法都会通过返回值来告知程序实际读写了多少字节。 ​ 有了非阻塞 IO 意味着线程在读写 IO 时可以不必再阻塞了，读写可以瞬间完成然后线程可以继续干别的事了。 事件轮询(多路复用)非阻塞 IO 有个问题，那就是线程要读数据，结果读了一部分就返回了，线程如何知道何时才应该继续读。也就是当数据到来时，线程如何得到通知。写也是一样，如果缓冲区满了，写不完，剩下的数据何时才应该继续写，线程也应该得到通知。 事件轮询 API 就是用来解决这个问题的，最简单的事件轮询 API 是 select 函数，它是操作系统提供给用户程序的 API。输入是读写描述符列表 read_fds &amp; write_fds，输出是与之对应的可读可写事件。同时还提供了一个 timeout 参数，如果没有任何事件到来，那么就最多等待 timeout 时间，线程处于阻塞状态。一旦期间有任何事件到来，就可以立即返回。时间过了之后还是没有任何事件到来，也会立即返回。拿到事件后，线程就可以继续挨个处理相应的事件。处理完了继续过来轮询。于是线程就进入了一个死循环，我们把这个死循环称为事件循环，一个循环为一个周期。 每个客户端套接字 socket 都有对应的读写文件描述符。 123456read_events, write_events = select(read_fds, write_fds, timeout)for event in read_events: handle_read(event.fd)for event in write_events: handle_write(event.fd)handle_others() # 处理其它事情，如定时任务等 因为我们通过 select 系统调用同时处理多个通道描述符的读写事件，因此我们将这类系统调用称为多路复用 API。现代操作系统的多路复用 API 已经不再使用 select 系统调用，而改用 epoll(linux)和 kqueue(freebsd &amp; macosx)，因为 select 系统调用的性能在描述符特别多时性能会非常差。它们使用起来可能在形式上略有差异，但是本质上都是差不多的，都可以使用上面的伪代码逻辑进行理解。 服务器套接字 serversocket 对象的读操作是指调用 accept 接受客户端新连接。何时有新连接到来，也是通过 select 系统调用的读事件来得到通知的。 事件轮询 API 就是 Java 语言里面的 NIO 技术 Java 的 NIO 并不是 Java 特有的技术，其它计算机语言都有这个技术，只不过换了一个词汇，不叫 NIO 而已。 指令队列Redis 会将每个客户端套接字都关联一个指令队列。客户端的指令通过队列来排队进行顺序处理，先到先服务。 响应队列Redis 同样也会为每个客户端套接字关联一个响应队列。Redis 服务器通过响应队列来将指令的返回结果回复给客户端。 如果队列为空，那么意味着连接暂时处于空闲状态，不需要去获取写事件，也就是可以将当前的客户端描述符从 write_fds 里面移出来。等到队列有数据了，再将描述符放进去。避免 select 系统调用立即返回写事件，结果发现没什么数据可以写出。这种情况的线程会飙高 CPU。 定时任务服务器处理要响应 IO 事件外，还要处理其它事情。比如定时任务就是非常重要的一件事。如果线程阻塞在 select 系统调用上，定时任务将无法得到准时调度。那 Redis 是如何解决这个问题的呢？ Redis 的定时任务会记录在一个称为最小堆的数据结构中。这个堆中，最快要执行的任务排在堆的最上方。在每个循环周期，Redis 都会将最小堆里面已经到点的任务立即进行处理。处理完毕后，将最快要执行的任务还需要的时间记录下来，这个时间就是 select 系统调用的 timeout 参数。因为 Redis 知道未来 timeout 时间内，没有其它定时任务需要处理，所以可以安心睡眠 timeout 的时间。 Nginx 和 Node 的事件处理原理和 Redis 也是类似的","link":"/2021/05/21/26000REDIS/2%E7%BA%BF%E7%A8%8BIO%E6%A8%A1%E5%9E%8B/"},{"title":"","text":"Redis可以做什么？ 记录帖子的点赞数、评论数和点击数 (hash)。 记录用户的帖子 ID 列表 (排序)，便于快速显示用户的帖子列表 (zset)。 记录帖子的标题、摘要、作者和封面信息，用于列表页展示 (hash)。 记录帖子的点赞用户 ID 列表，评论 ID 列表，用于显示和去重计数 (zset)。 缓存近期热帖内容 (帖子内容空间占用比较大)，减少数据库压力 (hash)。 记录帖子的相关文章 ID，根据内容推荐相关帖子 (list)。 如果帖子 ID 是整数自增的，可以使用 Redis 来分配帖子 ID(计数器)。 收藏集和帖子之间的关系 (zset)。 记录热榜帖子 ID 列表，总热榜和分类热榜 (zset)。 缓存用户行为历史，进行恶意行为过滤 (zset,hash)。 Redis 有 5 种基础数据结构 string (字符串)： 当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是字符串最大长度为 512M。 list (列表)： set (集合) hash (哈 希) zset (有序集合)。 123456789101112131415161718192021222324# 过期和 set 命令扩展&gt; setex name 5 codename # 5s 后过期，等价于 set+expire&gt; get name &quot;codename&quot;... # wait for 5s &gt; get name (nil)&gt; setnx name codename # 如果 name 不存在就执行 set 创建(integer) 1 &gt; get name &quot;codename&quot; &gt; setnx name codename (integer) 0 # 因为 name 已经存在，所以 set 创建不成功# 计数# 如果 value 值是一个整数，还可以对它进行自增操作。自增是有范围的，它的范围是signed long 的最大最小值，超 过了这个值，Redis 会报错。&gt; set codename 9223372036854775807 # Long.Max OK &gt; incr codename(error) ERR increment or decrement would overflow","link":"/2021/01/25/26000REDIS/3.Redis%E5%8F%AF%E4%BB%A5%E5%81%9A%E4%BB%80%E4%B9%88/"},{"title":"","text":"事务执行 订阅发布 主从复制 哨兵模式 Spring Boot Spring Cloud 位运算 12345678$ setbit year_record 1 1$ setbit year_record 2 1$ setbit year_record 4 1 // year_record 01101000$ get year_record&gt; &quot;h&quot; // 返回字符串h$ bitcount year_record 0 8 // 统计0-8位1的数量&gt; (integer) 3","link":"/2021/05/21/26000REDIS/4.Redis/"},{"title":"","text":"线程安全的机制线程表示一条单独的执行流，每个线程都有自己的执行计数器，有自己的栈，但可以共享内存。共享内存有2个问题：竞态条件、内存可见性 1.synchronized2.使用显式锁3.使用volatile4.使用原子变量和CAS5.写时复制6.ThreadLocal 显式锁是相对synchronized隐式锁而言的，需要自己创建锁，主要实现类ReentrantLock。相比synchronized，显示锁支持以非阻塞方式获取锁，可以响应中断、限时、指定公平性、解决死锁问题。在读多写少、读并发情景中，可以通过读写锁提高并发度，读写锁接口ReadWriteLock volatile。保证安全不一定需要锁，共享对象只有一个，操作也只是进行最简单的get、set操作，就不存在竞态条件问题，只有内存可见性问题。这时在变量声明上加上volatile就可以了。 写时复制之所以会有线程安全的问题，是因为多个线程并发读写同一个对象，如果每个线程读写的对象都是不同的，或者，如果共享访问的对象是只读的，不能修改，那也就不存在线程安全问题了。我们在介绍容器类CopyOnWriteArrayList和CopyOnWriteArraySet时介绍了写时复制技术，写时复制就是将共享访问的对象变为只读的，写的时候，再使用锁，保证只有一个线程写，写的线程不是直接修改原对象，而是新创建一个对象，对该对象修改完毕后，再原子性地修改共享访问的变量，让它指向新的对象。 20.2线程的协作机制wail/notify显示条件线程的中断协作工具类阻塞队列Future/FutureTask 23.1 静态代理23.2 JAVA SDK动态代理1234567891011121314151617181920212223242526272829public class SimpleJDKDynamicProxyDemo {￼ static interface IService {￼ public void sayHello();￼ } static class RealService implements IService {￼ @Override￼ public void sayHello() {￼ System.out.println(&quot;hello&quot;);￼ }￼ }￼ static class SimpleInvocationHandler implements InvocationHandler {￼ private Object realObj;￼ public SimpleInvocationHandler(Object realObj) {￼ this.realObj = realObj;￼ }￼ @Override￼ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {￼ System.out.println(&quot;entering &quot; + method.getName());￼ Object result = method.invoke(realObj, args);￼ System.out.println(&quot;leaving &quot; + method.getName());￼ return result;￼ }￼ }￼ public static void main(String[] args) { IService realService = new RealService();￼ IService proxyService = (IService) Proxy.newProxyInstance(￼IService.class.getClassLoader(), new Class&lt;? &gt;[] { IService.class },￼ new SimpleInvocationHandler(realService));￼ proxyService.sayHello(); }} 23.3 cglib动态代理Java SDK动态代理的局限在于，它只能为接口创建代理，返回的代理对象也只能转换到某个接口类型，如果一个类没有接口，或者希望代理非接口中定义的方法，那就没有办法了。有一个第三方的类库cglib（https://github.com/cglib/cglib），可以做到这一点，Spring、Hibernate等都使用该类库。我们看个简单的例子，如代码清单23-5所示。 1234567891011121314151617181920212223242526public class SimpleCGLibDemo {￼ static class RealService {￼ public void sayHello() {￼ System.out.println(&quot;hello&quot;);￼ }￼ }￼ static class SimpleInterceptor implements MethodInterceptor {￼ @Override￼ public Object intercept(Object object, Method method,￼ Object[] args, MethodProxy proxy) throws Throwable { ￼ System.out.println(&quot;entering &quot; + method.getName());￼ Object result = proxy.invokeSuper(object, args);￼ System.out.println(&quot;leaving &quot; + method.getName());￼ return result;￼ }￼ }￼ private static &lt;T&gt; T getProxy(Class&lt;T&gt; cls) {￼ Enhancer enhancer = new Enhancer();￼ enhancer.setSuperclass(cls);￼ enhancer.setCallback(new SimpleInterceptor());￼ return (T) enhancer.create();￼ }￼ public static void main(String[] args) throws Exception {￼ RealService proxyService = getProxy(RealService.class);￼ proxyService.sayHello();￼ }￼} RealService表示被代理的类，它没有接口。getProxy()为一个类生成代理对象，这个代理对象可以安全地转换为被代理类的类型，它使用了cglib的Enhancer类。Enhancer类的setSuperclass设置被代理的类，setCallback设置被代理类的public非final方法被调用时的处理类。Enhancer支持多种类型，这里使用的类实现了MethodInterceptor接口，它与Java SDK中的InvocationHandler有点类似，方法名称变成了intercept，多了一个MethodProxy类型的参数。与前面的InvocationHandler不同，SimpleInterceptor中没有被代理的对象，它通过MethodProxy的invokeSuper方法调用被代理类的方法：￼ Object result = proxy.invokeSuper(object, args);注意，它不能这样调用被代理类的方法：￼ Object result = method.invoke(object, args); object是代理对象，调用这个方法还会调用到SimpleInterceptor的intercept方法，造成死循环。在main方法中，我们也没有创建被代理的对象，创建的对象直接就是代理对象。cglib的实现机制与Java SDK不同，它是通过继承实现的，它也是动态创建了一个类，但这个类的父类是被代理的类，代理类重写了父类的所有public非final方法，改为调用Callback中的相关方法，在上例中，调用SimpleInterceptor的intercept方法。 23.4 Java SDK代理与cglib代理比较Java SDK代理面向的是一组接口，它为这些接口动态创建了一个实现类。接口的具体实现逻辑是通过自定义的InvocationHandler实现的，这个实现是自定义的，也就是说，其背后都不一定有真正被代理的对象，也可能有多个实际对象，根据情况动态选择。cglib代理面向的是一个具体的类，它动态创建了一个新类，继承了该类，重写了其方法。从代理的角度看，Java SDK代理的是对象，需要先有一个实际对象，自定义的InvocationHandler引用该对象，然后创建一个代理类和代理对象，客户端访问的是代理对象，代理对象最后再调用实际对象的方法；cglib代理的是类，创建的对象只有一个。如果目的都是为一个类的方法增强功能，Java SDK要求该类必须有接口，且只能处理接口中的方法，cglib没有这个限制。 12.1 抽象容器类抽象容器类与之前介绍的接口和具体容器类的关系如图12-1所示。虚线框表示接口，有Collection、List、Set、Queue、Deque和Map。有6个抽象容器类。1）AbstractCollection：实现了Collection接口，被抽象类AbstractList、AbstractSet、AbstractQueue继承，ArrayDeque也继承自AbstractCollection（图中未画出）。2）AbstractList：父类是AbstractCollection，实现了List接口，被ArrayList、Abstract-SequentialList继承。","link":"/2021/07/23/3.%E8%AF%BB%E4%B9%A6/JAVA%E7%BC%96%E7%A8%8B%E9%80%BB%E8%BE%91/"},{"title":"Kubernetes从入门到实践","text":"Kubernetes从入门到实践 操作系统和硬件之间加入一个虚拟机监控程序（hypervisor），可以实现虚拟化系统。 1.k8s架构与工作流程及核心概念2.k8s主要功能与特性。工作负载对象（Pod与控制器）的使用、应用部署、网络、服务发现、负载均衡、存储、配置、资源管理、资源调度 3.k8s配置、管理及扩展k8s的API Server、授权与安全、可视化管理，讨论如何扩展k8s的功能 4.项目示例，何如使用Helm部署项目","link":"/2022/05/08/3.%E8%AF%BB%E4%B9%A6/Kubernetes%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/"},{"title":"","text":"1.1Spring的整体框架1.1.1Core Container 1. Core、Beans、Context和Expression Language 1. Beans 1. Context 4. Expression Language 设置获取属性的值，属性的分配，方法的调用，访问数组上下文 1.1.2Data Access/Integration 1. JDBC模块提供了JDBC抽象层 1. ORM模块为流行的对象-关系映射API，如JPA、Hibernate、iBatis。Spring框架插入了若干个ORM框架，从而提供了ORM的对象关系工具，其中包含JDO、Hibernate和IBatisSQL Map。 1. OXM模块提供了一个对Objext、XML隐射实现的抽象层，隐射包括JAXB、Castor、XMLBeans、JiBX和XStream。 1. JMS Java Messaging Service模块提供了制造和消费消息的特性。 1. Transaction模块支持编程和声明性的事务管理，这些事务类必须实现特定的接口，并且对所有POJO都适用。 1.1.3Web","link":"/2021/12/28/3.%E8%AF%BB%E4%B9%A6/Spring%E6%BA%90%E7%A0%81/"},{"title":"","text":"不可重复读的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。 幻读问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录，我们把新插入的那些记录称之为幻影记录。 怎么解决脏读、不可重复读、幻读这些问题呢？其实有两种可选的解决方案： 方案一：读操作利用多版本并发控制（MVCC），写操作进行加锁 方案二：读、写操作都采用加锁的方式。 用MVCC方式的话，读-写操作彼此并不冲突，性能更高， 采用加锁方式的话，读-写操作彼此需要排队执行，影响性能 锁定读的语句对读取的记录加S锁： SELECT ... LOCK IN SHARE MODE; 对读取的记录加X锁： SELECT ... FOR UPDATE; 我们在对教学楼整体上锁（表锁）时，怎么知道教学楼中有没有教室已经被上锁（行锁）了呢？依次检查每一间教室门口有没有上锁？那这效率也太慢了吧！遍历是不可能遍历的，这辈子也不可能遍历的，于是乎设计InnoDB的大叔们提出了一种称之为意向锁（英文名：Intention Locks）的东东： 意向共享锁，英文名：Intention Shared Lock，简称IS锁。当事务准备在某条记录上加S锁时，需要先在表级别加一个IS锁。 意向独占锁，英文名：Intention Exclusive Lock，简称IX锁。当事务准备在某条记录上加X锁时，需要先在表级别加一个IX锁。","link":"/2021/07/30/3.%E8%AF%BB%E4%B9%A6/Mysql%E6%A0%B9/"},{"title":"","text":"夯实基础《深入理解计算机系统》–700页《计算机网络》–500页（可能节选着看，不会看全部）《Java编程思想》（重读，看的会稍微快一些） –800页技术架构《可伸缩服务架构》–550页《大型网站技术架构》–200页《大型网站系统与Java中间件实践》–350页《亿级流量网站架构核心技术》–450页深入Java《Java性能优化权威指南》–550页《Java性能权威指南》–300页《揭秘Java虚拟机》–700页《EffectiveJava》第三版（重读，看的会稍微快一些）–300页《重构 改善既有代码的设计》–400页《MyBatis技术内幕》–400页《Java并发编程之美》–350页","link":"/2021/12/10/3.%E8%AF%BB%E4%B9%A6/all/"},{"title":"","text":"分布式事务","link":"/2021/11/25/3.%E8%AF%BB%E4%B9%A6/%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84/"},{"title":"职场上如何管理时间","text":"职场上如何管理时间计划：规划和备忘做好计划表，拆分子任务和设定截止时间。 行动：把时间变成产出转：转给别人做：马上去做微信名片备注，什么时候认识，在哪认识，谁介绍 一个会议结束，马上把会议纪要整理出来，立刻发出去 存：放入代办清单扔：拒绝或者忽略它放弃无关紧要的指标、删掉某个无足轻重的邮件、退掉某次无所谓的交流 拖延：担心自己做不好的心态造成的敏捷工作法：让计划赶上变化建立“最小可交付”意识你在工作中投入了大量的精力和时间，但没有交付的动作，在旁人看来，你等于什么都没做。 当你接到领导派活时，更加敏捷的回答方式是”明天上午我就开始去准备，下班前给您一个初步反馈”，而不是“两周后给您最终方案”。 这样的好处在于，一方面，你源源不断地又产出。和你合作的伙伴，也始终处于一个合理的忙碌状态。而不是所有人在等你的时间，万一你憋了半天，出来的东西不是他们想要的，整体效率会被你拖累。 管理合作者的时间三个透明沟通习惯：在做计划的时候，就把别人拉进来。其次是实时同步，每天上午花个十分钟时间，站着开会同步手上的工作。第三尽可能地当面沟通。 管理上级的时间很多时候你的老板就不善于管理时间。老板布置工作总是想一出是一出。要解决这一类问题，核心就是“敢于去和老板一起管理时间”。每天下午四五点主动问一下老板，有没有什么需要帮忙的，可能就是老板忙忘了。人都有“向上负责的”心态，花在下属身上的注意力天然是不足的。 而且管理者往往有一种微妙的心态，那就是：让下属忙起来，显得自己的部门很重要。结果就是，一方面，老板对自己布置出去的任务没概念，又喜欢不断给下属加任务。如果下属不主动反馈手头事情的进度，老板对下属的工作量会产生误判。 另一方面，如果上级部署任务后，下属没有主动去汇报，一旦轮到上级想起来，开始盯细节、催进度，事后上级就会觉得，是这个下属执行力不行。 处理这种问题的好方法： 1.主动固定下来和老板一对一沟通的时间，放到他的日历表里2.利用项目管理表，定期和他汇报你的项目、任务的进度。在沟通中，可以就工作重点，请他进行一个优先级的排序。沟通时间不需要太长，15-30分钟就行。 上级对这种一起管理时间的做法非常欢迎。一方面他们知道下属是否把时间花在刀刃上，另一方面对部门工作的进展能够更容易看得清楚，不需要花精力去催进度。要记住，职场上要找事情做，是永远找得到的，关键在于，你有没有把时间花在重要的事情上。得到老板的支持，会让你的工作事半功倍。 作为管理者警惕2中思维模式： 1.这个事情下属做太费经，还不如自己做。或者他还没有达到那个水平，做不来的。下属不是因为成长了才能完成某个任务，恰恰相反，他是因为完成了某个任务才成长。 2.来自下属：他会对你说：老板这个事情你来决策一下吧。或者这个事情我做不来，要不你来做吧。当你听到下属需要你做事的时候，需要提醒自己：这件事的最终责任人，是他还是你？ 如果这件事的责任人属于你的下属，那么可以知道他、给出建议。但你必须坚定地告诉他，他才是这件事的最终负责人。 如何与忙碌相处避开“内卷化忙碌”工作非常忙，今天赶昨天的进度，一个人在干两个人的活。忙到没时间去想怎么改进方法、提高效率，或者忙到没时间做长期计划。很多高薪的行业是内卷化忙碌的重灾区。因为是高薪，所以暂时掩盖了自我提升的动力。 形成自驱式忙碌1.有属于自己的好目标。完成老板交代给你的；另一个是挑战自己想做的。 2.给成长留出足够的投入。类似于健身、学习充电这种不紧急，但是很重要的事。除了投入时间，还要敢于投入钱，投入让你心疼的钱。 3.跑出一个小循环，制造机会让自己尝到甜头。把学到的某个技能，立刻运用到工作中。","link":"/2022/05/10/3.%E8%AF%BB%E4%B9%A6/%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%97%B6%E9%97%B4/"},{"title":"","text":"实体、领域服务、限界上下文 学习路线图• 了解DDD可以为你的项目和团队带来哪些好处• 如何确定你的项目是否适合采用DDD• 了解DDD的常见替代方案和它们将导致问题的原因• 学习DDD的基础• 学习如何向你的管理层、领域专家和技术成员推销DDD• 了解使用DDD时所面临的挑战• 看看一个正在学习采用DDD的团队是如何工作的","link":"/2021/12/20/3.%E8%AF%BB%E4%B9%A6/%E5%AE%9E%E7%8E%B0%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"},{"title":"","text":"","link":"/2021/09/22/3.%E8%AF%BB%E4%B9%A6/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%88%98/"},{"title":"","text":"123456789101112131415ganttdateFormat YYYY-MM-DDtitle 深入理解JAVA虚拟机 section 二、内存管理机制 内存 : done, 2021-03-26, 1d 内存 : 2021-03-27, 1d section 三、虚拟机执行子系统 Completed task in the critical line : 2021-04-10,24h Implement parser and json :crit, done, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to ,mermaid :1d","link":"/2021/03/26/3.%E8%AF%BB%E4%B9%A6/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JAVA%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"title":"","text":"木兰词-纳兰性德 人生若只如初见，何事秋风悲画扇。 等闲变却故人心，却道故人心易变。 骊山语罢清宵半，泪雨霖铃终不怨。 何如薄幸锦衣郎，比翼连枝当日愿。","link":"/2021/12/20/3.%E8%AF%BB%E4%B9%A6/%E8%AF%97/"},{"title":"","text":"[TOC] 趣味逻辑游戏001座位与方向​ 甲的左边是乙，乙的左边的丙，丙的左边是丁。请问：丁在甲的哪一边？ 002谁是弟弟​ 在一场百米赛跑中，丽丽的弟弟得了倒数第一名，他告诉丽丽这样的情况：​ （1）丙没有获得第一名；​ （2）戊比丁高了两个名次，但戊不是第二名；​ （3）甲不是第一名也不是最后一句；​ （4）丙比乙高了一个名次。你能判断出，在甲、乙、丙、丁、戊中谁是丽丽的弟弟吗？ 003职业​ 一次聚会中，海伦遇到了甲、乙和丙三人。她想知道他们三人分别是干什么的，但三人只提供了以下信息：三人中一位是律师，一位是推销员，一位是医生。丙比医生年龄大，甲和推销员不同岁，推销员比乙年龄小。​ 根据上述信息海伦可以推出什么结论？ 004扑克的花色杰克在和他的朋友玩扑克。杰克手上拿到了13张牌。黑桃、红桃、梅花、方块这四种图案都至少有一张以上，但是，每种图案的张数都不一样。黑桃跟红桃的张数合计一共是6张。黑桃跟方块的张数合计一共是5张。杰克手中有一种相同花色的扑克牌是2张。请问，只有2张牌的花色是什么？ 005扒手是谁？派出所民警讯问公共汽车上的一桩盗窃案的嫌疑人甲、乙、丙、丁，笔录如下：甲说：“反正不是我干的。”乙说：“是丁干的。”丙主：“是乙干的。”丁说：“乙是诬陷。”他们当中有三人说真话，且扒手只有一个。那么，你能根据四人的口供推断出扒手是谁吗？ 006谁是冠军去年夏天，有兄弟三人分别参加了三项体育竞赛，即体操、撑杆跳和马拉松。已知的情况如下：老大没去参加马拉松比赛；老三没有参加体操比赛项目；在体操比赛中获得全能冠军称号的那个孩子，没有撑杆跳；马拉松冠军并非老三。你猜三兄弟中谁是体操全能冠军？ 007财主的女儿一个财主有四个女儿，她们都有很多的衣服。现在知道：小女儿的衣服比三女儿的多；大女儿的衣服和二女儿的衣服数量加在一起与三女儿的衣服和小女儿的衣服加在一起恰好一样多；二女儿和三女儿的衣服加在一起，比大女儿和小女儿的衣服加在一起要多。那么，你能判断出谁的衣服最多吗？谁的衣服第二多？ 008难解的血缘关系甲、乙和丙间有血缘关系，而且他们之间没有违背道德伦理的问题。现在只知道他们当中有甲的父亲、乙唯一的女儿和丙的同胞手足。但是丙的同胞手足既不是甲的父亲也不是乙的女儿。你知道他们当中哪一位与其他两人性别不同吗？ 009取玻璃球甲口袋放有N个白玻璃球和M个玻璃黑球，乙口袋中放有足够的黑玻璃球。现每次从甲口袋中任取2个玻璃球放在外面。当被取出的2玻璃球同色时，需再从乙口袋中取一个黑玻璃球放回甲口袋；当取出的2玻璃球异色时，将取出的白玻璃球再放回甲口袋。最后，甲口袋中只剩两个球，问剩下一黑一白玻璃球的概率有多大？ 010盒子里的东西在桌子上放着四个盒子。每个盒子上都有一张纸条，分别写着一句话。A盒子上写着：所有的盒子里都有水果；B盒子上写着：本盒子里有香蕉；C盒子上写着：本盒子里没有梨；D盒子上写着：有些盒子里没有水果。如果这里只有一句话是真的，你能断定从哪个盒子里能拿出水果来吗？ 011走路的问题哥哥巴里经常领着弟弟巴顿散步，巴里总是牵着巴顿的手，两个人并排走。巴里走路迈的步子大一些，所以巴顿迈步子的速度快一些，巴里走两步时，巴顿可以走三步。这样两个人走路的速度是一样的。那么，如果两个人同时迈右脚开始走路，到他们同时迈走脚那一时刻，巴顿需要走多少步？ 012乌龟的年龄有4只小乌龟，年龄从1岁到4岁各不相同。它们中有两只说话了，无论谁说话，如果说得是关于比它大的乌龟的话，都是假话。乌龟甲说：“乌龟乙3岁。”乌龟丙说：“乌龟甲不是1岁。”你知道这4只小乌龟分别是几岁吗？ 013真实身份有一个美丽的女孩在河边洗澡，当她洗完后发现放在岸边的衣服被人偷了。关于这件事，受害者、旁观者，目击者和救助者各有说法。她们的说法如果是关于被害者的就是假的，如果是关于其他人的就是真的。请你根据她们的说法判定谁是受害者。玛亚：“凯瑞不是旁观者。”凯瑞：“希尔不是目击者。”波西：“玛亚不是救助者。”希尔：“凯瑞不是目击者。” 014小球的颜色晚会上，老师和同学做一个游戏，他拿出三个小球，其中两个是绿色的，一个是红色的。他叫了甲、乙两名同学，让他们背靠背站立，然后分别给两个人每人一个小球。看谁可以先猜着对方手中球的颜色。在发完球后，两个人先都没有说话，然后乙说：我知道了，甲手里拿的球是绿色的。你知道甲是怎么猜测出来的吗？ 015三个邻居甲、乙、丙三人是邻居，他们分别是工程师、部门经理和艺术家。三个人中，艺术家是独生子女，他的工资最少；丙是甲的妹妹的男朋友，他挣的钱比部门经理多。依据这些，你能判断三个人分别是什么职务吗？ 016聚会的难题文字逻辑游戏304在最近十年中，美国生产的半导体的数量增加了200%，而日本生产的数量增加了500%。因此，现在日本生产的半导体要比美国多。以下哪项，如果是真的，最能减弱以上的论证？A.在最近五年中，美国生产的半导体的数量仅增加了100%。B.在最近十年中，美国生产的半导体的价格要高于日本在同期生产的半导体的价格。C.目前美国半导体产品的出口量占总出口量的比例和十年前比起来有很大的提高。D.十年前，美国生产了全世界半导体产品总量的90%，而日本生产的只占2%。 305某些经济学家是大学数学系的毕业生。因此，某些大学数学系的毕业生是对企业经营很有研究的人。下列哪项如果为真，则能够保证上述论断的正确？A.某些经济学家专攻经济学的某一领域，对企业经营没有太多的研究。B.某些对企业经营很有研究的经济学家不是大学数学系毕业的。C.所有对企业经营很有研究的人都是经济学家。D.所有的经济学家都是对企业经营很有研究的人。 306由于信息高速公路上信息垃圾问题的越来越严重，科学家们不断发出警告：如果我们不从现在开始就重视预防和消除信息高速公路上的信息垃圾，那么总有一天信息高速公路将无法正常通行。以下哪项的意思最接近这些科学家们的警告？A.总有那么一天，信息高速公路不再能正常通行。B.只要从现在起就开始重视信息高速公路上信息垃圾的预防和消除，信息高速公路就可以一直正常通行下去。C.只有从现在起就开始重视信息高速公路上信息垃圾的预防和消除，信息高速公路才可能预防无法正常通行的后果。D.信息高速公路如果有一天不再能正常通行，那是因为我们没有从现在起重视信息高速公路上信息垃圾的预防和消除。 307 许多孕妇都出现了维生素缺乏的症状，但这通常不是由于孕妇的饮食中缺乏维生素，而是由于腹内婴儿的生长使她们比其他人对维生素有更高的需求。 为了评价上述结论的确切程度，以下哪项操作最为重要？A.对某个缺乏维生素的孕妇的日常饮食进行检测，确定其中维生素的含量。B.对某个不缺乏维生素的孕妇的日常饮食进行检测，确定其中维生素的含量。C.对孕妇的科学食谱进行研究，以确定有利于孕妇摄入足量维生素的最佳食谱。D.对日常饮食中维生素足量的一个孕妇和一个非孕妇进行检测，并分别确定她们是否缺乏维生素。 308 龙口开发区消防站向市政府申请购置一辆新的云梯消防车，这种云梯消防车是扑灭高层建筑火灾的重要设施。市政府否决了这项申请，理由是：龙口开发区现只有五幢高层建筑，消防站现有的云梯消防车足够了。 以下哪项是市政府的决定所必须假设的？A.龙口开发区至少近期内不会有新的高层建筑封顶投入使用。B.市政府的财政面临困难无力购置云梯消防车。C.消防站的云梯消防车中，至少有一辆近期内不会退役。D.龙口开发区的高层建筑内的防火设施都符合标准。 309","link":"/2021/04/13/3.%E8%AF%BB%E4%B9%A6/%E9%80%BB%E8%BE%91%E6%B8%B8%E6%88%8F/"},{"title":"","text":"SpringBoot监听器源码分析 SpringBoot监听器源码分析 源码springBoot监听器的主要分为两类： 1）运行时监听器 1234Run Listenersorg.springframework.boot.SpringApplicationRunListener=\\org.springframework.boot.context.event.EventPublishingRunListener 2）上下文监听器 1234567891011# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.builder.ParentContextCloserApplicationListener,\\org.springframework.boot.context.FileEncodingApplicationListener,\\org.springframework.boot.context.config.AnsiOutputApplicationListener,\\org.springframework.boot.context.config.ConfigFileApplicationListener,\\org.springframework.boot.context.config.DelegatingApplicationListener,\\org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener,\\org.springframework.boot.logging.ClasspathLoggingApplicationListener,\\org.springframework.boot.logging.LoggingApplicationListener 注意：springBoot运行时监听器作用是用来触发springBoot上下文监听器，再根据各监听器监听的事件进行区分。上面默认监听器的作用如下： 监听器触发启动流程前面两篇已经有详细分析，所以本篇我们只看监听器相关逻辑。 123456789101112131415161718192021222324252627public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; configureHeadlessProperty(); //获取启动监听器的监听器 - 这里创建了一个关键类：SpringApplicationRunListeners。 SpringApplicationRunListeners listeners = getRunListeners(args); //用该监听器来启动所有监听器 listeners.started(); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); context = createAndRefreshContext(listeners, applicationArguments); afterRefresh(context, applicationArguments); listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } return context; } catch (Throwable ex) { handleRunFailure(context, listeners, ex); throw new IllegalStateException(ex); }} SpringApplicationRunListeners这是一个封装工具类，封装了所有的启动类监听器。默认只有一个实例，这里封装成List listeners，主要是方便我们扩展，我们可以定义自己的启动类监听器。 12345678910111213141516171819202122232425262728293031323334353637383940414243//启动类监听器private final List&lt;SpringApplicationRunListener&gt; listeners;SpringApplicationRunListeners(Log log, Collection&lt;? extends SpringApplicationRunListener&gt; listeners) { this.log = log; this.listeners = new ArrayList&lt;SpringApplicationRunListener&gt;(listeners);}//启动上下文事件监听public void started() { for (SpringApplicationRunListener listener : this.listeners) { listener.started(); }}//environment准备完毕事件监听public void environmentPrepared(ConfigurableEnvironment environment) { for (SpringApplicationRunListener listener : this.listeners) { listener.environmentPrepared(environment); }}//spring上下文准备完毕事件监听public void contextPrepared(ConfigurableApplicationContext context) { for (SpringApplicationRunListener listener : this.listeners) { listener.contextPrepared(context); }}//上下文配置类加载事件监听public void contextLoaded(ConfigurableApplicationContext context) { for (SpringApplicationRunListener listener : this.listeners) { listener.contextLoaded(context); }}//上下文构造完成事件监听public void finished(ConfigurableApplicationContext context, Throwable exception) { for (SpringApplicationRunListener listener : this.listeners) { callFinishedListener(listener, context, exception); }} 在前面的启动流程源码分析中介绍过，这些方法会在合适的时间点触发执行，然后广播出不同的事件。 跟进去EventPublishingRunListener: 123456789public EventPublishingRunListener(SpringApplication application, String[] args) { this.application = application; this.args = args; //spring事件机制通用的事件发布类 this.multicaster = new SimpleApplicationEventMulticaster(); for (ApplicationListener&lt;?&gt; listener : application.getListeners()) { this.multicaster.addApplicationListener(listener); }} 上面会默认创建全局的事件发布工具类SimpleApplicationEventMulticaster。 123456789101112131415161718192021222324252627282930@Overridepublic void started() { publishEvent(new ApplicationStartedEvent(this.application, this.args));}@Overridepublic void environmentPrepared(ConfigurableEnvironment environment) { publishEvent(new ApplicationEnvironmentPreparedEvent(this.application, this.args, environment));}@Overridepublic void contextPrepared(ConfigurableApplicationContext context) { registerApplicationEventMulticaster(context);}@Overridepublic void contextLoaded(ConfigurableApplicationContext context) { for (ApplicationListener&lt;?&gt; listener : this.application.getListeners()) { if (listener instanceof ApplicationContextAware) { ((ApplicationContextAware) listener).setApplicationContext(context); } context.addApplicationListener(listener); } publishEvent(new ApplicationPreparedEvent(this.application, this.args, context));}@Overridepublic void finished(ConfigurableApplicationContext context, Throwable exception) { publishEvent(getFinishedEvent(context, exception));} 可以看出每个方法都会发布不同的事件，所有的事件统一继承SpringApplicationEvent： 事件广播继续跟进事件广播方法： 1234567891011121314151617181920212223242526@Overridepublic void multicastEvent(ApplicationEvent event) { multicastEvent(event, resolveDefaultEventType(event));}@Overridepublic void multicastEvent(final ApplicationEvent event, ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); //根据事件类型选取需要通知的监听器 for (final ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { //获取线程池，如果为null，则同步执行 Executor executor = getTaskExecutor(); if (executor != null) { executor.execute(new Runnable() { @Override public void run() { invokeListener(listener, event); } }); } else { invokeListener(listener, event); } }} 重点来看一下根据类型获取监听器：getApplicationListeners(event, type)跟进该方法： 12345678910111213141516private Collection&lt;ApplicationListener&lt;?&gt;&gt; retrieveApplicationListeners( ResolvableType eventType, Class&lt;?&gt; sourceType, ListenerRetriever retriever) { //...... //根据类型匹配监听器 for (ApplicationListener&lt;?&gt; listener : listeners) { if (supportsEvent(listener, eventType, sourceType)) { if (retriever != null) { retriever.applicationListeners.add(listener); } allListeners.add(listener); } } //...... AnnotationAwareOrderComparator.sort(allListeners); return allListeners;} 上面省去了一些不相关代码，继续跟进：supportsEvent(listener, eventType, sourceType)： 123456protected boolean supportsEvent(ApplicationListener&lt;?&gt; listener, ResolvableType eventType, Class&lt;?&gt; sourceType) { //判断监听器是否是 GenericApplicationListener 的子类，如果不是就返回一个GenericApplicationListenerAdapter GenericApplicationListener smartListener = (listener instanceof GenericApplicationListener ? (GenericApplicationListener) listener : new GenericApplicationListenerAdapter(listener)); return (smartListener.supportsEventType(eventType) &amp;&amp; smartListener.supportsSourceType(sourceType));} 1234public interface GenericApplicationListener extends ApplicationListener&lt;ApplicationEvent&gt;, Ordered { boolean supportsEventType(ResolvableType eventType); boolean supportsSourceType(Class&lt;?&gt; sourceType);} 这里又出现一个关键类：GenericApplicationListener,该类是 spring 提供的用于重写匹配监听器事件的接口。就是说如果需要判断的监听器是GenericApplicationListener的子类，说明类型匹配方法已被重现，就调用子类的匹配方法。如果不是，则为我们提供一个默认的适配器用来匹配：GenericApplicationListenerAdapter： 继续跟进该类的supportsEventType(ResolvableType eventType)方法： 123456789public boolean supportsEventType(ResolvableType eventType) { if (this.delegate instanceof SmartApplicationListener) { Class&lt;? extends ApplicationEvent&gt; eventClass = (Class&lt;? extends ApplicationEvent&gt;) eventType.getRawClass(); return ((SmartApplicationListener) this.delegate).supportsEventType(eventClass); } else { return (this.declaredEventType == null || this.declaredEventType.isAssignableFrom(eventType)); }} 可以看到该类最终调用的是declaredEventType.isAssignableFrom(eventType)方法，也就是说，如果我们没有重写监听器匹配方法，那么发布的事件 event 会被监听 event以及监听event的父类的监听器监听到。 自定义监听器123456789101112131415161718192021public class SimpleApplicationListener implements GenericApplicationListener,ApplicationListener&lt;ApplicationEvent&gt; { @Override public void onApplicationEvent(ApplicationEvent event) { System.out.println(&quot;myApplistener execute...&quot;); } @Override public boolean supportsEventType(ResolvableType eventType) { return true; } @Override public boolean supportsSourceType(Class&lt;?&gt; sourceType) { return true; } @Override public int getOrder() { return 0; }} 上面supportsEventType和supportsSourceType是预留的扩展方法，这里全部为true，也就意味着监听所有的ApplicationEvent事件，方法会执行多次： 总结springBoot整体框架就是通过该监听器org.springframework.boot.SpringApplicationRunListeners，来触发上下文监听器。通过上下文监听器来完成整体逻辑，比如加载配置文件，加载配置类，初始化日志环境等。","link":"/2021/11/29/30000Spring/106SpringBoot%E7%9B%91%E5%90%AC%E5%99%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"","text":"BeanFactory容器这是一个最简单的容器，它主要的功能是为依赖注入 （DI） 提供支持，这个容器接口在 org.springframework.beans.factory.BeanFactory中被定义。BeanFactory 和相关的接口，比如BeanFactoryAware、DisposableBean、InitializingBean，仍旧保留在 Spring 中，主要目的是向后兼容已经存在的和那些 Spring 整合在一起的第三方框架。 在 Spring 中，有大量对 BeanFactory 接口的实现。其中，最常被使用的是 XmlBeanFactory 类。这个容器从一个 XML 文件中读取配置元数据，由这些元数据来生成一个被配置化的系统或者应用。 在资源宝贵的移动设备或者基于 applet 的应用当中， BeanFactory 会被优先选择。否则，一般使用的是 ApplicationContext，除非你有更好的理由选择 BeanFactory。","link":"/2021/06/24/30000Spring/2.3.2BeanFactory%E5%AE%B9%E5%99%A8/"},{"title":"","text":"还在使用kill -9 pid结束spring boot项目吗？[toc] kill -9 pid ？？？kill可将指定的信息送至程序。预设的信息为SIGTERM(15)，可将指定程序终止。若仍无法终止该程序，可使用SIGKILL(9)信息尝试强制删除程序。程序或工作的编号可利用ps指令或jobs指令查看（这段话来自菜鸟教程）。 讲的这个复杂，简单点来说就是用来杀死linux中的进程，啥？你问我啥是进程？请自行百度。 我相信很多人都用过kill -9 pid 这个命令，彻底杀死进程的意思，一般情况我们使用它没有上面问题，但是在我们项目中使用它就有可能存在致命的问题。 kill -9 pid 带来的问题由于kill -9 属于暴力删除，所以会给程序带来比较严重的后果，那究竟会带来什么后果呢？ 举个栗子：转账功能，再给两个账户进行加钱扣钱的时候突然断电了？这个时候会发生什么事情？对于InnoDB存储引擎来说，没有什么损失，因为它支持事务，但是对于MyISAM引擎来说那简直就是灾难，为什么？假如给A账户扣了钱，现在需要将B账户加钱，这个时候停电了，就会造成，A的钱被扣了，但是B没有拿到这笔钱，这在生产环境是绝对不允许的，kill -9 相当于突然断电的效果。 当然了，像转账这种，肯定不是使用MyISAM引擎，但是如今分布式火了起来，跨服务转账已经是很平常的事情，这种时候如果使用kill -9 去停止服务，那就不是你的事务能保证数据的准确性了，这个时候你可能会想到分布式事务，这个世界上没有绝对的安全系统或者架构，分布式事务也是一样，他也会存在问题，概率很小，如果一旦发生，损失有可能是无法弥补的，所以一定不能使用kill -9 去停止服务，因为你不知道他会造成什么后果。 在MyISAM引擎中表现的更明显，比如用户的信息由两张表维护，管理员修改用户信息的时候需要修改两张表，但由于你的kill -9 暴力结束项目，导致只修改成功了一张表，这也会导致数据的不一致性，这是小事，因为大不了再修改一次，但是金钱、合同这些重要的信息如果由于你的暴力删除导致错乱，我觉得可能比删库跑路还严重，至少删库还能恢复，你这个都不知道错在哪里。 那我们应该怎么结束项目呢？ 其实java给我们提供了结束项目的功能，比如：tomcat可以使用shutdown.bat/shutdown.sh进行优雅结束。 什么叫优雅结束？第一步：停止接收请求和内部线程。第二步：判断是否有线程正在执行。第三步：等待正在执行的线程执行完毕。第四步：停止容器。 以上四步才是正常的结束流程，那springboot怎么正常结束服务呢？下面我介绍几种正常结束服务的方案，请拿好小本本做好笔记。 优雅结束服务kill -15 pid这种方式也会比较优雅的结束进程（项目），使用他的时候需要慎重，为什么呢？我们来看个例子 我写了一个普通的controller方法做测试 1234567891011@GetMapping(value = &quot;/test&quot;)public String test(){ log.info(&quot;test --- start&quot;); try { Thread.sleep(100000); } catch (InterruptedException e) { e.printStackTrace(); } log.info(&quot;test --- end&quot;); return &quot;test&quot;;} 代码很简单，打印：test — start之后让让程序休眠100秒，然后再打印：test — end，在线程休眠中我们使用kill -15 pid来结束这个进程，你们猜 test — end会被打印吗？ 123# application.ymlserver: port: 9988 启动项目 sudo mvn spring-boot:run这是maven启动springboot项目的方式,看到这个就代表项目启动成了 找到项目的进程id sudo ps -ef |grep shutdown 这个就是项目的进程号，接下来我们先测试test接口，让线程进入休眠状态，然后再使用kill -15 14086停止项目 sudo curl 127.0.0.1:9988/test回到项目日志 我们发现请求已经到达服务，并且线程已经成功进入休眠，现在我们kill -15 14086结束进程 sudo kill -15 14086回到日志 123456789com.ymy.controller.TestController : test --- start[extShutdownHook] o.s.s.concurrent.ThreadPoolTaskExecutor : Shutting down ExecutorService 'applicationTaskExecutor'java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.ymy.controller.TestController.test(TestController.java:26)com.ymy.controller.TestController : test --- endo.s.web.servlet.HandlerExecutionChain : HandlerInterceptor.afterCompletion threw exceptionjava.lang.NullPointerException: null 居然报错了，但是test — end是打印出来了，为什么会报错呢？这就和sleep这个方法有关了，在线程休眠期间，当调用线程的interrupt方法的时候会导致sleep抛出异常，这里很明显就是kill -15 这个命令会让程序马上调用线程的interrupt方法，目的是为了让线程停止，虽然让线程停止，但线程什么时候停止还是线程自己说的算，这就是为什么我们还能看到：test — end的原因。 ConfigurableApplicationContext colse我们先看怎么实现 123456789101112131415161718192021222324252627282930313233343536373839404142import lombok.extern.slf4j.Slf4j;import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RestController;@RestController@Slf4jpublic class TestController implements ApplicationContextAware { private ApplicationContext context; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.context = applicationContext; } @GetMapping(value = &quot;/test&quot;) public String test(){ log.info(&quot;test --- start&quot;); try { Thread.sleep(100000); } catch (InterruptedException e) { e.printStackTrace(); } log.info(&quot;test --- end&quot;); return &quot;test&quot;; } /** * 停机 */ @PostMapping(value = &quot;shutdown&quot;) public void shutdown(){ ConfigurableApplicationContext cyx = (ConfigurableApplicationContext) context; cyx.close(); }} 重点在：cyx.close();，为什么他能停止springboot项目呢？请看源码 1234567public void close() { synchronized(this.startupShutdownMonitor) { this.doClose(); if (this.shutdownHook != null) Runtime.getRuntime().removeShutdownHook(this.shutdownHook); }} 程序在启动的时候向jvm注册了一个关闭钩子，我们在执行colse方法的时候会删除这个关闭钩子，jvm就会知道这是需要停止服务。 我们看测试结果 很明显，他也出发了线程的interrupt方法导致线程报错，原理和kill -15差不多。 actuator这种方式是通过引入依赖的方式停止服务，actuator提供了很多接口，比如健康检查，基本信息等等，我们也可以使用他来优雅的停机。 引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 123456789101112server: port: 9988management: endpoints: web: exposure: include: shutdown endpoint: shutdown: enabled: true server: port: 8888 我这里对actuator的接口重新给定了一个接口，这样可提高安全性，下面我们来测试一下 1234567891011@RequestMapping(value = &quot;/test&quot;,method = RequestMethod.GET)public String test(){ System.out.println(&quot;test --- start&quot;); try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;test --- end&quot;); return &quot;hello&quot;;} 我们发现发送停止服务请求之后还给我们返回了提示信息，很人性化，我们看看控制台 test — end被执行了，不过在停止线程池的时候还是调用了线程的interrupt方法，导致sleep报错，这三种方式都可以比较优雅的停止springboot服务，如果我项目中存在线程休眠，我希望10秒以后再停止服务可以吗？肯定是可以的，我们只需要稍微做点修改就可以了。 1.新增停止springboot服务类：ElegantShutdownConfig.java 123456789101112131415161718192021222324252627282930313233343536373839package com.ymy.config;import org.apache.catalina.connector.Connector;import org.springframework.boot.web.embedded.tomcat.TomcatConnectorCustomizer;import org.springframework.context.ApplicationListener;import org.springframework.context.event.ContextClosedEvent;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ElegantShutdownConfig implements TomcatConnectorCustomizer, ApplicationListener&lt;ContextClosedEvent&gt; { private volatile Connector connector; private final int waitTime = 10; @Override public void customize(Connector connector) { this.connector = connector; } @Override public void onApplicationEvent(ContextClosedEvent event) { connector.pause(); Executor executor = connector.getProtocolHandler().getExecutor(); if (executor instanceof ThreadPoolExecutor) { try { ThreadPoolExecutor threadPoolExecutor = (ThreadPoolExecutor) executor; threadPoolExecutor.shutdown(); if (!threadPoolExecutor.awaitTermination(waitTime, TimeUnit.SECONDS)) { System.out.println(&quot;请尝试暴力关闭&quot;); } } catch (InterruptedException ex) { System.out.println(&quot;异常了&quot;); Thread.currentThread().interrupt(); } } }} 2.在启动类中加入bean 12345678910111213141516171819202122232425262728293031323334353637383940package com.ymy;import com.ymy.config.ElegantShutdownConfig;import lombok.extern.slf4j.Slf4j;import org.apache.catalina.connector.Connector;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.embedded.tomcat.TomcatConnectorCustomizer;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.boot.web.servlet.server.ServletWebServerFactory;import org.springframework.context.ApplicationListener;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.context.annotation.Bean;import org.springframework.context.event.ContextClosedEvent;import java.util.concurrent.Executor;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;@SpringBootApplicationpublic class ShutdownServerApplication { public static void main(String[] args) { ConfigurableApplicationContext run = SpringApplication.run(ShutdownServerApplication.class, args); run.registerShutdownHook(); } @Bean public ElegantShutdownConfig elegantShutdownConfig() { return new ElegantShutdownConfig(); } @Bean public ServletWebServerFactory servletContainer() { TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory(); tomcat.addConnectorCustomizers(elegantShutdownConfig()); return tomcat; }} 这样我们就配置好了，我们再来测试一遍，test的接口还是休眠10秒 我们发现这次没有报错了，他是等待了一段时间之后再结束的线程池，这个时间就是我们在ElegantShutdownConfig类中配置的waitTime。 那可能你会有疑问了，jvm没有立即停止，那这个时候在有请求会发生什么呢？如果关闭的时候有新的请求，服务将不在接收此请求。 数据备份操作如果我想在服务停止的时候做点备份操作啥的，应该怎么做呢？其实很简单在你要执行的方法上添加一个注解即可：@PreDestroy Destroy：消灭、毁灭pre:前缀缩写 所以合在一起的意思就是在容器停止之前执行一次，你可以在这里面做备份操作，也可以做记录停机时间等。 新增服务停止备份工具类：DataBackupConfig.java 12345678910import org.springframework.context.annotation.Configuration;import javax.annotation.PreDestroy;@Configurationpublic class DataBackupConfig { @PreDestroy public void backData(){ System.out.println(&quot;正在备份数据。。。。。。。。。。。&quot;); }} 我们再来测试然后打印控制台日志：","link":"/2021/06/28/30000Spring/2.3.30%E4%BC%98%E9%9B%85%E5%85%B3%E9%97%AD%E7%A8%8B%E5%BA%8F/"},{"title":"","text":"[toc] 一、前言1.1 SpringBoot的优点SpringBoot是新一代流行的Spring应用开发框架，它具有更多的优点： 创建独立的Spring应用 内嵌Tomcat、Jetty或Undertow（无需部署war包） 提供自用的starter来简化构建配置 提供指标监控、运行状况检查和外部化配置 没有代码生成，也不需要XML配置（约定大于配置） 1.2 SpringBoot-starter的作用SpringBoot拥有很多方便使用的starter（Spring提供的starter命名规范spring-boot-starter-xxx.jar，第三方提供的starter命名规范xxx-spring-boot-starter.jar），比如spring-boot-starter-log4j、mybatis-spring-boot-starter.jar等，各自都代表了一个相对完整的功能模块。 SpringBoot-starter是一个集成接合器，完成两件事： 引入模块所需的相关jar包 自动配置各自模块所需的属性 二、SpringBoot-starter解析2.1 SpringBoot搭建SSM我们使用SpringBoot新建一个web工程（SSM），看看他的依赖都有哪些： 123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 可见，使用SpringBoot-starter来搭建web工程相当方便，不像以前搭建SSM，需要将那么多jar包依赖逐个加入到Maven工程，还需考虑jar包之间的版本兼容性等。另外，数据源所需的配置也仅需如下： 123spring.datasource.url=jdbc:mysql://100.10.14.116:3306/szhtest2?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=Asia/Shanghaispring.datasource.username=testspring.datasource.password=test Mybatis的配置如下： 12345@Mapperpublic interface UserDao { @Select(&quot;SELECT * from user where id = #{id}&quot;) Map&lt;String, Object&gt; get(@Param(&quot;id&quot;) String id);} 至此，不需要多余的配置，一个SpringBoot搭建的SSM工程就可以跑了。 2.2 SpringBoot的诸多配置得益于starter的作用，使用SpringBoot确实方便，但对刚刚上手SpringBoot的人来说，可能只知道配置属性是在application.xml或application.yml中添加，但他们各自的属性都有哪些，具体怎么配置，却无从下手。这里先解决SpringBoot-starter中各属性的配置问题。 以2.1中的示例来看，Mybatis的配置是怎么生效的？查看示例工程的pom依赖： 注意到mybatis-spring-boot-starter帮我们自动依赖了Mybatis所需jar包，其中有一个负责自动配置的mybatis-spring-boot-autoconfigure.jar，紧接着打开此jar，如下： META-INF/spring-configuration-metadata.json中便是Mybatis在SpringBoot中的所有配置属性和介绍。至此，第一个问题便得到解决。 2.3 SpringBoot-starter自动配置bean现在已得知jar包是怎么样自动依赖进来，以及他们的配置属性，那么接下来该考虑Mybatis所需的bean（如必需的sqlSessionFactory、sqlSessionTemplate等）是如何被自动加载的？ 理所应当地，我们继续去查看mybatis-spring-boot-autoconfigure.jar，注意到里面有一个自动配置的类MybatisAutoConfiguration： （1）@Configuration：被挂上@Configuration注解，表明它是一个配置类，作用等同于xml配置，里面有被@Bean注解的方法，也等同于xml配置的各种。 （2）@ConditionalOnClass/@ConditionalOnBean：自动配置条件注解，用于在某一部分配置中，将另一模块部分的配置自动加载进来，因为随着系统越来越大，配置内容越来越多，我们应当将Mybatis的配置放在一处，将log4j的配置放在一处，将SpringBoot自身的配置放在一处，当他们需要互相依赖时，可通过这类注解进行自动配置，如下： 12345678@ConditionalOnClass @ConditionalOnMissingClass@ConditionalOnBean @ConditionalOnMissingBean@ConditionalOnProperty@ConditionalOnResource@ConditionalOnWebApplication @ConditionalOnNotWebApplication@ConditionalOnExpression @AutoConfigureAfter @AutoConfigureBefore @AutoConfigureOrder（指定顺序） （3）@EnableConfigurationProperties：启用对@ConfigurationProperties注解的bean的支持，这里对应了配置属性类MybatisProperties，它里面定义了Mybatis的所有配置。 （4）@AutoConfigureAfter：应在其他指定的自动配置类之后应用自动配置。即org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration被自动配置后，才会接着自动配置MybatisAutoConfiguration。这里也解释了为什么我们在application.xml中只配置了数据源，而没有配置Mybatis，但是Mybatis可以正常查库的原因，就是因为它们配置之间的依赖关系。 到这里，差不多明白了starter自动配置bean的方式，但是如若再去深究，各种starter的bean是如何被自动加载的，猜想会不会是项目启动后，SpringBoot自动扫描里面所有的jar包，再去扫描所有的类，从而将各个bean放置IOC容器中。从结果来看，肯定是SpringBoot在启动时确确实实地自动加载了数据源和Mybatis相关的bean，不然他们无法正常工作。 回想在我们启动示例工程时，SpringBoot会自动扫描启动类所在包下的所有类，而如果还去扫描所有的jar包的话，又是具体怎么做到的？不妨从入口类调试一把，在SpringApplication.run(DemoApplication.class, args)打断点，一直追踪到getSpringFactoriesInstances这块： 查看SpringFactoriesLoader.loadFactoryNames的方法注释： 使用给定的类加载器从META-INF/spring.factories加载给定类型的工厂实现的完全限定类名。 有点眼熟，这里的spring.factories刚好也存在于mybatis-spring-boot-autoconfigure.jar中， 继续调试，进入SpringFactoriesLoader.loadFactoryNames， 这里用类加载器得到工程中所有jar包中的META-INF/spring.factories文件资源，进而通过此文件得到了一些包括自动配置相关的类的集合，有各种工厂类、监听器、处理器、过滤器、初始化器等等，如下： 最后的org.springframework.boot.autoconfigure.EnableAutoConfiguration集合中当然包括了org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration和org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration。接着必然是将实例化的各个bean放进IOC容器中。 至此我们便明白了SpringBoot是如何自动配置starter里面的bean的。","link":"/2021/08/04/30000Spring/2.3.31SpringBoot-starter%E7%9A%84%E5%8E%9F%E7%90%86/"},{"title":"","text":"SpringBoot 修改/解密入参，修改/加密出参一、概念解释：Servlet规范中的filter引入了一个功能强大的拦截模式。Filter能在request到达servlet的服务方法之前拦截HttpServletRequest对象，而在服务方法转移控制后又能拦截HttpServletResponse对象。 但是HttpServletRequest中的参数是无法改变的，若是手动执行修改request中的参数，则会抛出异常。且无法获取到HttpServletResponse中的输出流中的数据，因为HttpServletResponse中输出流的数据会写入到默认的输出端，你手动无法获取到数据。 我们可以利用HttpServletRequestWrapper包装HttpServletRequest，用HttpServletResponseWrapper包装HttpServletResponse，在Wrapper中实现参数的修改或者是response输出流的读取，然后用HttpServletRequestWrapper替换HttpServletRequest，HttpServletResponseWrapper替换HttpServletResponse。这样就实现了参数的修改设置和输出流的读取。 二、具体实现代码修改/解密入参 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158/** * 解决: request.getInputStream()只能读取一次的问题 */@Component@WebFilter(filterName = &quot;HttpServletRequestFilter&quot;, urlPatterns = &quot;/&quot;)@Order(10000)public class HttpServletRequestFilter implements Filter { private static final Logger log = LoggerFactory.getLogger(HttpServletRequestFilter.class); @Value(&quot;${spring.web.http.salt}&quot;) private String slat; private MultipartResolver multipartResolver; public HttpServletRequestFilter(MultipartResolver multipartResolver) { this.multipartResolver = multipartResolver; } @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { ServletRequest requestWrapper = null; String contentType = servletRequest.getContentType(); //剔除multipart/form-data的情况 if(StrUtil.isNotBlank(contentType) &amp;&amp; contentType.contains(&quot;multipart/form-data&quot;)){ MultipartHttpServletRequest multipartRequest = multipartResolver.resolveMultipart((HttpServletRequest) servletRequest); filterChain.doFilter(multipartRequest, servletResponse); } else{ if(servletRequest instanceof HttpServletRequest) { requestWrapper = new RequestWrapper((HttpServletRequest) servletRequest); } if(null == requestWrapper) { filterChain.doFilter(servletRequest, servletResponse); } else { filterChain.doFilter(requestWrapper, servletResponse); } } } /*** * HttpServletRequest 包装器 * 解决: request.getInputStream()只能读取一次的问题 * 目标: 流可重复读 */ public class RequestWrapper extends HttpServletRequestWrapper { private Map&lt;String, String[]&gt; paramMap = new HashMap&lt;&gt;(); /** * 请求体 */ private String mBody; public RequestWrapper(HttpServletRequest request) { super(request); if(Boolean.parseBoolean(request.getHeader(&quot;encrypt&quot;))){ processDecryption(request); }else{ mBody = getBody(request); } } /** * 获取请求参数 * @param request 请求 * @return 请求体 */ private String getBody(HttpServletRequest request) { return HttpContextUtils.getHttpServletRequestParameterMap(request); } public void setBody(String body){ this.mBody = body; } @Override public BufferedReader getReader() throws IOException { return new BufferedReader(new InputStreamReader(getInputStream())); } @Override public ServletInputStream getInputStream() { // 创建字节数组输入流 final ByteArrayInputStream bais = new ByteArrayInputStream(mBody.getBytes(StandardCharsets.UTF_8)); return new ServletInputStream() { @Override public boolean isFinished() { return false; } @Override public boolean isReady() { return false; } @Override public void setReadListener(ReadListener readListener) { } @Override public int read() { return bais.read(); } }; } public void setParamMap(Map&lt;String, String[]&gt; paramMap) { this.paramMap = paramMap; } @Override public Map&lt;String, String[]&gt; getParameterMap() { if(!paramMap.isEmpty()){ return paramMap; } return super.getParameterMap(); } /** * 请求解密处理 */ private void processDecryption(HttpServletRequest request) { try { String requestData = getBody(request); //请求方式不为GET if (!StrUtil.equalsIgnoreCase(request.getMethod(), RequestMethod.GET.name()) &amp;&amp; !StrUtil.equalsIgnoreCase(request.getMethod(), RequestMethod.DELETE.name())) { //对入参进行解密 String decryptRequestData = AesUtil.decrypt(requestData, slat); if(StrUtil.hasBlank(decryptRequestData)){ decryptRequestData = requestData; } log.info(&quot;Post请求数据解密:{}&quot;,decryptRequestData); setBody(decryptRequestData); } else{ // 请求方式为GET Map&lt;String, String[]&gt; paramMap = new HashMap&lt;&gt;(); Map&lt;String,Object&gt; hashMap = JSONUtil.parseObj(requestData); for(Map.Entry&lt;String,Object&gt; entry : hashMap.entrySet()){ String paramValue = String.valueOf(entry.getValue()); String decryptParamValue = AesUtil.decrypt(paramValue, slat); if(StrUtil.hasBlank(decryptParamValue)){ decryptParamValue = paramValue; } paramMap.put(entry.getKey(), new String[]{decryptParamValue}); } setParamMap(paramMap); log.info(&quot;GET请求数据解密:{}&quot;,paramMap); } } catch (Exception e) { log.error(&quot;请求数据解密失败&quot;, e); throw new RuntimeException(e); } } }} 修改/加密出参 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114@Component@WebFilter(filterName = &quot;HttpServletResponseFilter&quot;, urlPatterns = &quot;/&quot;)@Order(10000)public class HttpServletResponseFilter implements Filter { private static final Logger log = LoggerFactory.getLogger(HttpServletResponseFilter.class); @Value(&quot;${spring.web.http.salt}&quot;) private String slat; @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest req = (HttpServletRequest) servletRequest; HttpServletResponse resp = (HttpServletResponse) servletResponse; String url = req.getRequestURL().toString(); if(Boolean.parseBoolean(req.getHeader(&quot;encrypt&quot;))){ ResponseWrapper responseWrapper = new ResponseWrapper(resp); filterChain.doFilter(req, responseWrapper); String responseData = responseWrapper.getResponseData(); writeEncryptContent(responseData, servletResponse); }else{ filterChain.doFilter(req, resp); } } class ResponseWrapper extends HttpServletResponseWrapper { private ServletOutputStream filterOutput; private ByteArrayOutputStream output; /** * Constructs a response adaptor wrapping the given response. * * @param response The response to be wrapped * @throws IllegalArgumentException if the response is null */ public ResponseWrapper(HttpServletResponse response) { super(response); output = new ByteArrayOutputStream(); } @Override public ServletOutputStream getOutputStream() throws IOException { if (filterOutput == null) { filterOutput = new ServletOutputStream() { @Override public void write(int b) throws IOException { output.write(b); } @Override public boolean isReady() { return false; } @Override public void setWriteListener(WriteListener writeListener) { } }; } return filterOutput; } public String getResponseData() { return output.toString(); } } /** * 输出加密内容 * @param data * @param response * @throws IOException */ private void writeEncryptContent(String data, ServletResponse response){ ServletOutputStream out = null; String responseData; String encryptData; try { Map&lt;String,Object&gt; hashMap = JSONUtil.parseObj(data); //出参加密 encryptData = AesUtil.encrypt(String.valueOf(hashMap.get(&quot;data&quot;)), slat); if(StrUtil.hasBlank(encryptData)){ responseData = data; }else{ hashMap.put(&quot;data&quot;,encryptData); responseData = JSONUtil.toJsonStr(hashMap); } response.setCharacterEncoding(StandardCharsets.UTF_8.name()); response.setContentType(&quot;application/json&quot;); out = response.getOutputStream(); out.write(responseData.getBytes(StandardCharsets.UTF_8.name())); log.info(&quot;返回值为:{}&quot;,responseData); } catch (Exception e) { throw new RuntimeException(e); } finally { if (out != null) { try { out.flush(); out.close(); } catch (IOException e) { e.printStackTrace(); } } } }}","link":"/2021/09/08/30000Spring/2.3.32SpringBoot%20%E4%BF%AE%E6%94%B9%E8%A7%A3%E5%AF%86%E5%85%A5%E5%8F%82%EF%BC%8C%E4%BF%AE%E6%94%B9%E5%8A%A0%E5%AF%86%E5%87%BA%E5%8F%82/"},{"title":"","text":"Spring ApplicationContext 容器Application Context 是 BeanFactory 的子接口，也被称为 Spring 上下文。 Application Context 是 spring 中较高级的容器。和 BeanFactory 类似，它可以加载配置文件中定义的 bean，将所有的 bean 集中在一起，当有请求的时候分配 bean。 另外，它增加了企业所需要的功能，比如，从属性文件中解析文本信息和将事件传递给所指定的监听器。这个容器在 org.springframework.context.ApplicationContext interface 接口中定义。 ApplicationContext 包含 BeanFactory 所有的功能，一般情况下，相对于 BeanFactory，ApplicationContext 会更加优秀。当然，BeanFactory 仍可以在轻量级应用中使用，比如移动设备或者基于 applet 的应用程序。 最常被使用的 ApplicationContext 接口实现： FileSystemXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你需要提供给构造器 XML 文件的完整路径。 ClassPathXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你不需要提供 XML 文件的完整路径，只需正确配置 CLASSPATH 环境变量即可，因为，容器会从 CLASSPATH 中搜索 bean 配置文件。 WebXmlApplicationContext：该容器会在一个 web 应用程序的范围内加载在 XML 文件中已被定义的 bean。 我们已经在 Spring Hello World Example章节中看到过 ClassPathXmlApplicationContext 容器，并且，在基于 spring 的 web 应用程序这个独立的章节中，我们讨论了很多关于 WebXmlApplicationContext。所以，接下来，让我们看一个关于 FileSystemXmlApplicationContext 的例子。 例子:假设我们已经安装 Eclipse IDE，按照下面的步骤，我们可以创建一个 Spring 应用程序。 步骤 描述 1 创建一个名为 SpringExample 的工程， 在 src 下新建一个名为 com.tutorialspoint 的文件夹src 2 点击右键，选择 Add External JARs 选项，导入 Spring 的库文件，正如我们在 Spring Hello World Example 章节中提到的导入方式。 3 在 com.tutorialspoint 文件夹下创建 HelloWorld.java 和 MainApp.java 两个类文件。 4 文件夹下创建 Bean 的配置文件 Beans.xml。 5 最后的步骤是编辑所有 JAVA 文件的内容和 Bean 的配置文件,按照以前我们讲的那样去运行应用程序。 下面是文件 HelloWorld.java 的内容： 12345678910package com.tutorialspoint;public class HelloWorld { private String message; public void setMessage(String message){ this.message = message; } public void getMessage(){ System.out.println(&quot;Your Message : &quot; + message); }} 下面是文件 MainApp.java 的内容： 1234567891011package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext;public class MainApp { public static void main(String[] args) { ApplicationContext context = new FileSystemXmlApplicationContext (&quot;C:/Users/ZARA/workspace/HelloSpring/src/Beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); }} 在主程序当中，我们需要注意以下两点： 第一步生成工厂对象。加载完指定路径下 bean 配置文件后，利用框架提供的 FileSystemXmlApplicationContext API 去生成工厂 bean。FileSystemXmlApplicationContext 负责生成和初始化所有的对象，比如，所有在 XML bean 配置文件中的 bean。 第二步利用第一步生成的上下文中的 getBean() 方法得到所需要的 bean。 这个方法通过配置文件中的 bean ID 来返回一个真正的对象。一旦得到这个对象，就可以利用这个对象来调用任何方法。 下面是配置文件 Beans.xml 中的内容： 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;Hello World!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 如果你已经完成上面的内容，接下来，让我们运行这个应用程序。如果程序没有错误，你将从控制台看到以下信息： 1Your Message : Hello World!","link":"/2021/06/24/30000Spring/2.3.3ApplicationContext/"},{"title":"","text":"Spring Bean 后置处理器Bean 后置处理器允许在调用初始化方法前后对 Bean 进行额外的处理。 BeanPostProcessor 接口定义回调方法，你可以实现该方法来提供自己的实例化逻辑，依赖解析逻辑等。你也可以在 Spring 容器通过插入一个或多个 BeanPostProcessor 的实现来完成实例化，配置和初始化一个bean之后实现一些自定义逻辑回调方法。 你可以配置多个 BeanPostProcessor 接口，通过设置 BeanPostProcessor 实现的Ordered接口提供的 order 属性来控制这些 BeanPostProcessor 接口的执行顺序。 BeanPostProcessor 可以对 bean（或对象）实例进行操作，这意味着 Spring IoC 容器实例化一个 bean 实例，然后 BeanPostProcessor 接口进行它们的工作。 注意：ApplicationContext 会自动检测由 BeanPostProcessor 接口的实现定义的 bean，注册这些 bean 为后置处理器，然后通过在容器中创建 bean，在适当的时候调用它。 在你自定义的的 BeanPostProcessor 接口实现类中，要实现以下的两个抽象方法 BeanPostProcessor.postProcessBeforeInitialization(Object, String) 和 BeanPostProcessor.postProcessAfterInitialization(Object, String) 和，注意命名要准确 否则会出现： “ The type InitHelloWorld must implement the inherited abstract method BeanPostProcessor.postProcessBeforeInitialization(Object, String) ”之类的错误 例子：下面的例子显示如何在 ApplicationContext 的上下文中编写，注册和使用 BeanPostProcessor。 我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的**src**文件夹中创建一个包 *com.tutorialspoint*。 2 使用*Add External JARs*选项，添加所需的 Spring 库，解释见 *Spring Hello World Example* 章节。 3 在 *com.tutorialspoint* 包中创建 Java 类 *HelloWorld*、InitHelloWorld和 *MainApp*。 4 在**src**文件夹中创建Beans配置文件 *Beans.xml*。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 这里是 HelloWorld.java 文件的内容： 1234567891011package com.tutorialspoint;public class HelloWorld { private String message; public void init(){ System.out.println(&quot;Bean is going through init.&quot;); } public void destroy(){ System.out.println(&quot;Bean will destroy now.&quot;); }} 这是实现BeanPostProcessor的非常简单的例子，它在任何 bean 的初始化的之前和之后输入该 bean 的名称。你可以在初始化 bean 的之前和之后实现更复杂的逻辑，因为你有两个访问内置 bean 对象的后置处理程序的方法。 这里是 InitHelloWorld.java 文件的内容： 12345678910111213package com.tutorialspoint;import org.springframework.beans.factory.config.BeanPostProcessor;import org.springframework.beans.BeansException;public class InitHelloWorld implements BeanPostProcessor { public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;BeforeInitialization : &quot; + beanName); return bean; // you can return any other object as well } public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;AfterInitialization : &quot; + beanName); return bean; // you can return any other object as well }} 下面是 MainApp.java 文件的内容。在这里，你需要注册一个在 AbstractApplicationContext 类中声明的关闭 hook 的 registerShutdownHook() 方法。它将确保正常关闭，并且调用相关的 destroy 方法。 1234567891011package com.tutorialspoint;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp { public static void main(String[] args) { AbstractApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); context.registerShutdownHook(); }} 下面是 init 和 destroy 方法需要的配置文件 Beans.xml 文件： 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;Hello World!&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;com.tutorialspoint.InitHelloWorld&quot; /&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息： 12345BeforeInitialization : helloWorldBean is going through init.AfterInitialization : helloWorldYour Message : Hello World!Bean will destroy now.","link":"/2021/11/27/30000Spring/2.3.5Bean%E5%90%8E%E7%BD%AE%E5%A4%84%E7%90%86%E5%99%A8/"},{"title":"","text":"Spring Bean 生命周期理解 Spring bean 的生命周期很容易。当一个 bean 被实例化时，它可能需要执行一些初始化使它转换成可用状态。同样，当 bean 不再需要，并且从容器中移除时，可能需要做一些清除工作。 尽管还有一些在 Bean 实例化和销毁之间发生的活动，但是本章将只讨论两个重要的生命周期回调方法，它们在 bean 的初始化和销毁的时候是必需的。 为了定义安装和拆卸一个 bean，我们只要声明带有 init-method 和/或 destroy-method 参数的 。init-method 属性指定一个方法，在实例化 bean 时，立即调用该方法。同样，destroy-method 指定一个方法，只有从容器中移除 bean 之后，才能调用该方法。 Bean的生命周期可以表达为：Bean的定义——Bean的初始化——Bean的使用——Bean的销毁 初始化回调org.springframework.beans.factory.InitializingBean 接口指定一个单一的方法： 1void afterPropertiesSet() throws Exception; 因此，你可以简单地实现上述接口和初始化工作可以在 afterPropertiesSet() 方法中执行，如下所示： 12345public class ExampleBean implements InitializingBean { public void afterPropertiesSet() { // do some initialization work }} 在基于 XML 的配置元数据的情况下，你可以使用 init-method 属性来指定带有 void 无参数方法的名称。例如： 1&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/&gt; 下面是类的定义： 12345public class ExampleBean { public void init() { // do some initialization work }} 销毁回调org.springframework.beans.factory.DisposableBean 接口指定一个单一的方法： 1void destroy() throws Exception; 因此，你可以简单地实现上述接口并且结束工作可以在 destroy() 方法中执行，如下所示： 12345public class ExampleBean implements DisposableBean { public void destroy() { // do some destruction work }} 在基于 XML 的配置元数据的情况下，你可以使用 destroy-method 属性来指定带有 void 无参数方法的名称。例如： 1&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot; destroy-method=&quot;destroy&quot;/&gt; 下面是类的定义： 12345public class ExampleBean { public void destroy() { // do some destruction work }} 如果你在非 web 应用程序环境中使用 Spring 的 IoC 容器；例如在丰富的客户端桌面环境中；那么在 JVM 中你要注册关闭 hook。这样做可以确保正常关闭，为了让所有的资源都被释放，可以在单个 beans 上调用 destroy 方法。 建议你不要使用 InitializingBean 或者 DisposableBean 的回调方法，因为 XML 配置在命名方法上提供了极大的灵活性。 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包 com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，解释见 Spring Hello World Example 章节。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件 Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 这里是 HelloWorld.java 的文件的内容： 1234567891011package com.tutorialspoint;public class HelloWorld { private String message; public void init(){ System.out.println(&quot;Bean is going through init.&quot;); } public void destroy(){ System.out.println(&quot;Bean will destroy now.&quot;); }} 下面是 MainApp.java 文件的内容。在这里，你需要注册一个在 AbstractApplicationContext 类中声明的关闭 hook 的 registerShutdownHook() 方法。它将确保正常关闭，并且调用相关的 destroy 方法。 1234567891011package com.tutorialspoint;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp { public static void main(String[] args) { AbstractApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); context.registerShutdownHook(); }} 下面是 init 和 destroy 方法必需的配置文件 Beans.xml 文件： 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;Hello World!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息： 123Bean is going through init.Your Message : Hello World!Bean will destroy now. 默认的初始化和销毁方法如果你有太多具有相同名称的初始化或者销毁方法的 Bean，那么你不需要在每一个 bean 上声明初始化方法和销毁方法。框架使用 元素中的 default-init-method 和 default-destroy-method 属性提供了灵活地配置这种情况，如下所示： 123456789101112&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot; default-init-method=&quot;init&quot; default-destroy-method=&quot;destroy&quot;&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt;&lt;/beans&gt;","link":"/2021/06/24/30000Spring/2.3.4Bean%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/"},{"title":"","text":"[TOC] 1.如何创建Bean对象无参构造方法-&gt;对象-&gt;依赖注入-&gt;初始化前(@PostProcessor)-&gt;初始化(InitializingBean)-&gt;初始化后(AOP)-&gt;代理对象-&gt;放入Map单例池-&gt;Bean对象 12345678910111213141516public class User { private User admin; @PostProcessor public void a() { // 给admin赋值 } }populateBean();applyBeanPostProcessorsBeforeInitail(); 2.推断构造方法多个构造方法的时候，先取无参构造方法。 构造方法上增加@Autowired注解 单例 、工厂、原型、装饰器、适配器、模板、观察者模式、责任链模式、代理模式","link":"/2022/03/15/30000Spring/300001%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"title":"","text":"Bean 定义继承bean 定义可以包含很多的配置信息，包括构造函数的参数，属性值，容器的具体信息例如初始化方法，静态工厂方法名，等等。 子 bean 的定义继承父定义的配置数据。子定义可以根据需要重写一些值，或者添加其他值。 Spring Bean 定义的继承与 Java 类的继承无关，但是继承的概念是一样的。你可以定义一个父 bean 的定义作为模板和其他子 bean 就可以从父 bean 中继承所需的配置。 当你使用基于 XML 的配置元数据时，通过使用父属性，指定父 bean 作为该属性的值来表明子 bean 的定义。 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包 com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，解释见 Spring Hello World Example 章节。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld、HelloIndia 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件 Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 下面是配置文件 Beans.xml，在该配置文件中我们定义有两个属性 message1 和 message2 的 “helloWorld” bean。然后，使用 parent 属性把 “helloIndia” bean 定义为 “helloWorld” bean 的孩子。这个子 bean 继承 message2 的属性，重写 message1 的属性，并且引入一个属性 message3。 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello World!&quot;/&gt; &lt;property name=&quot;message2&quot; value=&quot;Hello Second World!&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;helloIndia&quot; class=&quot;com.tutorialspoint.HelloIndia&quot; parent=&quot;helloWorld&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello India!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 这里是 HelloWorld.java 文件的内容： 12345package com.tutorialspoint;public class HelloWorld { private String message1; private String message2;} 这里是 HelloIndia.java 文件的内容： 1234567package com.tutorialspoint;public class HelloIndia { private String message1; private String message2; private String message3;} 下面是 MainApp.java 文件的内容： 1234567891011121314151617181920package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld objA = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objA.getMessage1(); objA.getMessage2(); HelloIndia objB = (HelloIndia) context.getBean(&quot;helloIndia&quot;); objB.getMessage1(); objB.getMessage2(); objB.getMessage3(); }} 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息： 12345World Message1 : Hello World!World Message2 : Hello Second World!India Message1 : Hello India!India Message2 : Hello Second World!India Message3 : Namaste India! 在这里你可以观察到，我们创建 “helloIndia” bean 的同时并没有传递 message2，但是由于 Bean 定义的继承，所以它传递了 message2。 Bean 定义模板你可以创建一个 Bean 定义模板，不需要花太多功夫它就可以被其他子 bean 定义使用。在定义一个 Bean 定义模板时，你不应该指定类的属性，而应该指定带 true 值的抽象属性，如下所示： 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;beanTeamplate&quot; abstract=&quot;true&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello World!&quot;/&gt; &lt;property name=&quot;message2&quot; value=&quot;Hello Second World!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;helloIndia&quot; class=&quot;com.tutorialspoint.HelloIndia&quot; parent=&quot;beanTeamplate&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello India!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 父 bean 自身不能被实例化，因为它是不完整的，而且它也被明确地标记为抽象的。当一个定义是抽象的，它仅仅作为一个纯粹的模板 bean 定义来使用的，充当子定义的父定义使用。","link":"/2021/06/24/30000Spring/2.3.6Bean%E5%AE%9A%E4%B9%89%E7%BB%A7%E6%89%BF/"},{"title":"","text":"Bean注入Spring容器4种方式[TOC] 1.xml 方式123&lt;bean name=&quot;teacher&quot; class=&quot;org.springframework.demo.model.Teacher&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;阿Q&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 2.注解方式@Configuration + @Bean简单样例：将 RedisTemplate 注入 Spring 123456789@Configurationpublic class RedisConfig { @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(LettuceConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;String, Object&gt;(); ...... return redisTemplate; }} @Import我们在翻看Spring源码的过程中，经常会看到@Import注解，它也可以用来将第三方jar包注入Spring，但是它只可以作用在类上。 例如在注解EnableSpringConfigured上就包含了@Import注解，用于将SpringConfiguredConfiguration配置文件加载进Spring容器。 12@Import(SpringConfiguredConfiguration.class)public @interface EnableSpringConfigured {} @Import的value值是一个数组，一个一个注入比较繁琐，因此我们可以搭配ImportSelector接口来使用，用法如下： 12345678910@Configuration@Import(MyImportSelector.class)public class MyConfig {}public class MyImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { return new String[]{&quot;org.springframework.demo.model.Teacher&quot;,&quot;org.springframework.demo.model.Student&quot;}; }} 其中selectImports方法返回的数组就会通过@Import注解注入到Spring容器中。 无独有偶，ImportBeanDefinitionRegistrar接口也为我们提供了注入bean的方法。 1234@Import(AspectJAutoProxyRegistrar.class)public @interface EnableAspectJAutoProxy { ......} 我们点击AspectJAutoProxyRegistrar类，发现它实现了ImportBeanDefinitionRegistrar接口，它的registerBeanDefinitions方法便是注入bean的过程，可以参考下。 如果觉得源代码比较难懂，可以看一下我们自定义的类 1234567891011121314@Configuration@Import(value = {MyImportBeanDefinitionRegistrar.class})public class MyConfig {}public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { RootBeanDefinition tDefinition = new RootBeanDefinition(Teacher.class); // 注册 Bean，并指定bean的名称和类型 registry.registerBeanDefinition(&quot;teacher&quot;, tDefinition); } }} 3.FactoryBean提到FactoryBean，就不得不与BeanFactory比较一番。 BeanFactory : 是 Factory， IOC容器或者对象工厂，所有的Bean都由它进行管理 FactoryBean : 是Bean ，是一个能产生或者修饰对象生成的工厂 Bean，实现与工厂模式和修饰器模式类似 那么FactoryBean是如何实现bean注入的呢？ 先定义实现了FactoryBean接口的类 12345678910111213141516public class TeacherFactoryBean implements FactoryBean&lt;Teacher&gt; { /** * 返回此工厂管理的对象实例 **/ @Override public Teacher getObject() throws Exception { return new Teacher(); } /** * 返回此 FactoryBean 创建的对象的类型 **/ @Override public Class&lt;?&gt; getObjectType() { return Teacher.class; }} 然后通过 @Configuration + @Bean的方式将TeacherFactoryBean加入到容器中 1234567@Configurationpublic class MyConfig { @Bean public TeacherFactoryBean teacherFactoryBean(){ return new TeacherFactoryBean(); }} 注意：我们没有向容器中注入Teacher, 而是直接注入的TeacherFactoryBean，然后从容器中拿Teacher这个类型的bean，成功运行。 4.BDRegistryPostProcessor看到这个接口，不知道对于翻看过Spring源码的你来说熟不熟悉。如果不熟悉的话请往下看，要是熟悉的话就再看一遍吧😃。 123456789public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor { // 注册bean到spring容器中 void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException;}@FunctionalInterfacepublic interface BeanFactoryPostProcessor { void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;} BeanFactoryPostProcessor接口是BeanFactory的后置处理器，方法postProcessBeanFactory对bean的定义进行控制。今天我们重点来看看postProcessBeanDefinitionRegistry方法：它的参数是BeanDefinitionRegistry，顾名思义就是与BeanDefinition注册相关的。 通过观察该类，我们发现它里边包含了registerBeanDefinition方法，这个不就是我们想要的吗？为了能更好的使用该接口来达到注入bean的目的，我们先来看看Spring是如何操作此接口的。 看下invokeBeanFactoryPostProcessors方法，会发现没有实现PriorityOrdered和Ordered的bean（这种跟我们自定义的实现类有关）会执行以下代码。 12345678910111213while (reiterate) { ...... invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); ......}private static void invokeBeanDefinitionRegistryPostProcessors( Collection&lt;? extends BeanDefinitionRegistryPostProcessor&gt; postProcessors, BeanDefinitionRegistry registry) { for (BeanDefinitionRegistryPostProcessor postProcessor : postProcessors) { postProcessor.postProcessBeanDefinitionRegistry(registry); }} 会发现实现了BeanDefinitionRegistryPostProcessor接口的bean，其postProcessBeanDefinitionRegistry方法会被调用，也就是说如果我们自定义接口实现该接口，它的postProcessBeanDefinitionRegistry方法也会被执行。 实战话不多说，直接上代码。自定义接口实现类 123456789101112131415161718public class MyBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor { /** * 初始化过程中先执行 **/ @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Teacher.class); //Teacher 的定义注册到spring容器中 registry.registerBeanDefinition(&quot;teacher&quot;, rootBeanDefinition); } /** * 初始化过程中后执行 **/ @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {}} 启动类代码 1234567891011public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); MyBeanDefinitionRegistryPostProcessor postProcessor = new MyBeanDefinitionRegistryPostProcessor(); //将自定义实现类加入 Spring 容器 context.addBeanFactoryPostProcessor(postProcessor); context.refresh(); Teacher bean = context.getBean(Teacher.class); System.out.println(bean);}// org.springframework.demo.model.Teacher@2473d930 发现已经注入到Spring容器中了。","link":"/2021/06/24/30000Spring/30002%20Bean%E6%B3%A8%E5%85%A5Spring%E5%AE%B9%E5%99%A84%E7%A7%8D%E6%96%B9%E5%BC%8F/"},{"title":"","text":"Spring Security安全认证之SecurityContextHolder","link":"/2022/04/29/30000Spring/30100%20SpringSecurity/"},{"title":"","text":"JWT","link":"/2022/04/29/30000Spring/30101%20JWT/"},{"title":"","text":"SecurityContextHolder是SpringSecurity最基本的组件了，是用来存放SecurityContext的对象，默认是使用ThreadLocal实现的，这样就保证了本线程内所有的方法都可以获得SecurityContext对象。 SecurityContextHolder还有其他两种模式，分别为SecurityContextHolder.MODE_GLOBAL和SecurityContextHolder.MODE_INHERITABLETHREADLOCAL，前者表示SecurityContextHolder对象 的全局的，应用中所有线程都可以访问；后者用于线程有父子关系的情境中，线程希望自己的子线程和自己有相同的安全性。 3)大部分情况下我们不需要修改默认的配置，ThreadLocal是最常用也是最合适大部分应用的。 使用SecurityContextHolder获取当前登录的用户信息在SecurityContextHolder中保存的是当前访问者的信息。Spring Security使用一个Authentication对象来表示这个信息。一般情况下，我们都不需要创建这个对象，在登录过程中，Spring Security已经创建了该对象并帮我们放到了SecurityContextHolder中。从SecurityContextHolder中获取这个对象也是很简单的。比如，获取当前登录用户的用户名，可以这样 : 123456789101112131415161718// 获取安全上下文对象，就是那个保存在 ThreadLocal 里面的安全上下文对象// 总是不为null(如果不存在，则创建一个authentication属性为null的empty安全上下文对象)SecurityContext securityContext = SecurityContextHolder.getContext(); // 获取当前认证了的 principal(当事人),或者 request token (令牌)// 如果没有认证，会是 null,该例子是认证之后的情况Authentication authentication = securityContext.getAuthentication() // 获取当事人信息对象，返回结果是 Object 类型，但实际上可以是应用程序自定义的带有更多应用相关信息的某个类型。// 很多情况下，该对象是 Spring Security 核心接口 UserDetails 的一个实现类，你可以把 UserDetails 想像// 成我们数据库中保存的一个用户信息到 SecurityContextHolder 中 Spring Security 需要的用户信息格式的// 一个适配器。Object principal = authentication.getPrincipal();if (principal instanceof UserDetails) { String username = ((UserDetails)principal).getUsername();} else { String username = principal.toString();} 修改SecurityContextHolder的工作模式 综上所述，SecurityContextHolder可以工作在以下三种模式之一: 1）MODE_THREADLOCAL (缺省工作模式) 2）MODE_GLOBAL 3) MODE_INHERITABLETHREADLOCAL 4) 修改SecurityContextHolder的工作模式有两种方法 : a: 设置一个系统属性(system.properties) : spring.security.strategy; SecurityContextHolder会自动从该系统属性中尝试获取被设定的工作模式 b: 调用SecurityContextHolder静态方法setStrategyName() 程序化方式主动设置工作模式的方法 SecurityContextHolder存储SecurityContext的方式(默认就是mode_threadlocal) 1）单机系统，即应用从开启到关闭的整个生命周期只有一个用户在使用。由于整个应用只需要保存一个SecurityContext（安全上下文即可） 2）多用户系统，比如典型的Web系统，整个生命周期可能同时有多个用户在使用。这时候应用需要保存多个SecurityContext（安全上下文），需要利用ThreadLocal进行保存，每个线程都可以利用ThreadLocal获取其自己的SecurityContext，及安全上下文。 比如我们使用jwt实现登录功能的时候，我们可以将用户信息保存在安全上下文中 由上面代码可以发现我们获取用户的信息之后，通过SecurityContextHolder.getContext().setAuthentication(authentication);方式将用户相关的信息存放到系统的安全上下文中，并且由于 SecurityContextHolder默认是mode_threadlocal模式，那么会将所有登录的用户信息都保存，每个登录的用户都可以通过SecurityContextHolder.getContext().getAuthentication();方式获取当前自己保存的用户信息","link":"/2022/04/29/30000Spring/30101%20SecurityContextHolder/"},{"title":"","text":"常用注解 @EnableAutoConfiguration @ComponentScan @Bean @Controller @Service @ResponseBody @Autowired 如何再SpringBoot启动后运行特定方法12345678910111213141516171819202122232425262728293031/Users/admin/.m2/repository/org/springframework/boot/spring-boot/2.1.1.RELEASE/spring-boot-2.1.1.RELEASE.jar!/org/springframework/boot/SpringApplication.class:490public class Application implements ApplicationRunner { @Override public void run(ApplicationArguments args) throws Exception {}}public class Application implements CommandLineRunner { @Override public void run(String... args) throws Exception {}}private void callRunners(ApplicationContext context, ApplicationArguments args) { List&lt;Object&gt; runners = new ArrayList(); runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); AnnotationAwareOrderComparator.sort(runners); Iterator var4 = (new LinkedHashSet(runners)).iterator(); while(var4.hasNext()) { Object runner = var4.next(); if (runner instanceof ApplicationRunner) { this.callRunner((ApplicationRunner)runner, args); } if (runner instanceof CommandLineRunner) { this.callRunner((CommandLineRunner)runner, args); } }} 1．掌握SpringBoot与DataSource数据源整合。2．掌握SpringBoot与MyBatis开发框架整合。3．掌握SpringBoot与SpringDataJPA开发框架整合。4．掌握SpringBoot与消息组件（ActiveMQ、RabbitMQ、Kafka）整合。5．掌握SpringBoot与邮件服务整合。6．掌握SpringBoot与定时调度服务整合。7．掌握SpringBoot与Redis数据库整合。8．掌握SpringBoot与Restful服务整合。 如何销毁Spring容器中的Bean对象 @Bean注解 123456public class testBean implements { @Override public void destroy() {}}","link":"/2021/11/27/30000Spring/Spring/"},{"title":"","text":"Spring OXM实践 示例代码仓库： https://gitee.com/tonygeli/springoxmdemo OXMapper是什么？Spring3.0的一个新特性是O/XMapper。O/X映射器这个概念并不新鲜，O代表Object，X代表XML。它的目的是在Java对象（POJO）和XML文档之间来回转换。 用途？将Java对象转换为一个XML文档 要利用Spring的O/X功能，您需要一个在Java对象和XML之间来回转换的实用程序。Castor就是这样一个流行的第三方工具，本文将使用这个工具。其他这样的工具包括XMLBeans、JavaArchitectureforXMLBinding(JAXB)、JiBX和XStream。 SpringO/X框架只定义两个接口：Marshaller编组和Unmarshaller解组，它们用于执行O/X功能，这是使用这个框架的另一个重大好处。这些接口的实现完全对独立开发人员开放，开发人员可以轻松切换它们而无需修改代码。例如，如果您一开始使用Castor进行O/X转换，但后来发现它缺乏您需要的某个功能，这时您可以切换到XMLBeans而无需任何代码更改。唯一需要做的就是更改Spring配置文件以使用新的O/X框架。 Maven依赖1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-oxm&lt;/artifactId&gt; &lt;version&gt;5.3.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.3.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.thoughtworks.xstream&lt;/groupId&gt; &lt;artifactId&gt;xstream&lt;/artifactId&gt; &lt;version&gt;1.4.18&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; AppConfig.class 123456789101112131415@Configurablepublic class AppConfig { @Bean public XStreamMarshaller XStreamMarshaller() { XStreamMarshaller xStreamMarshaller = new XStreamMarshaller(); xStreamMarshaller.setSupportedClasses(Company.class); // com.thoughtworks.xstream.security.ForbiddenClassException AnyTypePermission anyTypePermission = new AnyTypePermission(); xStreamMarshaller.setTypePermissions(anyTypePermission); return xStreamMarshaller; }} Application.class 12345678910111213141516171819202122public class Application { public static void main(String[] args) throws IOException { AnnotationConfigApplicationContext annoCtx = new AnnotationConfigApplicationContext(); annoCtx.register(AppConfig.class); annoCtx.refresh(); XStreamMarshaller xStreamMarshaller = annoCtx.getBean(XStreamMarshaller.class); Company company = new Company(); company.setId(100); company.setCompanyName(&quot;PQR&quot;); company.setCeoName(&quot;MNO&quot;); company.setNoEmp(50); xStreamMarshaller.marshal(company, new StreamResult(new FileOutputStream(&quot;company.xml&quot;))); System.out.println(&quot;marshal success&quot;); Company outputCompany = (Company) xStreamMarshaller.unmarshal(new StreamSource(new FileInputStream(&quot;company.xml&quot;))); System.out.println(outputCompany); }}","link":"/2021/12/28/30000Spring/3033Spring%E9%81%87%E5%88%B0XML%E7%9A%84%E6%95%85%E4%BA%8B/"},{"title":"","text":"参数校验一、Validator参数校验器","link":"/2022/02/21/30000Spring/3034%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/"},{"title":"","text":"Gateway基本介绍什么是API网关？主要起到隔离外部访问与内部系统的作用。解决访问认证、报文转换、访问统计等问题。随着微服务架构概念的提出，API网关成为了微服务架构的一个标配组件。","link":"/2022/03/08/31000SpringCloud/31001Gateway%E4%BB%8B%E7%BB%8D/"},{"title":"ZooKeeper数据模型和节点类型","text":"数据模型和节点类型数据模型：树形结构 zk维护的数据主要有：客户端的回话（session）状态及数据节点（dataNode）信息 zk在内存中构造了DataTree的数据结构，维护着path到dataNode的映射以及dataNode间的树状层级关系，为了提高读取性能，集群中每个服务节点都是将数据全量存储在内存中。所以zk适合读多写少且轻量级数据的应用场景 数据仅存储在内存是很不安全的，zk采用事务日志文件及快照文件的方案来落盘数据，保障数据在不丢失的请求下能快速恢复。 树中的每个节点被称为 Znode 1.Znode兼备文件和目录两种特点。可以做路径标识，也可以存储数据，并可以具有子Znode。具有增删改查等操作。2.Znode具有原子性操作，读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每个节点都拥有自己的ACL（访问控制列表），这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作3.Znode数据存储大小有限制，每个Znode的数据大小之多1M，常规使用远小于此值。4.Znode通过路径引用，路径必须是绝对的。 持久节点：一旦创建，该数据节点会一直存储在zk服务器上，即使会话关闭了，该节点也不会被删除 临时节点：当创建该节点的客户端因超时或发生异常而关闭时，该节点也应该在zk上被删除。 有序节点：不是一种单独类型的节点，而是在持久节点和临时节点的基础上，增加了一个节点有序的性质。","link":"/2022/03/11/36000Zookeeper/36001ZooKeeper%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B/"},{"title":"ZooKeeper的用户","text":"命名服务、配置管理、集群管理命名服务：通过指定的名字来获取资源或者服务地址。Zookeeper可以创建一个全局唯一的路径，这个路径就可以作为一个名字。被命名的实体可以是集群中的机器，服务的地址，或者是远程的对象等。一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息。 配置管理：项目开发，使用的.properties和xml需要配置很多信息，如数据库连接信息、fps地址端口等。程序分布式部署时，如果把程序的这些配置信息保存在zk的znode节点下，当你修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，改变配置。 集群管理：集群管理包括集群监控和集群控制，就是监控集群机器状态，剔除机器和加入机器。zk可以方便集群机器的管理，它可以实时监控znode节点的变化，一旦发生有机器挂了，该机器就会与zk断开连接，对应的目录节点会被删除，其他机器收到通知。","link":"/2022/03/11/36000Zookeeper/36002ZooKeeper%E7%9A%84%E7%94%A8%E6%88%B7/"},{"title":"ZooKeeper的数据同步原理","text":"ZK的数据同步原理 peerLastZxid:Learner服务器（Follower或Observer）最后处理的zxid minCommittedLog:Leader服务器proposal缓存队列committedLog中最小的zxid maxCommittedLog:Leader服务器proposal缓存队列committedLog中最大的zxid ZK中数据同步一共有四类： DIFF：直接差异化同步 peerLastZxid介于minCommittedLog和maxCommittedLog TRUNC+DIFF：先回滚再差异化同步 TRUNC 仅回滚同步 peerLastZxid大于maxCommittedLog，Leader会要求Learner回滚到ZXID值为maxCommittedLog对应的事务操作 SNAP 全量同步 peerLastZxid小于minCommittedLog","link":"/2021/09/26/36000Zookeeper/36003ZooKeeper%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/"},{"title":"ZooKeeper Watch机制的实现原理","text":"Watch机制的实现原理先进先出FIFO 服务端触发过程： 调用WatchManager中的方法触发数据变更事件 封装了一个具有会话状态、事件类型、数据节点3种属性的WatchedEvent对象，之后查询该节点注册的Watch事件，如果为空说明该节点没有注册过Watch事件。如果存在Watch事件则添加到定义的Watchers集合中，并在WatchManager管理中删除。最后，通过调用process方法向客户端发送通知。 客户端回调过程： 使用SendThread.readResponse()方法来统一出来服务端 ZK分布式锁实现原理 上来直接创建一个锁节点下的一个接一个的临时顺序节点 如果自己不是第一个节点，就对自己上一个节点加监听器 只要上一个节点释放锁，自己就排到前面去了，相当于是一个排队机制。 而且用临时顺序节点，如果某个客户端创建临时顺序节点之后，自己宕机了，zk感知到那个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队。解决了惊群效应。 zk的经典应用场景通过对Zookeeper中丰富的数据节点进行交叉使用，配合Watcher时间通知机制","link":"/2021/11/11/36000Zookeeper/36004ZooKeeper%20Watch%E6%9C%BA%E5%88%B6%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"title":"","text":"这么做的主要的原因是公司受够了fastjson频繁的安全漏洞问题，每一次出现漏洞都要推一次全公司的fastjson强制版本升级，很令公司头疼。 文章的前半部分，我会简单分析各种json解析框架的优劣，并给出企业级项目迁移json框架的几种解决方案。 在文章的后半部分，我会结合这一个月的经验，总结下Gson的使用问题，以及fastjson迁移到Gson踩过的深坑。 为何要放弃fastjson？究其原因，是fastjson漏洞频发，导致了公司内部需要频繁的督促各业务线升级fastjson版本，来防止安全问题。 fastjson在2020年频繁暴露安全漏洞，此漏洞可以绕过autoType开关来实现反序列化远程代码执行并获取服务器访问权限。 从2019年7月份发布的v1.2.59一直到2020年6月份发布的 v1.2.71 ，每个版本的升级中都有关于AutoType的升级，涉及13个正式版本。 相比之下，其他的json框架，如Gson和Jackson，漏洞数量少很多，高危漏洞也比较少，这是公司想要替换框架的主要原因。 fastjson替代方案本文主要讨论Gson替换fastjson框架的实战问题，所以在这里不展开详细讨论各种json框架的优劣，只给出结论。 经过评估，主要有Jackson和Gson两种json框架放入考虑范围内，与fastjson进行对比。","link":"/2021/01/19/90000%E6%96%87%E7%AB%A0/4.fastjson%E6%9B%BF%E6%8D%A2Gson%E5%AE%9E%E6%88%98/"},{"title":"Hexo配置与Icarus主题","text":"1.本地初始化项目12345678$ mac install nmp # 如果没有安装过npm$ hexo init blog # 初始化项目$ cd blog$ npm install $ hexo server # 启动项目$ hexo new &quot;New Post&quot; # 新建一个文章$ hexo generate # 生成静态文件 2.Icarus主题配置12345$ npm install -S hexo-theme-icarus # 通过npm安装主题# 需要查看下 themes/icarus 是否创建成功# 如果没有创建成功，需要执行下git clone$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus$ hexo server # 运行下看看效果 1234567891011title: 标题cover: 缩略图date: 时间toc: true 打开文章目录category: 文章类型- aaatags: 文章标签- Hexo- Icarus 3.部署到Github3.1.修改本地配置文件12345# _config.yml 加上git仓库配置deploy: type: git repo: git@github.com:xxxx.io.git branch: master 3.2.部署1$ hexo deploy 3.3.ERROR Deployer not found: git解决方案这是因为没安装hexo-deployer-git插件，在站点目录下输入下面的插件安装就好了： 1npm install hexo-deployer-git --save 然后在使用Hexo d命令就可以推送了。 4.自定义域名Github Page如何自定义域名，参考下图 问题：每次hexo d后，域名都需要重新配置 解决：在source目录下添加一个名为CNAME的文件，没有后缀，里面填上域名","link":"/2022/05/06/90000%E6%96%87%E7%AB%A0/90000Hexo%E9%85%8D%E7%BD%AE%E4%B8%8EIcarus%E4%B8%BB%E9%A2%98/"},{"title":"90001分布式事务","text":"分布式事务分布式提交的问题在分布式系统中，为了保证数据的高可用，通常会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。在数据有多份副本的情况下，如果网络、服务器或者软件出现故障，会导致部分副本写入成功，部分副本写入失败。这就造成各个副本之间的数据不一致，数据内容冲突，造成事实上的数据不一致。 解决思路可以看到，这里出现问题的主要原因是多个副本之间没有同步机制，可以增加一个协调机制来解决数据不一致问题。下面介绍的两阶段提交和三阶段提交都是通过引入一个协调者来进行协调。 两阶段提交概述 请求阶段事务协调者通知每个参与者准备提交或取消事务，然后进入表决过程，参与者要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种”万事俱备，只欠东风”的状态。请求阶段，参与者将告知协调者自己的决策: 同意(事务参与者本地作业执行成功)或取消（本地作业执行故障）。 提交阶段在该阶段，写调整将基于第一个阶段的投票结果进行决策: 提交或取消。当且仅当所有的参与者同意提交事务，协调者才通知所有的参与者提交事务，否则协调者将通知所有的参与者取消事务。参与者在接收到协调者发来的消息后将执行响应的操作。 解决了哪些问题在正常的情况下，如果第一阶段某些参与者出现问题，那么其他所有参与者都能够知道事务失败了，可以执行取消操作，数据保持了一致性。如果所有的参与者都能够执行成功，那么在提交阶段，所有事务提交，数据也是一致的。 还有哪些问题 阻塞在请求和提交阶段都是阻塞的，多个参与者都要进行决策，阻塞时间边长，如果出现网络问题，长时间等待阻塞。 单点故障协调者的作用非常重要，协调者挂了，参与者长期阻塞。 不一致如果协调者在提交阶段中间挂了，某些参与者收到了提交命令，某些参与者没有收到，还是会出现数据不一致情况。 三阶段提交概述三阶段提交协议在协调者和参与者中都引入超时机制，并且把两阶段提交协议的第一个阶段分成了两步: 询问，然后再锁资源，最后真正提交。 canCommit阶段协调者向参与者发送commit请求，参与者如果可以提交就返回yes响应，否则返回no响应 preCommit阶段协调者根据参与者canCommit阶段的响应来决定是否可以继续事务的preCommit操作。根据响应情况，有下面两种可能: 协调者从所有参与者得到的反馈都是yes:那么进行事务的预执行，协调者向所有参与者发送preCommit请求，并进入prepared阶段。参与者接收到preCommit请求后会执行事务操作，并将undo和redo信息记录到事务日志中。如果一个参与者成功地执行了事务操作，则返回ACK响应，同时开始等待最终指令。协调者从所有参与者得到的反馈有一个是No或是等待超时之后协调者都没收到响应:那么就要中断事务，协调者向所有的参与者发送abort请求。参与者在收到来自协调者的abort请求，或超时后仍未收到协调者请求，执行事务中断。3. doCommit阶段协调者根据参与者preCommit阶段的响应来决定是否可以继续事务的doCommit操作。根据响应情况，有下面两种可能: 协调者从参与者得到了ACK的反馈:协调者接收到参与者发送的ACK响应，那么它将从预提交状态进入到提交状态，并向所有参与者发送doCommit请求。参与者接收到doCommit请求后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源，并向协调者发送haveCommitted的ACK响应。协调者收到这个ACK响应之后，完成任务。协调者从参与者没有得到ACK的反馈, 也可能是接收者发送的不是ACK响应，也可能是响应超时:执行事务中断。 解决了哪些问题减少阻塞如果第一个阶段，参与者挂了，或者协调者挂了，最终数据都是一致的，因为超时机制，也不会一直等待。 还有哪些问题如果进入PreCommit后，Coordinator发出的是abort请求，假设只有一个Cohort收到并进行了abort操作，而其他对于系统状态未知的Cohort会根据3PC选择继续Commit，此时系统状态发生不一致性。","link":"/2022/04/19/90000%E6%96%87%E7%AB%A0/90001%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"90002 快速了解TCP的流量控制与拥塞控制","text":"快速了解TCP的流量控制与拥塞控制有关TCP你不能不知道的三次握手和四次挥手问题，关于三次握手与四次挥手你要知道这些 流量控制1. 滑动窗口 数据的传送过程中很可能出现接收方来不及接收的情况，这时就需要对发送方进行控制以免数据丢失。利用滑动窗口机制可以很方便地在TCP连接上对发送方的流量进行控制。TCP的窗口单位是字节，不是报文段，发送方的发送窗口不能超过接收方给出的接收窗口的数值。 TCP规定，即使设置为零窗口，也必须接收以下几种报文段： 零窗口探测报文段 确认报文段 携带紧急数据的报文段 确认丢失和确认迟到 持续计时器 存在这样一种情况：发送方接收到零窗口报文之后将发送窗口设置为0，停止发送数据。但等到接收方有足够缓存，发送了非零窗口大小的报文，但是这个报文中途丢失，那么发送方的发送窗口就一直为0导致死锁。 为此，TCP为每一个连接设有一个持续计时器(Persistence Timer)：当TCP连接的一方收到对方的零窗口通知时就启动持续计时器。若持续计时器时间到期，就发送一个零窗口探测报文段(携有1字节的数据)，那么收到这个报文段的一方就在确认这个探测报文段时给出了现在的窗口值。若窗口仍然是零，则收到这个报文段的一方就重新设置持续计时器；若窗口不是零，则死锁的僵局就可以打破了。 2. 延迟ACK 如果TCP对每个数据包都发送一个ACK确认，那么只是一个单独的数据包为了发送一个ACK代价比较高，所以TCP会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ACK，如果在延迟ACK定时器触发时候，发现ACK尚未发送，则立即单独发送； 延迟ACK好处： 避免糊涂窗口综合症。 发送数据的时候将ACK捎带发送，不必单独发送ACK。如果延迟时间内有多个数据段到达，那么允许协议栈发送一个ACK确认多个报文段。减少流量消耗。 糊涂窗口综合症：TCP接收方的缓存已满，而交互式的应用进程一次只从接收缓存中读取1字节（这样就使接收缓存空间仅腾出1字节），然后向发送方发送确认，并把窗口设置为1个字节（但发送的数据报为40字节的的话）。当发送方又发来1个字节的数据（发送方的IP数据报是41字节），接收方发回确认，仍然将窗口设置为1个字节。这样，网络的效率很低。要解决这个问题，可让接收方等待一段时间，使得或者接收缓存已有足够空间容纳一个最长的报文段或者等到接收方缓存已有一半的空闲空间。只要出现这两种情况，接收方就发回确认报文，并向发送方通知当前的窗口大小。此外，发送方也不要发送太小的报文段，而是把数据报积累成足够大的报文段，或达到接收方缓存的空间的一半大小。 拥塞控制 拥塞控制与流量控制的区别 : 拥塞控制是防止过多的数据注入到网络中，可以使网络中的路由器或链路不致过载，是一个全局性的过程。 流量控制是点对点通信量的控制，是一个端到端的问题，主要就是抑制发送端发送数据的速率，以便接收端来得及接收。 拥塞控制的作用 拥塞控制是为了防止过多的数据注入到网络中，这样可以使网络中的路由器或者链路不至于过载。 拥塞控制的算法 我们假定: 数据单方向传送，而另外一个方向只传送确认。 接收方总是有足够大的缓存空间，因为发送窗口的大小由网络的拥塞程度来决定。 发送方的发送窗口的上限值应当取为接收方窗口rwnd和拥塞窗口cwnd这两个变量中较小的一个，即发送窗口的上限值为Min[rwnd, cwnd] 当rwnd &lt; cwnd时，是接收方的接收能力限制发送窗口的最大值 当cwnd &lt; rwnd时，则是网络的拥塞限制发送窗口的最大值 拥塞控制的过程一共涉及了4种算法: 慢启动 拥塞避免 快重传 快恢复 1. 慢启动 发送方维护一个拥塞窗口cwnd的状态变量，拥塞窗口的大小取决于网络的拥塞程度，动态变化。通过逐渐增加cwnd的大小来探测可用的网络容量，防止连接开始时采用不合适的发送量导致网络拥塞。 当主机开始发送数据时，如果通过较大的发送窗口立即将全部数据字节都注入到网络中，由于不清楚网络状况，有可能引起网络拥塞。较好的方法是试探，从小到大逐渐增大发送端拥塞窗口的cwnd数值。 例如：开始发送方先设置cwnd=1，发送第一个报文段M1，接收方接收到M1后，ACK返回给发送端，发送端将cwnd增加到2，接着发送方发送M2，再次接受到ACK后将cwnd增加到4…慢启动算法每经过一个传输轮次，拥塞窗口cwnd就加倍。 当rwnd足够大时，为防止拥塞窗口cwind的增长引起网络拥塞，还需要另外一个变量，慢开始门限ssthresh 当cwnd＜ssthresh，使用慢开始算法 当cwnd=ssthresh，既可使用慢开始算法，也可以使用拥塞避免算法 当cwnd＞ssthresh，使用拥塞避免算法 首次慢启动的ssthresh值，可以参阅网上的各种讨论，限于篇幅，本文不作介绍~ 2.拥塞避免 控制过程: TCP连接初始化，将拥塞窗口cwnd设置为1个报文段，即cwnd=1 执行慢开始算法，cwnd按指数规律增长，直到cwnd == ssthresh时，开始拥塞避免算法，cwnd按线性规律增长 当网络发生阻塞，把ssthresh值更新为拥塞前cwnd的一半(12=24/2)，cwnd重新设置为1，再按照(2)执行 让拥塞窗口cwnd缓慢地增大，每经过一个往返时间RTT就把发送方的拥塞窗口cwnd+1，而不是加倍。这样拥塞窗口cwnd线性缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢地多。 无论慢启动开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞(没收到ACK)，就把慢启动门限ssthresh设置为出现拥塞时的cwnd的一半。然后把拥塞窗口cwnd重新设置为1，执行慢启动算法。这样做的目的是能迅速的减少主机向网络中传输数据，使发生拥塞的路由器能够把队列中堆积的分组处理完毕。拥塞窗口是按照线性的规律增长，比慢启动算法拥塞窗口增长快的多。 拥塞避免是由指数增长拉低到线性增长，降低出现拥塞的可能，并不是能完全避免网络拥塞 3.快重传 一条TCP连接有时会因等待重传计时器的超时而空闲较长的时间，慢开始和拥塞避免无法很好地解决这类问题，因此提出了快重传和快恢复的拥塞控制方法。 为使发送方及早知道有报文没有达到对方，快重传算法首先要求接受方每收到一个报文段后就立即发出重复确认。快重传算法并非取消了重传机制，只是在某些情况下更早地重传丢失的报文段。即，当TCP源端收到3个相同的ACK确认时，即认为有数据包丢失，则源端重传丢失的数据包，而不必等待RTO(Retransmission Timeout)超时。由于发送方尽早重传未被确认的报文段。因此，采用快重传后可以使整个网络吞吐量提高20% 快重传算法要求首先接收方收到一个失序的报文段后就立刻发出重复确认，而不要等待自己发送数据时才进行捎带确认。接收方成功的接受了发送方发送来的M1、M2并且分别给发送了ACK，现在接收方没有收到M3，而接收到了M4，显然接收方不能确认M4，因为M4是失序的报文段。如果根据可靠性传输原理接收方什么都不做，但是按照快速重传算法，在收到M4、M5等报文段的时候，不断重复的向发送方发送M2的ACK,如果接收方一连收到三个重复的ACK,那么发送方不必等待重传计时器到期，由于发送方尽早重传未被确认的报文段。 4.快恢复 快恢复算法控制过程: 当发送方连续收到3个重复确认时，发送方认为网络很可能没有发生拥塞，因此不执行慢启动。而是把cwnd值设为新的门限值，然后执行拥塞避免算法，cwnd值线性增大，避免了当网络拥塞不够严重时采用”慢启动”算法而造成过大地减小发送窗口尺寸的现象，这就是快恢复。","link":"/2021/07/14/90000%E6%96%87%E7%AB%A0/90002%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3TCP%E7%9A%84%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E4%B8%8E%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/"},{"title":"90003彻底搞懂NIO效率高的原理","text":"彻底搞懂NIO效率高的原理前言 这篇文章读不懂的没关系，可以先收藏一下。笔者准备介绍完epoll和NIO等知识点，然后写一篇Java网络IO模型的介绍，这样可以使Java网络IO的知识体系更加地完整和严谨。初学者也可以等看完IO模型介绍的博客之后，再回头看这些博客，会更加有收获。 NIO相比BIO的优势NIO（Non-blocking I/O，在Java领域，也称为New I/O），是一种同步非阻塞的I/O模型，也是I/O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O处理问题的有效方式。 bio与nio 面向流与面向缓冲 Java NIO和BIO之间第一个最大的区别是，BIO是面向流的，NIO是面向缓冲区的。 JavaIO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。Java NIO的缓冲读取方法略有不同。数据读取到一个缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 有关面向缓冲读取数据的示例和注意点，可以点击查看TCP粘拆包详解与Netty代码示例 阻塞IO与非阻塞IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或write()时，该线程被阻塞，直到有数据被读取或者数据写入。该线程在阻塞期间不能做其他事情。而Java NIO的非阻塞模式，如果通道没有东西可读，或不可写，读写函数马上返回，而不会阻塞，这个线程可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程可以管理多个输入和输出通道（channel），即IO多路复用的原理。 零拷贝 在传统的文件IO操作中，我们都是调用操作系统提供的底层标准IO系统调用函数read()、write() ，此时调用此函数的进程（在JAVA中即java进程）由当前的用户态切换到内核态，然后OS的内核代码负责将相应的文件数据读取到内核的IO缓冲区，然后再把数据从内核IO缓冲区拷贝到进程的私有地址空间中去，这样便完成了一次IO操作。 IO 而NIO的零拷贝与传统的文件IO操作最大的不同之处就在于它虽然也是要从磁盘读取数据，但是它并不需要将数据读取到OS内核缓冲区，而是直接将进程的用户私有地址空间中的一部分区域与文件对象建立起映射关系，这样直接从内存中读写文件，速度大幅度提升。 NIO 详细的解析，之后会有单独的博客进行讲解 NIO的核心部分Java NIO主要由以下三个核心部分组成： Channel Buffer Selector Channel 基本上，所有的IO在NIO中都从一个Channel开始。数据可以从Channel读到Buffer中，也可以从Buffer写到Channel中。这里有个图示： channel与buffer Channel和Buffer有好几种类型。下面是Java NIO中的一些主要Channel的实现： FileChannel(file) DatagramChannel(UDP) SocketChannel(TCP) ServerSocketChannel(TCP) 这些通道涵盖了UDP和TCP网络IO以及文件IO。 最后两个channel的关系。通过ServerSocketChannel.accept() 方法监听新进来的连接。当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel。因此, accept()方法会一直阻塞到有新连接到达。通常不会仅仅只监听一个连接,在while循环中调用 accept()方法. //打开 ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9999)); while(true){ SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel… } //关闭ServerSocketChannel serverSocketChannel.close(); Buffer 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。 Java NIO里关键的Buffer实现： ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些Buffer覆盖了你能通过IO发送的基本数据类型：byte、short、int、long、float、double和char。 为了理解Buffer的工作原理，需要熟悉它的三个属性： capacity position limit position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。 buffer模型 capacity 作为一个内存块，Buffer有个固定的最大值，就是capacity。Buffer只能写capacity个byte、long、char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 position 当写数据到Buffer中时，position表示当前的位置。初始的position值为0。当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1. 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 limit 在写模式下，Buffer的limit表示最多能往Buffer里写多少数据。 写模式下，limit等于capacity。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。 Selector Selector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。例如，在一个聊天服务器中。 这是在一个单线程中使用一个Selector处理3个Channel的图示： Selector 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件例如有新连接进来，数据接收等。 NIO与epoll的关系 Java NIO根据操作系统不同， 针对NIO中的Selector有不同的实现： macosx:KQueueSelectorProvider solaris:DevPollSelectorProvider Linux:EPollSelectorProvider (Linux kernels &gt;= 2.6)或PollSelectorProvider windows:WindowsSelectorProvider 所以不需要特别指定，Oracle JDK会自动选择合适的Selector。 如果想设置特定的Selector，可以设置属性，例如： -Djava.nio.channels.spi.SelectorProvider=sun.nio.ch.EPollSelectorProvider JDK在Linux已经默认使用epoll方式，但是JDK的epoll采用的是水平触发，所以Netty自4.0.16起, Netty为Linux通过JNI的方式提供了native socket transport。Netty重新实现了epoll机制， 采用边缘触发方式 netty epoll transport暴露了更多的nio没有的配置参数，如 TCP_CORK, SO_REUSEADDR等等。 C代码，更少GC，更少synchronized 使用native socket transport的方法很简单，只需将相应的类替换即可。 1234NioEventLoopGroup → EpollEventLoopGroupNioEventLoop → EpollEventLoopNioServerSocketChannel → EpollServerSocketChannelNioSocketChannel → EpollSocketChannel 有关epoll的详细讲解，可以点击查看彻底搞懂epoll高效运行的原理 NIO处理消息的核心思路结合示例代码，总结NIO的核心思路： NIO 模型中通常会有两个线程，每个线程绑定一个轮询器 selector ，在上面例子中serverSelector负责轮询是否有新的连接，clientSelector负责轮询连接是否有数据可读 服务端监测到新的连接之后，不再创建一个新的线程，而是直接将新连接绑定到clientSelector上，这样就不用BIO模型中1w 个while循环在阻塞，参见(1) clientSelector被一个 while 死循环包裹着，如果在某一时刻有多条连接有数据可读，那么通过clientSelector.select(1)方法可以轮询出来，进而批量处理，参见(2) 数据的读写面向 Buffer，参见(3) NIO的示例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class NIOServer { public static void main(String[] args) throws IOException { Selector serverSelector = Selector.open(); Selector clientSelector = Selector.open(); new Thread(() -&gt; { try { // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(8000)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) { // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) &gt; 0) { Set&lt;SelectionKey&gt; set = serverSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { try { // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); } finally { keyIterator.remove(); } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -&gt; { try { while (true) { // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) &gt; 0) { Set&lt;SelectionKey&gt; set = clientSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isReadable()) { try { SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println(Charset.defaultCharset().newDecoder().decode(byteBuffer) .toString()); } finally { keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); } } } } } } catch (IOException ignored) { } }).start(); }} 更多内容，欢迎关注微信公众号：全菜工程师小辉~","link":"/2021/07/14/90000%E6%96%87%E7%AB%A0/90003%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82NIO%E6%95%88%E7%8E%87%E9%AB%98%E7%9A%84%E5%8E%9F%E7%90%86/"},{"title":"90004Python公众号通知","text":"1. 新建应用登陆网页版企业微信 (https://work.weixin.qq.com/)，点击 应用管理 -&gt; 应用 -&gt; 创建应用 上传应用的 logo，输入应用名称，再选择可见范围，成功创建一个告警应用 2. 获取Secret使用 Python 发送告警请求，其实就只使用到两个接口 获取 Token ：https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={corpid}&amp;corpsecret={secret} 发送请求：https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={token} 可以看到，最重要的是 corpid 和 secret: corpid：唯一标识你的企业 secret：应用级的密钥，有了它程序才知道你要发送该企业的哪个应用 corpid 可以通过 我的企业 -&gt; 企业信息 获取 而 secret 获取相对麻烦一点，点击前面创建应用，点击 查看 secret 然后再点击发送就会发送到你的企业微信上 最后将 corpid 和 secret 填入下面的常量中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import jsonimport datetimeimport requests CORP_ID = &quot;&quot;SECRET = &quot;&quot; class WeChatPub: s = requests.session() def __init__(self): self.token = self.get_token() def get_token(self): url = f&quot;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={CORP_ID}&amp;corpsecret={SECRET}&quot; rep = self.s.get(url) if rep.status_code != 200: print(&quot;request failed.&quot;) return return json.loads(rep.content)['access_token'] def send_msg(self, content): url = &quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=&quot; + self.token header = { &quot;Content-Type&quot;: &quot;application/json&quot; } form_data = { &quot;touser&quot;: &quot;@all&quot;, &quot;toparty&quot;: &quot; PartyID1 | PartyID2 &quot;, &quot;totag&quot;: &quot; TagID1 | TagID2 &quot;, &quot;msgtype&quot;: &quot;textcard&quot;, &quot;agentid&quot;: 1000002, &quot;textcard&quot;: { &quot;title&quot;: &quot;服务异常告警&quot;, &quot;description&quot;: content, &quot;url&quot;: &quot;URL&quot;, &quot;btntxt&quot;: &quot;更多&quot; }, &quot;safe&quot;: 0 } rep = self.s.post(url, data=json.dumps(form_data).encode('utf-8'), headers=header) if rep.status_code != 200: print(&quot;request failed.&quot;) return return json.loads(rep.content) 然后就可以通过 send_msg 函数发送消息了。 1234wechat = WeChatPub()now = datetime.datetime.now()timenow = now.strftime('%Y年%m月%d日 %H:%M:%S')wechat.send_msg(f&quot;&lt;div class=\\&quot;gray\\&quot;&gt;{timenow}&lt;/div&gt; &lt;div class=\\&quot;normal\\&quot;&gt;阿里云 cookie 已失效&lt;/div&gt;&lt;div class=\\&quot;highlight\\&quot;&gt;请尽快更换新的 cookie&lt;/div&gt;&quot;) 只要你的企业微信没有关闭通知的权限，那你的手机立马就会弹出这个告警信息。 简单几步就对接了企业微信，实现了手机的实时告警功能，推荐有企业微信的同学使用。 当然一定有更多，更好用的实现方法，我只是我选择了其中一种，大家有不错的思路也可以分享在评论区。","link":"/2022/04/19/90000%E6%96%87%E7%AB%A0/90004Python%E5%85%AC%E4%BC%97%E5%8F%B7%E9%80%9A%E7%9F%A5/"},{"title":"90005Stream代码优化","text":"Stream代码优化问题：filter中带逻辑 1234567891011121314151617181920212223public List&lt;FeedItemVo&gt; getFeeds(Query query,Page page){ List&lt;String&gt; orgiList = new ArrayList&lt;&gt;(); List&lt;FeedItemVo&gt; collect = page.getRecords().stream() .filter(this::addDetail) .map(FeedItemVo::convertVo) .filter(vo -&gt; this.addOrgNames(query.getIsSlow(),orgiList,vo)) .collect(Collectors.toList()); //...其他逻辑 return collect;}private boolean addDetail(FeedItem feed){ vo.setItemCardConf(service.getById(feed.getId())); return true;}private boolean addOrgNames(boolean isSlow,List&lt;String&gt; orgiList,FeedItemVo vo){ if(isShow &amp;&amp; vo.getOrgIds() != null){ orgiList.add(vo.getOrgiName()); } return true;} 如何改善 1.合理换行123456Stream.of(&quot;i&quot;, &quot;am&quot;, &quot;xjjdog&quot;).map(toUpperCase()).map(toBase64()).collect(joining(&quot; &quot;));Stream.of(&quot;i&quot;, &quot;am&quot;, &quot;xjjdog&quot;) .map(toUpperCase()) .map(toBase64()) .collect(joining(&quot; &quot;)); 2. 舍得拆分函数1234567891011public Stream&lt;OrderDto&gt; getOrderByUser(String userId){ return orderRepo.findOrderByUser().stream() .map(this::toOrderDto); // map的方法拆出来，实体转换}public OrderDto toOrderDto(Order order){ OrderDto dto = new OrderDto(); dto.setOrderId(order.getOrderId()); dto.setTitle(order.getTitle().split(&quot;#&quot;)[0]); dto.setCreateDate(order.getCreateDate().getTime()); return dto;} 如果OrderDto的构造函数，参数就是Order的话public OrderDto(Order order) 1234public Stream&lt;OrderDto&gt; getOrderByUser(String userId){ return orderRepo.findOrderByUser().stream() .map(OrderDto::new);} 除了map和flatMap的函数可以做语义化，更多的filter可以使用Predicate去代替。比如： 1234Predicate&lt;Registar&gt; registarIsCorrect = reg -&gt; reg.getRegulationId() != null &amp;&amp; reg.getRegulationId() != 0 &amp;&amp; reg.getType() == 0; registarIsCorrect，就可以当作filter的参数。 3. 合理的使用Optional由于NullPointerException代码中到处充满了这样的代码 Java8引入了Optional类 1234567891011121314151617181920212223if (order != null) { Logistics logistics = order.getLogistics(); if(logistics != null){ Address address = logistics.getAddress(); if (address != null) { Country country = address.getCountry(); if (country != null) { Isocode isocode = country.getIsocode(); if (isocode != null) { return isocode.getNumber(); } } } }}// 替换String result = Optional.ofNullable(order) .flatMap(order-&gt;order.getLogistics()) .flatMap(logistics -&gt; logistics.getAddress()) .flatMap(address -&gt; address.getCountry()) .map(country -&gt; country.getIsocode()) .orElse(Isocode.CHINA.getNumber()); 当你不确定你提供的东西，是不是为空的时候，一个好的习惯是不要返回null 123public Optional&lt;String&gt; getUserName() { return Optional.ofNullable(userName);} 另外，我们要尽量的少使用Optional的get方法，它同样会让代码变丑。 12345678Optional&lt;String&gt; userName = &quot;xjjdog&quot;;String defaultEmail = userName.get() == null ? &quot;&quot;:userName.get() + &quot;@xjjdog.cn&quot;;// 替换Optional&lt;String&gt; userName = &quot;xjjdog&quot;;String defaultEmail = userName .map(e -&gt; e + &quot;@xjjdog.cn&quot;) .orElse(&quot;&quot;); 4. 返回Stream还是返回List？如果你返回的是一个List，比如ArrayList，那么你去修改这个List，会直接影响里面的值，除非你使用不可变的方式对其进行包裹。 1.需要修改，我们推荐直接返回Stream流，而不是返回集合。 2.不需要再做修改，那么直接返回List就是比较好的，比如函数在Controller中。 1234public Stream&lt;User&gt; getAuthUsers(){ ... return Stream.of(users);} 5. 少用或者不用并行流线程安全问题。在迭代的过程中，如果使用了线程不安全的类，那么就容易出现问题。比如下面这段代码，大多数情况下运行都是错误的。 123456789101112List transform(List source){ List dst = new ArrayList&lt;&gt;(); if(CollectionUtils.isEmpty()){ return dst; } source.stream. .parallel() .map(..) .filter(..) .foreach(dst::add); // 改成collect return dst;} 并行流还有一个滥用问题，就是在迭代中执行了耗时非常长的IO任务。在用并行流之前，你有没有一个疑问？既然是并行，那它的线程池是怎么配置的？ 所有的并行流，共用了一个ForkJoinPool。它的大小，默认是CPU个数-1，大多数情况下，是不够用的。 如果有人在并行流上跑了耗时的IO业务，那么你即使执行一个简单的数学运算，也需要排队。关键是，你是没办法阻止项目内的其他同学使用并行流的，也无法知晓他干了什么事情。 那怎么办？我的做法是一刀切，直接禁止。虽然残忍了一些，但它避免了问题。","link":"/2022/05/05/90000%E6%96%87%E7%AB%A0/90005Stream%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96/"},{"title":"90006 TCP粘拆包详解与Netty代码示例","text":"TCP粘拆包详解与Netty代码示例TCP是个“流”协议，所谓流，就是没有界限的一串数据。可以想想河里的流水，是连成一片的，其间并没有分界线。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。 有关TCP的详细讲解，可以点击关于三次握手与四次挥手你要知道这些和快速了解TCP的流量控制与拥塞控制 TCP粘包或拆包的原因 应用程序写入的数据大于套接字缓冲区大小，这将会发生拆包。 应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发生粘包。 进行MSS（最大报文长度）大小的TCP分段，当TCP报文长度-TCP头部长度&gt;MSS的时候将发生拆包。 接收方法不及时读取套接字缓冲区数据，这将发生粘包。 拆包和粘包的形式 第一种情况：接收端正常收到两个数据包，即没有发生拆包和粘包的现象，此种情况不在本文的讨论范围内。 发生拆包 第二种情况：接收端只收到一个数据包，由于TCP是不会出现丢包的，所以这一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。这种情况由于接收端不知道这两个数据包的界限，所以对于接收端来说很难处理。 发生粘包 第三种情况：这种情况有两种表现形式，如下图。接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。这两种情况如果不加特殊处理，对于接收端同样是不好处理的。 发生拆包和粘包 发生拆包和粘包 粘包和拆包的解决办法 发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。 发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。 可以在数据包之间设置边界，添加特殊符号（如：回车符），这样，接收端通过这个边界就可以将不同的数据包拆分开。 Netty中的代码示例 Netty封装了JDK的NIO，是一个异步事件驱动的网络应用框架，用于快速开发可维护的高性能服务器和客户端。一般开发中并不会用JDK原生NIO，原因如下： 使用JDK自带的NIO需要了解太多的概念，编程复杂，一不小心bug横飞 Netty底层IO模型随意切换，而这一切只需要做微小的改动，改改参数，Netty可以直接从NIO模型变身为IO模型 Netty自带的拆包解包，异常检测等机制让你从NIO的繁重细节中脱离出来，让你只需要关心业务逻辑 Netty解决了JDK的很多包括空轮询在内的bug Netty底层对线程，selector做了很多细小的优化，精心设计的reactor线程模型做到非常高效的并发处理 自带各种协议栈让你处理任何一种通用协议都几乎不用亲自动手 Netty社区活跃，遇到问题随时邮件列表或者issue Netty已经历各大rpc框架，消息中间件，分布式通信中间件线上的广泛验证，健壮性无比强大 所以，本文选择演示Netty的编解码代码。 在Netty中，我们定义MessageToByteEncoder的继承类，重写其encode函数，来自定义编码器。 12345678910public class SocketEncoder extends MessageToByteEncoder&lt;Packet&gt; { @Override protected void encode(ChannelHandlerContext channelHandlerContext, NetPacket msg, ByteBuf byteBuf) throws Exception { byte body[] = msg.getBody(); int packetLen = body.length; // 先设置包长度，然后写入二进制数据 byteBuf.writeInt(packetLen); byteBuf.writeBytes(body); }} 在Netty中，我们定义ByteToMessageDecoder的继承类，重写其decode函数，用来自定义解码器。 123456789101112131415161718192021222324252627public class SocketDecoder extends ByteToMessageDecoder { @Override void decode(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf, List&lt;Object&gt; list) throws Exception { int bufLen = byteBuf.readableBytes(); // 解决粘包问题（不够一个包头的长度） // 4字节是报文中使用了一个int表示了报文长度 if (bufLen &lt; 4) { return; } // 标记一下当前的readIndex的位置 byteBuf.markReaderIndex(); int packetLength = byteBuf.readInt(); // 读到的消息体长度如果小于我们传送过来的消息长度，则resetReaderIndex。重置读索引,继续接收 if (byteBuf.readableBytes() &lt; packetLength) { // 配合markReaderIndex使用的。把readIndex重置到mark的地方 byteBuf.resetReaderIndex(); return; } NetPacket netPacket = new NetPacket(); netPacket.setPacketLen(packetLength); // 传送过来数据的长度，满足我们的要求了 byte body[] = new byte[packetLength]; byteBuf.readBytes(body); netPacket.setBody(body); list.add(netPacket); }}","link":"/2021/07/14/90000%E6%96%87%E7%AB%A0/90006TCP%E7%B2%98%E6%8B%86%E5%8C%85%E8%AF%A6%E8%A7%A3%E4%B8%8ENetty%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/"},{"title":"90007 SpringBoot自动配置原理","text":"前言 自从有了 SpringBoot 之后，咋们就起飞了！各种零配置开箱即用，而我们之所以开发起来能够这么爽，自动配置的功劳少不了，今天我们就一起来讨论一下 SpringBoot 自动配置原理。 本文主要分为三大部分： SpringBoot 源码常用注解拾遗 SpringBoot 启动过程 SpringBoot 自动配置原理 一. SpringBoot 源码常用注解拾遗这部分主要讲一下 SpringBoot 源码中经常使用到的注解，以扫清后面阅读源码时候的障碍。 组合注解 当可能大量同时使用到几个注解到同一个类上，就可以考虑将这几个注解到别的注解上。被注解的注解我们就称之为组合注解。 元注解：可以注解到别的注解上的注解。 组合注解：被注解的注解我们就称之为组合注解。 @Value 【Spring 提供】 @Value 就相当于传统 xml 配置文件中的 value 字段。 假设存在代码： 123456@Component public class Person { @Value(&quot;i am name&quot;) private String name; } 上面代码等价于的配置文件： 123&lt;bean class=&quot;Person&quot;&gt; &lt;property name =&quot;name&quot; value=&quot;i am name&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 我们知道配置文件中的 value 的取值可以是： 字面量 通过 ${key} 方式从环境变量中获取值 通过 ${key} 方式全局配置文件中获取值 #{SpEL} 所以，我们就可以通过 @Value(${key}) 的方式获取全局配置文件中的指定配置项。 @ConfigurationProperties 【SpringBoot 提供】如果我们需要取 N 个配置项，通过 @Value 的方式去配置项需要一个一个去取，这就显得有点 low 了。我们可以使用 @ConfigurationProperties 。 标有 @ConfigurationProperties 的类的所有属性和配置文件中相关的配置项进行绑定。（默认从全局配置文件中获取配置值），绑定之后我们就可以通过这个类去访问全局配置文件中的属性值了。 下面看一个实例： 在主配置文件中添加如下配置 123person.name=kundy person.age=13 person.sex=male 创建配置类，由于篇幅问题这里省略了 setter、getter 方法，但是实际开发中这个是必须的，否则无法成功注入。另外，@Component 这个注解也还是需要添加的。 12345678@Component @ConfigurationProperties(prefix = &quot;person&quot;) public class Person { private String name; private Integer age; private String sex; } 这里 @ConfigurationProperties 有一个 prefix 参数，主要是用来指定该配置项在配置文件中的前缀。 测试，在 SpringBoot 环境中，编写个测试方法，注入 Person 类，即可通过 Person 对象取到配置文件的值。 @Import 【Spring 提供】 @Import 注解支持导入普通 java 类，并将其声明成一个bean。主要用于将多个分散的 java config 配置类融合成一个更大的 config 类。 @Import 注解在 4.2 之前只支持导入配置类。 在4.2之后 @Import 注解支持导入普通的 java 类,并将其声明成一个 bean。 **@Import 三种使用方式 ** 直接导入普通的 Java 类。 配合自定义的 ImportSelector 使用。 配合 ImportBeanDefinitionRegistrar 使用。 1. 直接导入普通的 Java 类 创建一个普通的 Java 类。 1234567public class Circle { public void sayHi() { System.out.println(&quot;Circle sayHi()&quot;); } } 创建一个配置类，里面没有显式声明任何的 Bean，然后将刚才创建的 Circle 导入。 12345@Import({Circle.class}) @Configuration public class MainConfig { } 创建测试类。 12345public static void main(String[] args) { ApplicationContext context = new AnnotationConfigApplicationContext(MainConfig.class); Circle circle = context.getBean(Circle.class); circle.sayHi(); } 运行结果： Circle sayHi() 可以看到我们顺利的从 IOC 容器中获取到了 Circle 对象，证明我们在配置类中导入的 Circle 类，确实被声明为了一个 Bean。 2. 配合自定义的 ImportSelector 使用 ImportSelector 是一个接口，该接口中只有一个 selectImports 方法，用于返回全类名数组。所以利用该特性我们可以给容器动态导入 N 个 Bean。 创建普通 Java 类 Triangle。 1234567public class Triangle { public void sayHi(){ System.out.println(&quot;Triangle sayHi()&quot;); } } 创建 ImportSelector 实现类，selectImports 返回 Triangle 的全类名。 12345678public class MyImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { return new String[]{&quot;annotation.importannotation.waytwo.Triangle&quot;}; } } 创建配置类，在原来的基础上还导入了 MyImportSelector。 12345@Import({Circle.class,MyImportSelector.class}) @Configuration public class MainConfigTwo { } 创建测试类 123456789public static void main(String[] args) { ApplicationContext context = new AnnotationConfigApplicationContext(MainConfigTwo.class); Circle circle = context.getBean(Circle.class); Triangle triangle = context.getBean(Triangle.class); circle.sayHi(); triangle.sayHi(); } 运行结果： Circle sayHi()Triangle sayHi() 可以看到 Triangle 对象也被 IOC 容器成功的实例化出来了。 3. 配合 ImportBeanDefinitionRegistrar 使用 ImportBeanDefinitionRegistrar 也是一个接口，它可以手动注册bean到容器中，从而我们可以对类进行个性化的定制。(需要搭配 @Import 与 @Configuration 一起使用。） 创建普通 Java 类 Rectangle。 12345public class Rectangle { public void sayHi() { System.out.println(&quot;Rectangle sayHi()&quot;); } } 创建 ImportBeanDefinitionRegistrar 实现类，实现方法直接手动注册一个名叫 rectangle 的 Bean 到 IOC 容器中。 1234567891011public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata annotationMetadata, BeanDefinitionRegistry beanDefinitionRegistry) { RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(Rectangle.class); // 注册一个名字叫做 rectangle 的 bean beanDefinitionRegistry.registerBeanDefinition(&quot;rectangle&quot;, rootBeanDefinition); } } 创建配置类，导入 MyImportBeanDefinitionRegistrar 类。 12345@Import({Circle.class, MyImportSelector.class, MyImportBeanDefinitionRegistrar.class}) @Configuration public class MainConfigThree { } 创建测试类。","link":"/2021/01/04/90000%E6%96%87%E7%AB%A0/90007SpringBoot%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86/"},{"title":"90008 线程的sleep操作实现原理","text":"线程的sleep操作实现原理，休眠0秒有什么意义？sleep操作是我们经常会使用的操作，特别是在开发环境中调试的时候，有时为了模拟长时间的执行就会使用sleep来模拟。该操作对应着java.lang.Thread类的sleep本地方法，该方法能使当前线程睡眠指定的时间。 先通过一个简单的例子来看sleep的操作，代码如下，我们让主线程睡眠3000ms，即3秒。于是主线程先输出“是当前线程睡眠3000ms”，然后暂停三秒钟，最后再输出“睡眠结束”。 sleep方法的要点该方法只针对当前线程，即让当前线程进入休眠状态，哪个线程调用Thread.sleep则哪个线程会睡眠。注意sleep方法传入的睡眠时间并不精准，这个取决于操作系统的计时器和调度器。 如果在synchronized块里面进行sleep操作，或在已获得锁的线程中执行sleep操作，都不会让线程失去锁，这点与Object.wait方法不同。 当前线程执行sleep操作进入睡眠状态后，其它线程能够中断当前线程，使其解除睡眠状态并抛出InterruptedException异常。 不会释放锁前面说到sleep操作不会使得当前线程释放锁，现在我们看一个例子，该例子通过synchronized同步块实现锁机制。主线程创建thread1和thread2两个线程，其中thread1比thread2先启动。thread1获取锁后输出”thread1 gets the lock.”并开始睡眠三秒。接着thread2启动，但它会因为获取不到锁而阻塞，直到thread1睡眠结束输出”thread1 releases the lock.”并释放锁，thread2才能获得锁往下执行，输出”thread2 gets the lock 3 second later.”。 关于sleep(0)sleep方法传入的时间参数值必须大于等于0，正常情况下我们都会传入大于0的值，但有时我们也会看到竟然有传入0的。0？是不是表示不睡眠呢？那不睡眠的话又为什么要执行sleep操作呢？实际上sleep(0)并非指睡眠0秒钟，它的意义是让出该轮cpu时间，也就是说它的意义与yield方法相同，而JVM实现的时候也可以用yield操作来代替。下面例子中MyThread会不断让出自己的CPU时间，而主线程则得到更多的执行时间，这个过程一共输出100次“main thread”，而仅输出几次“yield cpu time”。 总结睡眠不会释放当前线程已获得的锁，并且sleep方法支持中断。","link":"/2020/11/21/90000%E6%96%87%E7%AB%A0/90008%E7%BA%BF%E7%A8%8B%E7%9A%84sleep%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"title":"90009用什么数据类型存IP地址","text":"如果要存 IP 地址，用什么数据类型比较好？大部人都会答错！在看高性能MySQL第3版（4.1.7节）时，作者建议当存储IPv4地址时，应该使用32位的无符号整数（UNSIGNED INT）来存储IP地址，而不是使用字符串。 但是没有给出具体原因。为了搞清楚这个原因，查了一些资料，记录下来。 相对字符串存储，使用无符号整数来存储有如下的好处： 节省空间，不管是数据存储空间，还是索引存储空间 便于使用范围查询（BETWEEN…AND），且效率更高 通常，在保存IPv4地址时，一个IPv4最小需要7个字符，最大需要15个字符，所以，使用VARCHAR(15)即可。MySQL在保存变长的字符串时，还需要额外的一个字节来保存此字符串的长度。而如果使用无符号整数来存储，只需要4个字节即可。 另外还可以使用4个字段分别存储IPv4中的各部分，但是通常这不管是存储空间和查询效率应该都不是很高（可能有的场景适合使用这种方式存储）。 使用字符串和无符号整数来存储IP的具体性能分析及benchmark，可以看这篇文章。 https://bafford.com/2009/03/09/mysql-performance-benefits-of-storing-integer-ip-addresses/ 使用无符号整数来存储也有缺点： 不便于阅读 需要手动转换 对于转换来说，MySQL提供了相应的函数来把字符串格式的IP转换成整数INET_ATON，以及把整数格式的IP转换成字符串的INET_NTOA。如下所示： 123456789101112131415mysql&gt; select inet_aton('192.168.0.1');+--------------------------+| inet_aton('192.168.0.1') |+--------------------------+| 3232235521 |+--------------------------+1 row in set (0.00 sec)mysql&gt; select inet_ntoa(3232235521);+-----------------------+| inet_ntoa(3232235521) |+-----------------------+| 192.168.0.1 |+-----------------------+1 row in set (0.00 sec) 对于IPv6来说，使用VARBINARY同样可获得相同的好处，同时MySQL也提供了相应的转换函数，即INET6_ATON和INET6_NTOA。 对于转换字符串IPv4和数值类型，可以放在应用层，下面是使用java代码来对二者转换： 12345678910111213141516171819202122232425262728293031323334353637383940package com.mikan;/** * @author Mikan */public class IpLongUtils { /** * 把字符串IP转换成long * * @param ipStr 字符串IP * @return IP对应的long值 */ public static long ip2Long(String ipStr) { String[] ip = ipStr.split(&quot;\\\\.&quot;); return (Long.valueOf(ip[0]) &lt;&lt; 24) + (Long.valueOf(ip[1]) &lt;&lt; 16) + (Long.valueOf(ip[2]) &lt;&lt; 8) + Long.valueOf(ip[3]); } /** * 把IP的long值转换成字符串 * * @param ipLong IP的long值 * @return long值对应的字符串 */ public static String long2Ip(long ipLong) { StringBuilder ip = new StringBuilder(); ip.append(ipLong &gt;&gt;&gt; 24).append(&quot;.&quot;); ip.append((ipLong &gt;&gt;&gt; 16) &amp; 0xFF).append(&quot;.&quot;); ip.append((ipLong &gt;&gt;&gt; 8) &amp; 0xFF).append(&quot;.&quot;); ip.append(ipLong &amp; 0xFF); return ip.toString(); } public static void main(String[] args) { System.out.println(ip2Long(&quot;192.168.0.1&quot;)); System.out.println(long2Ip(3232235521L)); System.out.println(ip2Long(&quot;10.0.0.1&quot;)); }} 输出结果为： 1233232235521192.168.0.1167772161","link":"/2021/09/26/90000%E6%96%87%E7%AB%A0/90009%E7%94%A8%E4%BB%80%E4%B9%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AD%98IP%E5%9C%B0%E5%9D%80/"},{"title":"90010同域下的单点登录","text":"同域下的单点登录一个企业一般情况下只有一个域名，通过二级域名区分不同的系统。比如我们有个域名叫做：a.com，同时有两个业务系统分别为：app1.a.com和app2.a.com。我们要做单点登录（SSO），需要一个登录系统，叫做：sso.a.com。 我们只要在sso.a.com登录，app1.a.com和app2.a.com就也登录了。通过上面的登陆认证机制，我们可以知道，在sso.a.com中登录了，其实是在sso.a.com的服务端的session中记录了登录状态，同时在浏览器端（Browser）的sso.a.com下写入了Cookie。那么我们怎么才能让app1.a.com和app2.a.com登录呢？这里有两个问题： Cookie是不能跨域的，我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的。 sso、app1和app2是不同的应用，它们的session存在自己的应用内，是不共享的。 那么我们如何解决这两个问题呢？针对第一个问题，sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给baidu.com的域设置Cookie。 Cookie的问题解决了，我们再来看看session的问题。我们在sso系统登录了，这时再访问app1，Cookie也带到了app1的服务端（Server），app1的服务端怎么找到这个Cookie对应的Session呢？这里就要把3个系统的Session共享，如图所示。共享Session的解决方案有很多，例如：Spring-Session。这样第2个问题也解决了。 同域下的单点登录就实现了，但这还不是真正的单点登录。 不同域下的单点登录同域下的单点登录是巧用了Cookie顶域的特性。如果是不同域呢？不同域之间Cookie是不共享的，怎么办？ 这里我们就要说一说CAS流程了，这个流程是单点登录的标准流程。 上图是CAS官网上的标准流程，具体流程如下： 用户访问app系统，app系统是需要登录的，但用户现在没有登录。 跳转到CAS server，即SSO登录系统，以后图中的CAS Server我们统一叫做SSO系统。 SSO系统也没有登录，弹出用户登录页。 用户填写用户名、密码，SSO系统进行认证后，将登录状态写入SSO的session，浏览器（Browser）中写入SSO域下的Cookie。 SSO系统登录完成后会生成一个ST（Service Ticket），然后跳转到app系统，同时将ST作为参数传递给app系统。 app系统拿到ST后，从后台向SSO发送请求，验证ST是否有效。 验证通过后，app系统将登录状态写入session并设置app域下的Cookie。 至此，跨域单点登录就完成了。以后我们再访问app系统时，app就是登录的。接下来，我们再看看访问app2系统时的流程。 用户访问app2系统，app2系统没有登录，跳转到SSO。 由于SSO已经登录了，不需要重新登录认证。 SSO生成ST，浏览器跳转到app2系统，并将ST作为参数传递给app2。 app2拿到ST，后台访问SSO，验证ST是否有效。 验证成功后，app2将登录状态写入session，并在app2域下写入Cookie。 这样，app2系统不需要走登录流程，就已经是登录了。SSO，app和app2在不同的域，它们之间的session不共享也是没问题的。 有的同学问我，SSO系统登录后，跳回原业务系统时，带了个参数ST，业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？ 其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。 总结单点登录（SSO）的所有流程都介绍完了，原理大家都清楚了。总结一下单点登录要做的事情： 单点登录（SSO系统）是保障各业务系统的用户资源的安全 。 各个业务系统获得的信息是，这个用户能不能访问我的资源。 单点登录，资源都在各个业务系统这边，不在SSO那一方。 用户在给SSO服务器提供了用户名密码后，作为业务系统并不知道这件事。 SSO随便给业务系统一个ST，那么业务系统是不能确定这个ST是用户伪造的，还是真的有效，所以要拿着这个ST去SSO服务器再问一下，这个用户给我的ST是否有效，是有效的我才能让这个用户访问。","link":"/2021/06/01/90000%E6%96%87%E7%AB%A0/90010%E5%90%8C%E5%9F%9F%E4%B8%8B%E7%9A%84%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/"},{"title":"90011TCP没发完数据内核会怎么处理","text":"背景有一次，光神 在群问了个问题： 当 close 一个 TCP 连接时，如果还有没发送完的数据在缓冲区中，内核会怎么处理？ 当时我认为，因为关闭 TCP 连接会触发四次挥手过程，而为了让四次挥手能够快速完成，应该会把发送缓冲区的数据清空，然后发送四次挥手的数据包。 带着疑问，我去查阅 Linux 源码的实现，下面就是关闭一个 TCP 连接的过程。 关闭 TCP 连接过程关闭一个 TCP 连接可以使用 close() 系统调用，我们来分析一下当调用 close() 关闭一个 TCP 连接时会发生什么事情。 当调用 close() 系统调用时，会触发调用 sys_close() 内核函数，其实现如下： 1234567asmlinkage long sys_close(unsigned int fd){ struct file * filp; struct files_struct *files = current-&gt;files; ... return filp_close(filp, files); ...} sys_close() 函数最终会调用 file_close() 函数来关闭文件（由于在 Linux 中 socket 是一种特殊的文件），我们接着分析 filp_close() 函数的实现： 123456int filp_close(struct file *filp, fl_owner_t id){ ... fput(filp); return retval;}void fput(struct file * file){ ... if (atomic_dec_and_test(&amp;file-&gt;f_count)) { ... if (file-&gt;f_op &amp;&amp; file-&gt;f_op-&gt;release) file-&gt;f_op-&gt;release(inode, file); ... }} 可以看到，最终会调用文件系统对应的 release() 方法来处理关闭操作。对于 socket 文件系统，release() 方法对应的是 sock_close() 函数，而 sock_close() 函数最终会调用 sock_release() 函数，所以我们来看看 sock_release() 函数的实现： 1void sock_release(struct socket *sock){ if (sock-&gt;ops) sock-&gt;ops-&gt;release(sock); ...} sock_release() 函数也很简单，就是调用对应 协议族 的 release() 方法，因为 Linux 的 socket 文件系统可以支持多种协议族，比如 INET、Unix Domain Socket、Netlink 等。而对应 INET协议族（网络） 来说，这个 release() 方法对应的是 inet_release() 函数，inet_release() 函数实现如下： 12345678910111213int inet_release(struct socket *sock){ struct sock *sk = sock-&gt;sk; if (sk) { long timeout; ... timeout = 0; if (sk-&gt;linger &amp;&amp; !(current-&gt;flags &amp; PF_EXITING)) timeout = sk-&gt;lingertime; sock-&gt;sk = NULL; sk-&gt;prot-&gt;close(sk, timeout); } return(0);} inet_release() 函数最终会调用对应 传输层（TCP或者UDP） 的 close() 方法，对于 TCP协议 来说，close() 方法对应的是 tcp_close() 函数，tcp_close() 就是关闭 TCP 连接的最后站点。 由于 tcp_close() 函数比较复杂，我们这里只分析当发生缓冲区还有数据的情况下，内核会怎么处理缓冲区的数据。 123456789101112131415161718192021222324void tcp_close(struct sock *sk, long timeout){ struct sk_buff *skb; int data_was_unread = 0; ... // 如果接收缓冲区有数据, 那么先情况接收缓冲区的数据 while((skb= __skb_dequeue(&amp;sk-&gt;receive_queue)) != NULL) { u32 len = TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq - skb-&gt;h.th-&gt;fin; data_was_unread += len; __kfree_skb(skb); } ... if (data_was_unread != 0) { // 如果接收缓冲区有数据没有处理 tcp_set_state(sk, TCP_CLOSE); // 把socket状态设置为TCP_CLOSE tcp_send_active_reset(sk, GFP_KERNEL); // 发送一个reset包给对端连接 } else if (sk-&gt;linger &amp;&amp; sk-&gt;lingertime==0) { ... } else if (tcp_close_state(sk)) { tcp_send_fin(sk); // 开始发生四次挥手包 } ...} 从 tcp_close() 函数的实现可以看出，关闭过程主要有两种情况： 如果接收缓冲区还有数据没有被用户处理，那么就先把接收缓冲区的数据清空，并且发送一个 reset 包给对端连接。 如果接收缓冲区没有数据，那么就调用 tcp_send_fin() 函数开始进行四次挥手过程。 四次挥手过程如下图： 接下来，我们分析 tcp_send_fin() 函数的实现： 123456789101112131415161718192021222324252627282930void tcp_send_fin(struct sock *sk){ struct tcp_opt *tp = &amp;(sk-&gt;tp_pinfo.af_tcp); struct sk_buff *skb = skb_peek_tail(&amp;sk-&gt;write_queue); // 发送缓冲区列表最后一个缓冲块 unsigned int mss_now; ... if (tp-&gt;send_head != NULL) { // 如果发送缓冲区不为空 TCP_SKB_CB(skb)-&gt;flags |= TCPCB_FLAG_FIN; // 把最后一个发送缓冲块设置FIN标志 TCP_SKB_CB(skb)-&gt;end_seq++; tp-&gt;write_seq++; } else { // 如果发送缓冲区为空 for (;;) { skb = alloc_skb(MAX_TCP_HEADER, GFP_KERNEL); // 申请一个新的缓冲块 if (skb) break; current-&gt;policy |= SCHED_YIELD; schedule(); ' } skb_reserve(skb, MAX_TCP_HEADER); skb-&gt;csum = 0; TCP_SKB_CB(skb)-&gt;flags = (TCPCB_FLAG_ACK | TCPCB_FLAG_FIN); // 设置FIN标志 TCP_SKB_CB(skb)-&gt;sacked = 0; TCP_SKB_CB(skb)-&gt;seq = tp-&gt;write_seq; TCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(skb)-&gt;seq + 1; tcp_send_skb(sk, skb, 1, mss_now); // 发送给对端连接 } ...} 在 tcp_send_fin() 函数我们终于找到了当发送缓冲区不为空的处理，当发送缓冲区不为空时，首先会获取发送缓冲区的最后一个缓冲块，然后把这个缓冲区的 FIN标志位 设置上。 所以我前面的想法是错的，当关闭一个 TCP 连接时，如果发送缓冲区还有数据没发送完，那么内核只会把发送缓冲区最后一个缓冲块设置上 FIN标志，而不是把发送缓冲区清空。","link":"/2020/11/28/90000%E6%96%87%E7%AB%A0/90011TCP%E6%B2%A1%E5%8F%91%E5%AE%8C%E6%95%B0%E6%8D%AE%E5%86%85%E6%A0%B8%E4%BC%9A%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86/"},{"title":"自动转换神器 Mapstruct","text":"Bean 自动转换神器 Mapstruct简介Mapstruct这个神器，它可以代替BeanUtil来进行DTO、VO、PO之间的转换。它使用的是Java编译期的 annotation processor 机制，说白了它就是一个代码生成器，代替你手工进行类型转换期间的取值赋值操作。 常见的转换方式有：1.调用getter/setter方法进行属性赋值（需要写一大堆赋值代码）2.调用BeanUtil.copyPropertie进行反射属性赋值（难定位哪个字段在哪里进行的赋值，同时用到反射性能不好）3.MapStruct工具进行转换，原理是在代码编译阶段生成对应的赋值代码，底层还是getter/setter方法 如何使用1234@Mapper(componentModel = &quot;spring&quot;)public interface AreaMapping { List&lt;AreaInfoListVO&gt; toVos(List&lt;Area&gt; areas);}","link":"/2022/05/07/90000%E6%96%87%E7%AB%A0/90012Bean%E8%87%AA%E5%8A%A8%E8%BD%AC%E6%8D%A2%E7%A5%9E%E5%99%A8Mapstruct/"},{"title":"","text":"企业战略、项目价值分析面向的挑战老板认可？老板干预细节、干预团队 项目成败？轰轰烈烈、半途而废 成就感？ 团队士气涣散、个人成长阻塞","link":"/2022/03/15/90000%E6%96%87%E7%AB%A0/DDD%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8/"},{"title":"","text":"使用Dockerfile构建容器$touch Dockerfile 12345678910FROM ubuntu:14.04MAINTAINER Tony ENV REFRESHED_AT 2017-05-08RUN apt-get updateRUN apt-get -y -q install nginxRUN mkdir -p /var/www/htmlADD nginx/global.conf /etc/nginx/conf.d/ADD nginx/nginx.conf /etc/nginx/nginx.confEXPOSE 80 这个Dockerfile内容包括以下几项。 安装Nginx 在容器中创建一个目录/var/www/html 将来自我们下载的本地文件的Nginx配置文件添加到镜像中。 公开镜像80端口 将文件nginx/global.conf用ADD指令复制到/etc/nginx/conf.d目录中。 global.conf 12345678910server{ listen 0.0.0.0:80; server_name _; root /var/www/html/website; index index.html index.htm; access_log /var/log/nginx/default_access.log; error_log /var/log/nginx/default_error.log;} 这个文件将Nginx设置为监听80端口，并将网络服务的根路径设置为/var/www/html/website，这个目录是我们用RUN指令创建的。 然后我们还需要将Nginx配置为非守护进程的模式，这样可以让Nginx在Docker容器里工作。将文件nginx/nginx.conf复制到/etc/nginx目录就可以达到这个目的。 nginx.conf配置文件 1234567891011121314151617181920user www-data;worker_processes 4;pid /run/nginx.pid;daemon off;events { }http{ sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; gzip on; gzip_disable &quot;msie6&quot;; include /etc/nginx/conf.d/*.conf;}","link":"/2021/04/14/13000Linux/Docker/Dockerfile%E6%9E%84%E5%BB%BA%E5%AE%B9%E5%99%A8/"},{"title":"","text":"Docker Index(Host) Docker Daemon Container 1 Container 2 Container 3 Docker ClientDocker Indexdocker pulldocker rundocker …Docker ClientDocker Index The Docker daemon如上图所示，daemon在主机上面执行。用户只能通过client同daemon通讯。The Docker clientDocker client是用户与Docker之间的重要接口。它从用户那里接受命令，并且将daemon的返回数据展现出来。 Inside Docker为了深入理解docker的内部机制，需要了解以下三个组件：Docker images.(镜像)Docker registries.(仓库)Docker containers.(容器) So how does Docker work?目前为止，我们已经可以完成以下几个工作： 1、创建一个包含你需要执行应用的镜像。2、根据这个镜像，你可以创建一个容器。3、你可以将这个容器上传到仓库中提供给其他人使用。 下面，我们看一下如何执行Docker。 How does a Docker Image work?我们知道Docker containers启动时所以来的Docker images其实是一个只读性质的模板。每个模板都包含若干层。Docker采取了 union file systems 的机制将这些曾聚合为一个image。Union file systems 允许物理隔离的文件或者目录，相互重叠覆盖，形成线性的文件系统。 Docker也正是基于上述层的实现方式而做到了轻量级。当你修改一个image时，docker并没有修改原有的image数据，而是新建了一个数据层。当你在docker中修改整个image或者重建实体时，原有数据都没有变化，只是若干层发生了变化。所以当image发生了变化时，不需要重新同步整个image，而只要将发生变化的层同步一次就可以。这样就使docker image做的快速并且简单。 每个image都是从base image演变过来的。你可以创建你的base image。如果你有apache的image，就可以把这个镜像作为你应用程序的base image。 Note：Docker 一般都是从Dock Hub上面获取base images。 Docker通过一些很简单的步骤就可以依据base images创建新的image。每执行一个步骤，新的image就会创建一个新层(layer)。基本的步骤如下： Run a command. Add a file or directory. Create an environment variable. What process to run when launching a container from this image. 这些命令可以再Dockerfile中定义。当你需要新建一个image是，docker可以自动读取Dockerfile中的命令，并且执行这些命令。最终生成一个新的image。 How does a Docker registry work?Docker registry是用来保存images的。当你新建好image后，就可以将image上传到Dock Hub或者你私有的store中。 借助于Docker client，你可以在Dock Hub检索你所需的image，同时将这些image下载到本地。 同时Dock Hub也提供公开和私有两种模式，处于公开模式下的image，所有人都可以下载和使用这些image。而处于私有模式下的image，只有本人或者经过授权后的人才能下载并且使用这些image。 How does a container work?一个标准容器包括：操作系统，用户自定义的文件和原数据。正如我们所知的那样，每个容器都是由image所创建的。image告诉docker，这个容器运行时，应该有哪些进程和应该有哪些配置参数。因为image是只读的，所以容器在运行时会在image原有层的基础上面创建一些可读可写的新层。而你的应用运行所需的数据将会被记录到这些数据层中。 What happens when you run a container?不论是使用docker程序或者API，docker client都会通知docker daemon如何操作容器。 当我们执行如下命令时： 1$ docker run -i -t ubuntu /bin/bash docker client会启动，然后使用后面的run参数来通知docker daemon启动一个新容器。这个简短的命令将会通知docker daemon以下信息： 1.容器所需的image在哪里，这里image名称是ubuntu，是一个base image。2.当容器启动时，你想让容器初始化的动作，这里我们需要容器启动时自动切换到/bin/bash下面。 所以当我们敲下回车后，docker将会如何处理呢？ Pulls the ubuntu image:Docker检测image是否存在，如果本地不存在，则默认从Dock Hub下载。如果本地存在，则使用本地的image启动容器。 Creates a new container:Docker加载image，然后创建容器。 Allocates a filesystem and mounts a read-write layer : 容器开始创建文件系统，并且在image上面添加可读可写的数据层。 Allocates a network / bridge interface: Docker开始创建网络接口，并且允许容器同主机进行关联。 Sets up an IP address:Docker从IP资源池中挑选一个分配给容器。 Executes a process that you specify: Docker开始执行指定的应用或者命令 Captures and provides application output: Docker将执行过程当中的输出或者错误信息返回给Client。让用户可以知道当前应用执行的情况. 以上是容器的执行过程，下面我们将开始描述如何管理容器，包括：结束，停止和移除。 The underlying technologyDocker 底层使用的是Linux内核中的虚拟化技术，来呈现我们刚才所看到的一切功能。 NamespacesDocker采用了称之为”Namespaces”的技术解决方案来隔离不同的workspace(也就是上面所定义的容器)。当你执行一个容器时，docker会为这个容器创建一系列的namespace。 以下是docker所创建的namespace： The pid namespace: 用来隔离进程。(PID就是process id) The net namespace: 用来管理网络接口 The ipc namespace: 用来控制IPC资源的访问。 The mnt namespace: 用来管理挂载点(mnt是 mount point) The uts namespace: 用来隔离内核和版本信息(UTS，分时复用系统 Unix Timesharing System) Control groupsDocker同时也采用了一种称之为”cgroups”的技术来控制group。不同应用之间隔离的关键在于，每个应用只能访问属于自己的资源。这样才能确保主机上面同时存在多个用户。Cgroups可以确保docker将可用的硬件资源共享给所有容器，并且可以在必要时间，对容器限制硬件资源。例如可以限制每个容器可以访问的内存容量。 Union file systemsUnion file systems 或者称为”UnionFS”是docker在创建层时采用的文件系统。这种文件系统使docker变得很轻量级并且执行速度很快。Docker使用UnionFS为容器提供相对应的数据块(data blocks)。Docker可以使用多种类型的UnionFS，比如：AUFS, btrfs, vfs, and DeviceMapper. Container formatDocker将上面我们所描述的各种组件封装成container数据类型(我们就称其为容器)。默认的容器类型是libcontainer。Docker同样也支持传统Linux使用LXC实现的容器类型。再未来，Docker也将支持其他类型的容器，比如：BSD Jails 或者Solaris Zones 版本的容器类型。 安装Docker1234567sudo apt install docker.iosudo usermod -a -G docker &lt;your username&gt;restart your linuxusermod:-modify a user account-a add the user to the supplementary group. Use only with -G option Q:Cannot connect to the Docker daemon. Is the docker daemon running on this host? A:The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can access it with sudo. For this reason, docker daemon always runs as the root user. To avoid having to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read/writable by the docker group. 测试Docker123service docker statussudo docker run hello-worldsudo docker run docker/whalesay cowsay xxx123 显示所有images1sudo docker images 显示所有containers1sudo docker ps -a 删除container和image12sudo docker rm xxx_containersudo docker rmi yyy_image 交互启动container1sudo docker start -a -i xxx_container 交互式运行image1sudo docker run -it xxx_container bash 挂载某container1sudo docker attach xxx_container 显示container或者image相关信息1sudo docker inspect img_name | container_name 显示container IP1sudo docker inspect -f '{{.NetworkSettings.IPAddress}}' xxx_container 复制文件12sudo docker cp local_file_path container:container_file_pathsudo docker cp container:container_file_path local_file_path 创建镜像image1sudo docker build -f dockerfile -t img_name . 保存镜像为文件1docker save -o filename.tgz img_name 导入镜像1docker load -i img_name SSH 到镜像，即登录进行进行debug12345678910docker exec -it img_name /bin/bashDocker Registry基本操作docker registry: https://github.com/docker/distribution.gitStart docker registry server:docker run -d -it \\ --name con_docker_registry \\ -h hostname \\ -v /data/docker-registry:/var/lib/registry \\ -p 5000:5000 \\ docker-registryView the image listcurl -v -X GET localhost:5000/v2/_catalog","link":"/2021/04/14/13000Linux/Docker/Docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"title":"","text":"Docker via HomebrewRunning Docker on Mac requires VirtualBox so if you don’t have it already: 1brew cask install virtualbox Then install Docker and the helper tool boot2docker: 12brew install dockerbrew install boot2docker 使用命令行12345678910111213141516171819202122232425261、 创建一个新的 Boot2Docker 虚拟机&gt;$ boot2docker init这会创建一个新的虚拟主机，你只需要运行一次这个命令就可以了，以后就不需要了。2、 启动 boot2docker 虚拟机。&gt;$ boot2docker startTo connect the Docker client to the Docker daemon, please set: export DOCKER_CERT_PATH=/Users/user/.boot2docker/certs/boot2docker-vm export DOCKER_TLS_VERIFY=1 export DOCKER_HOST=tcp://192.168.59.103:2376Or run: `eval &quot;$(boot2docker shellinit)&quot;`3、 通过 docker 客户端来查看环境变量&gt;$ boot2docker shellinit4、 使用 shell 命令来设置环境变量。&gt;$ eval &quot;$(boot2docker shellinit)&quot;Writing /Users/user/.boot2docker/certs/boot2docker-vm/ca.pemWriting /Users/user/.boot2docker/certs/boot2docker-vm/cert.pemWriting /Users/user/.boot2docker/certs/boot2docker-vm/key.pem5、 运行 hello-word 容器来验证安装。&gt;$ docker run hello-world Boot2Docker 基本练习123456&gt;$ boot2docker status&gt;$ docker version 容器端口访问1、 在 Docker 主机上启动一个 Nginx 容器。&gt;$ docker run -d -P --name web nginx 一般来说，docker run 命令会启动一个容器，运行这个容器，然后退出。-d 标识可以让容器在 docker run 命令完成之后继续在后台运行。 -P 标识会将容器的端口暴露给主机，这样你就可以从你的 MAC 上访问它。 1234567891011121314151617181920212、 使用 docker ps 命令来查看你运行的容器CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd478595f206d nginx &quot;nginx -g 'daemon off&quot; 4 minutes ago Up 4 minutes 0.0.0.0:32769-&gt;80/tcp, 0.0.0.0:32768-&gt;443/tcp web3、 查看容器端口&gt; docker port web443/tcp -&gt; 0.0.0.0:3276880/tcp -&gt; 0.0.0.0:32769上边的显示告诉我们，web 容器将 80 端口映射到 Docker 主机的 32769 端口上。4、 在浏览器输入地址 http://localhost:32769/ (localhost 是 0.0.0.0):没有正常工作。没有正常工作的原因是 DOCKER_HOST 主机的地址不是 localhost (0.0.0.0),但是你可以使用 boot2docker 虚拟机的IP地址来访问。5、 获取 boot2docker 主机地址&gt;$ boot2docker ip192.168.59.1036、在浏览器中输入 http://192.168.59.103:491577、 通过如下方法，停止并删除 nginx 容器。&gt;$ docker stop web&gt;$ docker rm web 给容器挂载一个卷当你启动 boot2docker 的时候，它会自动共享 /Users 目录给虚拟机。你可以利用这一点，将本地目录挂载到容器中。这个练习中我们将告诉你如何进行操作。 12345678910111213141516171819202122231、 回到你的 $HOME 目录 $ cd $HOME2、 创建一个新目录，并命名为 site$ mkdir site3、 进入 site 目录。$ cd site4、 创建一个 index.html 文件。$ echo &quot;my new site&quot; &gt; index.html5、 启动一个新的 nginx 容器,并将本地的 site 目录替换容器中的 html 文件夹。&gt;$ docker run -d -P -v $HOME/site:/usr/share/nginx/html --name mysite nginx6、 获取 mysite 容器端口&gt;$ docker port mysite80/tcp -&gt; 0.0.0.0:49166443/tcp -&gt; 0.0.0.0:491657、 在浏览器中打开站点。8、 现在尝试在 $HOME/site 中创建一个页面&gt;$ echo &quot;This is cool&quot; &gt; cool.html9、 在浏览器中打开新创建的页面。10、 停止并删除 mysite 容器。&gt;$ docker stop mysite&gt;$ docker rm mysite","link":"/2021/04/14/13000Linux/Docker/Docker%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0/"},{"title":"","text":"12345678910FROM composer:2.0RUN docker-php-source extract \\ &amp;&amp; apk add --no-cache --virtual .phpize-deps-configure $PHPIZE_DEPS \\ &amp;&amp; pecl install channel://pecl.php.net/apcu-5.1.20 \\ &amp;&amp; pecl install channel://pecl.php.net/yac-2.3.0 \\ &amp;&amp; docker-php-ext-enable apcu \\ &amp;&amp; docker-php-ext-enable yac \\ &amp;&amp; apk del .phpize-deps-configure \\ &amp;&amp; docker-php-source delete 1docker run --rm -it -v &quot;$PWD:/app&quot; -v &quot;$HOME/.ssh:/root/.ssh&quot; xiongyouli/composer composer install","link":"/2021/04/16/13000Linux/Docker/Mac%20Homebrew/"},{"title":"Ubuntu LNMP","text":"由于当前多数服务器都是Linux的原因，本文只讲ubuntu下的安装！ 第一步：安装docker安装教程：http://www.runoob.com/docker/ubuntu-docker-install.html 第二步：拉取ubuntu:16.04镜像 docker pull ubuntu:16.04 拉取成功后，查看所有镜像 docker images 第三步：将该镜像在一个容器中运行，并进入容器123docker run -dit --name my-lnmp ubuntu:16.04docker exec -it my-lnmp /bin/bash 第四步：更新容器 apt 源，安装curl，vim12345apt-get updateapt-get install curlapt-get install vim 第五步：安装nginx123456789101112131415161718apt-get install nginx# 配置文件位置#&gt; /etc/nginx/nginx.conf /etc/nginx/conf.d/*.conf# 默认主目录#&gt; /usr/share/nginx/html/# 管理nginx服务service nginx start // 启动service nginxstop // 停止service nginx restart // 重启测试 curl localhost 第六步：安装php712345apt-get install phpapt-get install php7.0-mysql php7.0-curl php7.0-xml php7.0-mcrypt php7.0-json php7.0-fpmphp7.0-gd php7.0-mbstring php-mongodb php-memcached php-redis 测试 (如果有结果，则表示安装成功) 1php-v 配置php.ini 12345vim /etc/php/7.0/fpm/php.ini# 将cgi.fix_pathinfo=1这一行去掉注释，将1改为0#&gt; / 是vi查找的命令 配置php-fpm 123vim /etc/php/7.0/fpm/pool.d/www.conf# 修改 listen = /var/run/php/php7.0-fpm.sock 配置nginx 123vim /etc/nginx/sites-enabled/default将index index.html index.htm;改成index index.php index.html index.htm; 在service里面，location /{}下面增加以下配置 123456789101112131415161718192021location ~ \\.php$ {fastcgi_split_path_info ^(.+\\.php)(/.+)$;# NOTE: You should have &quot;cgi.fix_pathinfo = 0;&quot; in php.ini# With php5-cgi alone:# fastcgi_pass 127.0.0.1:9000;# With php5-fpm:fastcgi_pass unix:/var/run/php/php7.0-fpm.sock;fastcgi_index index.php;fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;include fastcgi_params;} 容器中运行 启动 php-fpm service php7.0-fpm start 重启 Nginx ，检测配置是否成功 service nginx restart 新建 index.php 测试文件 123&lt;?phpecho &quot;Hello World&quot;; 执行 curl localhost #如果看到hello world!表示运行成功 第七步：安装mysql12345apt-get install mysql-server//测试service mysql startmysql -uroot -p 第八步：设置容器开机启动项12345678910111213141516171819202122232425262728293031在.bashrc写入开机启动项vim~/.bashrc写入以下内容，保存# 开机启动项service php7.0-fpm startservice mysql startservice nginx start# tail -f /var/log/nginx/error.log将配置好的Docker容器，打包上传阿里云退出 dockerexit查看容器对应的 CONTAINER IDdocker ps -as将容器打包成新镜像docker commit [CONTAINER ID] new-lnmp停止正在运行的容器docker stop my-lnmp 第九步：设置ssh登录docker12345678910111213141516171819//暴露docker端口22到主机端口9000dokcer run -p 9000:22apt-get updateapt-get install openssh-servervim /etc/ssh/sshd_config//修改 PermitRootLogin yes# StrictMode yes#修改用户密码passwd#启动ssh服务器service ssh startdocker restart containerssh 192.168.99.100 -p 9000 使用刚打包的镜像，创建容器，-p 端口映射# -v 本地目录映射到容器内123docker run -dit -p 80:80 -p 3306:3306 -p 9000:22 -v /var/www/:/var/www/ --name nginx-mysql-php7 new-lnmp /bin/bash -p的意思是 将docker的80端口和container的80端口绑定 在浏览器通过访问docker的ip（192.168.99.100）响应成功，则大功告成。如果是Windows操作系统，docker的根目录是/c/user/User，可通过pwd查看。此时通过-v挂载磁盘的时候，建议直接在此目录下生成一个www文件夹。那么就可以通过以下命令生成容器了。12345docker run -dit -p 80:80 -p 3306:3306 -p 9000:22 -v ~/www:/var/www/html --name lnmp1 new-lnmp /bin/bash或e:/path/todocker ip：192.168.99.100 container ip：172.17.0.3 下面的是打包和拉取容器到阿里云的方法： 登录阿里云docker registry: $ sudo docker login –username=laopo890220 registry.cn-hangzhou.aliyuncs.com 登录registry的用户名是您的阿里云账号全名，密码是您开通namespace时设置的密码。 你可以在镜像管理首页点击右上角按钮修改docker login密码。 从registry中拉取镜像： $ sudo docker pull [registry.cn-hangzhou.aliyuncs.com/gaven/nginx-mysql-php7:镜像版本号] 将镜像推送到registry： $ sudo docker login –username=laopo890220 registry.cn-hangzhou.aliyuncs.com $ sudo docker tag [ImageId] [registry.cn-hangzhou.aliyuncs.com/gaven/nginx-mysql-php7:镜像版本号] $ sudo docker push [registry.cn-hangzhou.aliyuncs.com/gaven/nginx-mysql-php7:镜像版本号] 其中[ImageId],[镜像版本号]请你根据自己的镜像信息进行填写。","link":"/2022/05/06/13000Linux/Docker/Ubuntu%20LNMP/"},{"title":"","text":"windows docker 问题env: ubuntu ifconfig not found? apt-get update apt install net-tools # ifconfig apt install iputils-ping # ping 进入container docker attach name docker exec -it name /bin/bash Ubuntu中文显示乱码 apt-get update apt-get install locales Ubuntu默认的中文字符编码为zh_CN.UTF-8 sudo dpkg-reconfigure locales 选择zh_CN.UTF-8 ssh重新连接 db_rcm db_kdkj db_kdkj_rd db_kdkj_risk ‘kdkj_behavior’ mongodb_log mongodb_new mongodb_data_snapshot ‘mongodb_data_snapshot_02’ ‘redis_risk_permanent’ ‘uniform_alarm’","link":"/2021/04/14/13000Linux/Docker/Windows%20docker%E9%97%AE%E9%A2%98/"},{"title":"","text":"Mac环境目录123456789// 通过phpinfo()打印页面1. php.ini文件目录Loaded Configuration File/usr/local/etc/php/7.1/php.ini2.扩展存放目录extension_dir/usr/local/lib/php/pecl/20160303 安装apcu1234567891011121314// 1.下载源码 git clone https://github.com/krakjoe/apcu// 2.编译安装 cd apcuphpize./configuremakesudo make install// 3. 修改php.ini文件extension=apcuphp --ini// apcu.so目录/opt/homebrew/Cellar/php@7.4/7.4.16/pecl/20190902 MacOs OS11.1 Install Nginx 12345brew install nginx# Docroot is:/opt/homebrew/var/www# Config files/opt/homebrew/etc/nginx/servers/ OS11.1 配置php-fpm 123456789101112131415161718$ php-fpmERROR: failed to load configuration file '/private/etc/php-fpm.conf'ERROR: FPM initialization failed$ cd /private/etc$ sudo cp php-fpm.conf.default php-fpm.conf$ php-fpmWARNING: Nothing matches the include pattern '/private/etc/php-fpm.d/*.conf' from /private/etc/php-fpm.conf at line 143ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory (2)ERROR: failed to post process the configurationERROR: FPM initialization failed# cd /usr/var/log 发现根本没有这个目录，甚至连 var 目录都没有，加上为了避免权限问题，干脆配置到 /usr/local/var/log 目录。# 修改 php-fpm.conf error_log 配置为 /usr/local/var/log/php-fpm.log，并把 user 和 group 改为和当前用户一样。$ php-fpmWARNING: Nothing matches the include pattern '/private/etc/php-fpm.d/*.conf' from /private/etc/php-fpm.conf at line 143.ERROR: No pool defined. at least one pool section must be specified in config file$ cd /private/etc/php-fpm.d$ cp www.conf.default www.conf 异常 1.fatal error: ‘pcre2.h’ file not found #include “pcre2.h” 1$cp /opt/homebrew/Cellar/pcre2/10.36/include/pcre2.h /opt/homebrew/Cellar/php\\@7.4/7.4.16/include/php/ext/pcre/pcre2.h","link":"/2021/05/12/13000Linux/MAC/%E5%AE%89%E8%A3%85%E6%89%A9%E5%B1%95/"},{"title":"","text":"[TOC] Mac OS12macOS Mojave10.14.6 安装PHP7.41234567891011brew install php@7.4Now homebrew-php has been migrated to homebrew-core and by default, PECL should be installed along with your PHP.现在，homebrew php已经迁移到homebrew core，默认情况下，PECL应该与php一起安装。To install PHP extensions, you need to use PECL as a recommended way. 要安装PHP扩展，建议使用PECL。For example: pecl install apc or pecl install xdebug.查找apcu扩展pecl search apcusudo pecl install APCu Mac环境目录123456789// 通过phpinfo()打印页面1. php.ini文件目录Loaded Configuration File/usr/local/etc/php/7.1/php.ini2.扩展存放目录extension_dir/usr/local/lib/php/pecl/20160303 安装apcu12345678910111213// 1.下载源码 git clone https://github.com/krakjoe/apcu// 2.编译安装 cd apcuphpize./configuremakesudo make install// 3. 修改php.ini文件extension=apcu.soInstalling shared extensions: /usr/lib/php/extensions/no-debug-non-zts-20160303/ phpize编译提示Cannot find autoconf解决办法1$ brew install autoconf make命令 ./apc.h:67:10: fatal error: ‘php.h’ file not found1open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg 错误12345678装不了php的扩展，make install失败RudonMacBook:igbinary-master rudon$ make installInstalling shared extensions: /usr/lib/php/extensions/no-debug-non-zts-20131226/cp: /usr/lib/php/extensions/no-debug-non-zts-20131226/#INST@12567#: Operation not permittedmake: *** [install-modules] Error 1cp: /usr/lib/php/extensions/no-debug-non-zts-20121212/#INST@17000#: Operation not permitted 原来是OSX 10.11 El Capitan（或更高）新添加了一个新的安全机制叫系统完整性保护System Integrity Protection (SIP)，所以对于目录/System/sbin/usr不包含(/usr/local/)仅仅供系统使用，其它用户或者程序无法直接使用，而我们的/usr/lib/php/extensions/刚好在受保护范围内 解决所以解决方法就是禁掉SIP保护机制，步骤是： 重启系统 按住Command + R （重新亮屏之后就开始按，象征地按几秒再松开，出现苹果标志，ok） 菜单“实用工具” ==&gt;&gt; “终端” ==&gt;&gt; 输入csrutil disable；执行后会输出：Successfully disabled System Integrity Protection. Please restart the machine for the changes to take effect. 再次重启系统 禁止掉SIP后，就可以顺利的安装了，当然装完了以后你可以重新打开SIP，方法同上，只是命令是csrutil enable 错误’pcre2.h’ file not found123456/opt/homebrew/Cellar/php@7.4/7.4.16/include/php/ext/pcre/php_pcre.h:25:10: fatal error: 'pcre2.h' file not found#include &quot;pcre2.h&quot; ^~~~~~~~~1 error generated.make: *** [php_apc.lo] Error 1ERROR: `make' failed ./configure --with-php-config=/opt/homebrew/opt/php@7.4/bin/php-config php@7.2安装php-redis123456/usr/local/opt/php@7.2/bin/php-configgit clone https://www.github.com/phpredis/phpredis.gitphpize &amp;&amp; ./configure --with-php-config=/usr/local/etc/php/7.2 &amp;&amp; make &amp;&amp; sudo make installInstalling shared extensions: /usr/local/Cellar/php@7.2/7.2.34_4/pecl/20170718/","link":"/2021/04/14/212PHP/%E6%89%A9%E5%B1%95%E5%AE%89%E8%A3%85/mac/"},{"title":"","text":"Spring依赖注入源码解析-下 1234567891011121314151617181920212223242526272829public class UserService { private OrderService orderService; @Autowired public void serOrderService(OrderService orderService) { this.orderService = orderService; }}public class DefaultListableBeanFactory { public }// 在属性上加@Lazy，返回一个AOP的代理对象@Value(&quot;123&quot;)// 解析@Value// QualifierAnnotationAutowireCandidateResolver.java// Determine a suggested value from any of the given candidate annotationsprotected Object findValue(Annotation[] annotationsToSearch) { if (annotationsToSearch.length &gt; 0) { AnnotationAttributes attr = AnnotatedElementUtils.getMergedAnrrnotationAttributes( AnnotatedElementUtils.forAnnotations(annotationsToSearch), this.valueAnnotationType); if (attr != null) { return ; } }} @Value也可以对象赋值 1234567// DefaultListableBeanFactory.javaTypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter());try { return converter.convertIfnecessary(value, type, descriptor.getTypeDescriptor());} catch (UnsupportedOperationException ex) { return (descriptor.getField() != null ? converter.convertIfnecessary(value, type, descriptor.getField()) : converter.convertIfnecessary(value, type, descriptor.getMethodParameter()));} @Autowired map对象 123// DefaultListableBeanFactory.javaMap&lt;String, Object&gt; matchingBeans = findAutowireCandidates(beanName, elementType, new MultiElementDescriptor(descriptor));","link":"/2022/03/13/40000%E8%A7%86%E9%A2%91/41000Spring/41001%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/"}],"tags":[{"name":"限流","slug":"限流","link":"/tags/%E9%99%90%E6%B5%81/"},{"name":"内存分配","slug":"内存分配","link":"/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"引用","slug":"引用","link":"/tags/%E5%BC%95%E7%94%A8/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"死锁","slug":"死锁","link":"/tags/%E6%AD%BB%E9%94%81/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"CPU指标","slug":"CPU指标","link":"/tags/CPU%E6%8C%87%E6%A0%87/"},{"name":"线程","slug":"线程","link":"/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"堆缓存","slug":"堆缓存","link":"/tags/%E5%A0%86%E7%BC%93%E5%AD%98/"},{"name":"分布式缓存","slug":"分布式缓存","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"分布式事务","slug":"分布式事务","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"公众号","slug":"公众号","link":"/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"Stream","slug":"Stream","link":"/tags/Stream/"},{"name":"代码优化","slug":"代码优化","link":"/tags/%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"自动装配","slug":"自动装配","link":"/tags/%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/"},{"name":"MySql","slug":"MySql","link":"/tags/MySql/"},{"name":"单点登录","slug":"单点登录","link":"/tags/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/"},{"name":"Bean","slug":"Bean","link":"/tags/Bean/"}],"categories":[{"name":"面试","slug":"面试","link":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"Linux命令","slug":"Linux命令","link":"/categories/Linux%E5%91%BD%E4%BB%A4/"},{"name":"缓存","slug":"缓存","link":"/categories/%E7%BC%93%E5%AD%98/"},{"name":"JAVA","slug":"JAVA","link":"/categories/JAVA/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"读书","slug":"读书","link":"/categories/%E8%AF%BB%E4%B9%A6/"},{"name":"时间管理","slug":"时间管理","link":"/categories/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"网络","slug":"网络","link":"/categories/%E7%BD%91%E7%BB%9C/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/categories/SpringBoot/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"}]}